# References

<!-- This document was automatically generated with bibtex2html 1.95
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     "C:\Program Files (x86)\Bibtex2html\bibtex2html.exe" -o - -s abbrv -q -nodoc references.bib  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hollemans2020">1</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Hollemans.
 New mobile neural network architectures.
 Accessed 2020-09-26:
  <a href="https://machinethink.net/blog/mobile-architectures">https://machinethink.net/blog/mobile-architectures</a>, 2020.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="i2016squeezenet">2</a>]
</td>
<td class="bibtexitem">
F.&nbsp;N. Iandola, S.&nbsp;Han, M.&nbsp;W. Moskewicz, K.&nbsp;Ashraf, W.&nbsp;J. Dally, and K.&nbsp;Keutzer.
 Squeezenet: Alexnet-level accuracy with 50x fewer parameters and
  &lt;0.5mb model size, 2016.
[&nbsp;<a href="http://arxiv.org/abs/1602.07360">arXiv</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="howard2017mobilenets">3</a>]
</td>
<td class="bibtexitem">
A.&nbsp;G. Howard, M.&nbsp;Zhu, B.&nbsp;Chen, D.&nbsp;Kalenichenko, W.&nbsp;Wang, T.&nbsp;Weyand,
  M.&nbsp;Andreetto, and H.&nbsp;Adam.
 Mobilenets: Efficient convolutional neural networks for mobile vision
  applications, 2017.
[&nbsp;<a href="http://arxiv.org/abs/1704.04861">arXiv</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Sandler_2018">4</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Sandler, A.&nbsp;Howard, M.&nbsp;Zhu, A.&nbsp;Zhmoginov, and L.-C. Chen.
 Mobilenetv2: Inverted residuals and linear bottlenecks.
 <em>2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition</em>, Jun 2018.
[&nbsp;<a href="http://dx.doi.org/10.1109/cvpr.2018.00474">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2018.00474">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Howard_2019">5</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Howard, M.&nbsp;Sandler, B.&nbsp;Chen, W.&nbsp;Wang, L.-C. Chen, M.&nbsp;Tan, G.&nbsp;Chu,
  V.&nbsp;Vasudevan, Y.&nbsp;Zhu, R.&nbsp;Pang, and et&nbsp;al.
 Searching for mobilenetv3.
 <em>2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)</em>, Oct 2019.
[&nbsp;<a href="http://dx.doi.org/10.1109/iccv.2019.00140">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICCV.2019.00140">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Gholami_2018">6</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Gholami, K.&nbsp;Kwon, B.&nbsp;Wu, Z.&nbsp;Tai, X.&nbsp;Yue, P.&nbsp;Jin, S.&nbsp;Zhao, and K.&nbsp;Keutzer.
 Squeezenext: Hardware-aware neural network design.
 <em>2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)</em>, Jun 2018.
[&nbsp;<a href="http://dx.doi.org/10.1109/cvprw.2018.00215">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPRW.2018.00215">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Zhang_2018">7</a>]
</td>
<td class="bibtexitem">
X.&nbsp;Zhang, X.&nbsp;Zhou, M.&nbsp;Lin, and J.&nbsp;Sun.
 Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
 <em>2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition</em>, Jun 2018.
[&nbsp;<a href="http://dx.doi.org/10.1109/cvpr.2018.00716">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2018.00716">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Ma_2018">8</a>]
</td>
<td class="bibtexitem">
N.&nbsp;Ma, X.&nbsp;Zhang, H.-T. Zheng, and J.&nbsp;Sun.
 Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
 <em>Lecture Notes in Computer Science</em>, page 122â€“138, 2018.
[&nbsp;<a href="http://dx.doi.org/10.1007/978-3-030-01264-9_8">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-030-01264-9_8">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="tan2019efficientnet">9</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Tan and Q.&nbsp;V. Le.
 Efficientnet: Rethinking model scaling for convolutional neural
  networks, 2019.
[&nbsp;<a href="http://arxiv.org/abs/1905.11946">arXiv</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Chao_2019">10</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Chao, C.-Y. Kao, Y.&nbsp;Ruan, C.-H. Huang, and Y.-L. Lin.
 Hardnet: A low memory traffic network.
 <em>2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)</em>, Oct 2019.
[&nbsp;<a href="http://dx.doi.org/10.1109/iccv.2019.00365">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICCV.2019.00365">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Han_2020">11</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Han, Y.&nbsp;Wang, Q.&nbsp;Tian, J.&nbsp;Guo, C.&nbsp;Xu, and C.&nbsp;Xu.
 Ghostnet: More features from cheap operations.
 <em>2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)</em>, Jun 2020.
[&nbsp;<a href="http://dx.doi.org/10.1109/cvpr42600.2020.00165">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/cvpr42600.2020.00165">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="KrauseStarkDengFei-Fei_3DRR2013">12</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Krause, M.&nbsp;Stark, J.&nbsp;Deng, and L.&nbsp;Fei-Fei.
 3d object representations for fine-grained categorization.
 In <em>4th International IEEE Workshop on 3D Representation and
  Recognition (3dRR-13)</em>, Sydney, Australia, 2013.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pandey2018">13</a>]
</td>
<td class="bibtexitem">
A.&nbsp;Pandey.
 Depth-wise convolution and depth-wise separable convolution.
 Accessed 2020-09-26:
  <a href="https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec">https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec</a>,
  2018.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Hu_2018">14</a>]
</td>
<td class="bibtexitem">
J.&nbsp;Hu, L.&nbsp;Shen, and G.&nbsp;Sun.
 Squeeze-and-excitation networks.
 <em>2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition</em>, Jun 2018.
[&nbsp;<a href="http://dx.doi.org/10.1109/cvpr.2018.00745">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2018.00745">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="Poulopoulos2020">15</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Poulopoulos.
 How to use label smoothing for regularization.
 Accessed 2020-09-27:
  <a href="https://medium.com/towards-artificial-intelligence/how-to-use-label-smoothing-for-regularization-aa349f7f1dbb">https://medium.com/towards-artificial-intelligence/how-to-use-label-smoothing-for-regularization-aa349f7f1dbb</a>,
  2020.

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="loshchilov2017decoupled">16</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Loshchilov and F.&nbsp;Hutter.
 Decoupled weight decay regularization, 2017.
[&nbsp;<a href="http://arxiv.org/abs/1711.05101">arXiv</a>&nbsp;]
