EXPERIMENT: C-1
================

NAME: ghost_20200917224346

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'False',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': 'None',
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'CrossEntropyLoss',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': 'None',
 'normalize': 'False',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.005981    5.595125   0.008006    7.911017  0.001
1     1.0   0.008179    5.458806   0.011161    5.484153  0.001
2     2.0   0.011841    5.320856   0.015323    5.364559  0.001
3     3.0   0.013550    5.183889   0.019663    5.166673  0.001
4     4.0   0.022583    5.061832   0.025065    5.123150  0.001
5     5.0   0.026245    4.916092   0.027173    5.087267  0.001
6     6.0   0.036133    4.822882   0.028963    4.996418  0.001
7     7.0   0.053223    4.654538   0.036597    5.378551  0.001
8     8.0   0.066650    4.499673   0.039752    5.027032  0.001
9     9.0   0.077271    4.397918   0.045154    4.974580  0.001
10   10.0   0.097656    4.173130   0.046945    4.848983  0.001
11   11.0   0.120850    3.930448   0.059097    4.884971  0.001
12   12.0   0.148926    3.727883   0.053075    5.031929  0.001
13   13.0   0.187866    3.468127   0.066786    5.029840  0.001
14   14.0   0.238525    3.127347   0.066786    5.140965  0.001
15   15.0   0.314941    2.754153   0.052718    5.848137  0.001
16   16.0   0.391968    2.373498   0.066359    5.771030  0.001
17   17.0   0.470215    2.012918   0.065352    5.963117  0.001
18   18.0   0.562744    1.632329   0.072932    6.176042  0.001
19   19.0   0.645386    1.298776   0.073056    6.274137  0.001
20   20.0   0.732910    0.976363   0.072381    6.413125  0.001
21   21.0   0.813477    0.698321   0.072466    6.749290  0.001
22   22.0   0.872437    0.496531   0.077342    6.837774  0.001
23   23.0   0.900024    0.392856   0.074063    7.232221  0.001
24   24.0   0.924927    0.307848   0.078691    7.410985  0.001
25   25.0   0.922729    0.296121   0.081503    7.423842  0.001

EXPERIMENT: C-2
================

NAME: ghost_20200917224430

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'False',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': 'None',
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': 'None',
 'normalize': 'False',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.006348    5.448853   0.008061    5.654287  0.001
1     1.0   0.010620    5.397802   0.013765    5.434716  0.001
2     2.0   0.016724    5.275406   0.022143    6.119265  0.001
3     3.0   0.025024    5.174518   0.017059  147.443665  0.001
4     4.0   0.030518    5.090715   0.026662   11.456124  0.001
5     5.0   0.043335    4.912503   0.037768    5.038192  0.001
6     6.0   0.057007    4.768844   0.039132    5.237935  0.001
7     7.0   0.071411    4.608239   0.040923    5.004666  0.001
8     8.0   0.088379    4.449420   0.052346    5.065063  0.001
9     9.0   0.111206    4.268521   0.059484   10.853314  0.001
10   10.0   0.147583    4.082502   0.065546    5.090950  0.001
11   11.0   0.190430    3.832327   0.070010    4.872975  0.001
12   12.0   0.232544    3.620708   0.071057    5.633388  0.001
13   13.0   0.292480    3.361412   0.067158    5.194790  0.001
14   14.0   0.356445    3.119573   0.075962    5.214139  0.001
15   15.0   0.434937    2.821318   0.066855    5.243701  0.001
16   16.0   0.533081    2.521337   0.077327    5.511707  0.001
17   17.0   0.615234    2.260288   0.076210    5.990569  0.001
18   18.0   0.730225    1.959198   0.074544    5.756028  0.001
19   19.0   0.819214    1.717269   0.081915    5.545774  0.001
20   20.0   0.876953    1.550838   0.085759    5.516080  0.001
21   21.0   0.923706    1.408906   0.082356    5.555523  0.001
22   22.0   0.948486    1.313782   0.078264    5.467360  0.001
23   23.0   0.968872    1.244917   0.083720    5.308877  0.001
24   24.0   0.977417    1.198019   0.089370    5.312594  0.001
25   25.0   0.983521    1.161040   0.090045    5.209046  0.001
26   26.0   0.988892    1.132855   0.091231    5.148112  0.001

EXPERIMENT: C-3
================

NAME: ghost_20200918004934

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'False',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': 'None',
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.007935    5.451772   0.006944   33.135426  0.001
1     1.0   0.010742    5.355644   0.013765    5.430552  0.001
2     2.0   0.015991    5.311000   0.015873    6.663340  0.001
3     3.0   0.025757    5.185424   0.024058    5.362433  0.001
4     4.0   0.033081    5.036987   0.022569    5.221271  0.001
5     5.0   0.044067    4.863683   0.037596    5.191522  0.001
6     6.0   0.059204    4.675900   0.039589    5.312067  0.001
7     7.0   0.077515    4.493219   0.054439    4.908135  0.001
8     8.0   0.110352    4.307200   0.055562    4.870788  0.001
9     9.0   0.143311    4.085728   0.066801    5.120204  0.001
10   10.0   0.177490    3.884516   0.078240    4.830685  0.001
11   11.0   0.238281    3.607229   0.086960    5.011515  0.001
12   12.0   0.296875    3.358972   0.082480    4.852828  0.001
13   13.0   0.369995    3.069398   0.096384    4.791996  0.001
14   14.0   0.450439    2.776353   0.096702    4.938046  0.001
15   15.0   0.571411    2.407923   0.087634    5.387604  0.001
16   16.0   0.661133    2.130939   0.100144    5.124842  0.001
17   17.0   0.769043    1.854754   0.101366    5.221021  0.001
18   18.0   0.850220    1.625946   0.100685    5.173743  0.001
19   19.0   0.916016    1.429291   0.108815    5.030534  0.001
20   20.0   0.955322    1.303690   0.108746    5.009414  0.001
21   21.0   0.967529    1.251424   0.112085    5.044139  0.001
22   22.0   0.981567    1.182666   0.117581    4.931659  0.001
23   23.0   0.989014    1.137823   0.118310    4.866004  0.001
24   24.0   0.989014    1.122984   0.119604    4.813447  0.001
25   25.0   0.990479    1.112554   0.110954    4.878379  0.001
26   26.0   0.994141    1.092893   0.114689    4.816515  0.001
27   27.0   0.994507    1.075054   0.112799    4.830457  0.001
28   28.0   0.993896    1.075881   0.113177    4.799740  0.001

EXPERIMENT: C-4
================

NAME: ghost_20200918063934

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.006226    5.461461   0.007440   13.408026  0.001
1      1.0   0.009033    5.428660   0.009177    5.809033  0.001
2      2.0   0.012817    5.382262   0.012525    8.033515  0.001
3      3.0   0.018921    5.249013   0.020337    5.270470  0.001
4      4.0   0.023804    5.125769   0.022569    5.459584  0.001
5      5.0   0.033203    5.051504   0.028715    6.386159  0.001
6      6.0   0.037598    4.938690   0.030948    6.268939  0.001
7      7.0   0.046387    4.816183   0.036528    6.175183  0.001
8      8.0   0.052002    4.721771   0.053284    5.188688  0.001
9      9.0   0.068359    4.582071   0.061112    5.180923  0.001
10    10.0   0.081787    4.447351   0.073056    5.169614  0.001
11    11.0   0.096924    4.322036   0.081449    4.548910  0.001
12    12.0   0.114624    4.207198   0.079559    5.026164  0.001
13    13.0   0.134033    4.088762   0.087386    6.251687  0.001
14    14.0   0.161133    3.952367   0.122828    8.618488  0.001
15    15.0   0.181396    3.839917   0.133835    4.541536  0.001
16    16.0   0.205566    3.713493   0.138369    5.510671  0.001
17    17.0   0.234131    3.580904   0.183468    5.558022  0.001
18    18.0   0.281616    3.415619   0.195164    4.295217  0.001
19    19.0   0.312256    3.268610   0.222174    4.113319  0.001
20    20.0   0.347778    3.118717   0.232932    4.030760  0.001
21    21.0   0.393677    2.953693   0.266003    3.981362  0.001
22    22.0   0.429199    2.811478   0.240621    3.901869  0.001
23    23.0   0.474243    2.669566   0.284302    3.902390  0.001
24    24.0   0.529785    2.492221   0.297447    3.555491  0.001
25    25.0   0.576904    2.347570   0.330061    3.488780  0.001
26    26.0   0.609985    2.232369   0.340493    3.385057  0.001
27    27.0   0.650269    2.106173   0.320334    4.383698  0.001
28    28.0   0.686523    2.013334   0.348568    3.488869  0.001
29    29.0   0.732300    1.895314   0.368549    3.307703  0.001
30    30.0   0.761841    1.813777   0.356435    3.591188  0.001
31    31.0   0.804565    1.719476   0.363736    3.524558  0.001
32    32.0   0.823608    1.651003   0.381337    3.300700  0.001
33    33.0   0.845581    1.585353   0.391738    3.506137  0.001
34    34.0   0.870728    1.529936   0.401907    3.433707  0.001
35    35.0   0.888062    1.472982   0.405790    3.468116  0.001
36    36.0   0.913330    1.405700   0.397706    6.341680  0.001
37    37.0   0.915161    1.396255   0.404665    3.186635  0.001
38    38.0   0.927856    1.344872   0.396287    3.593980  0.001
39    39.0   0.937256    1.331210   0.415315    3.154593  0.001
40    40.0   0.948730    1.294443   0.399496    3.295781  0.001
41    41.0   0.947388    1.290777   0.414710    3.431441  0.001
42    42.0   0.958618    1.254790   0.431823    3.094430  0.001
43    43.0   0.958984    1.235678   0.429343    3.481904  0.001
44    44.0   0.964600    1.236181   0.427839    3.132331  0.001
45    45.0   0.969604    1.204803   0.430568    3.102345  0.001
46    46.0   0.971069    1.200631   0.437527    3.051227  0.001
47    47.0   0.976685    1.179333   0.443177    3.033713  0.001
48    48.0   0.972168    1.181815   0.441426    3.065190  0.001
49    49.0   0.977539    1.164661   0.446952    3.010637  0.001
50    50.0   0.979980    1.154438   0.455508    2.970415  0.001
51    51.0   0.976440    1.165635   0.446441    3.015681  0.001
52    52.0   0.978516    1.154075   0.451595    2.984624  0.001
53    53.0   0.983521    1.135906   0.436287    3.061893  0.001
54    54.0   0.983521    1.135429   0.439581    3.048776  0.001
55    55.0   0.982544    1.133505   0.453703    2.977650  0.001
56    56.0   0.979980    1.145370   0.440364    3.017063  0.001
57    57.0   0.984985    1.118848   0.464546    2.946768  0.001
58    58.0   0.986938    1.114784   0.451595    2.993368  0.001
59    59.0   0.982178    1.122965   0.458733    2.970187  0.001
60    60.0   0.988037    1.111061   0.463266    2.964409  0.001
61    61.0   0.985352    1.109282   0.468901    2.954134  0.001
62    62.0   0.988647    1.095407   0.465731    2.943841  0.001
63    63.0   0.988281    1.099241   0.465816    2.968189  0.001
64    64.0   0.990723    1.086606   0.469809    2.915358  0.001
65    65.0   0.990967    1.075517   0.477682    2.889809  0.001
66    66.0   0.989258    1.082321   0.471312    2.941521  0.001
67    67.0   0.991211    1.076061   0.465111    2.949761  0.001
68    68.0   0.989990    1.085627   0.473598    2.913264  0.001
69    69.0   0.992676    1.071044   0.483038    2.891428  0.001
70    70.0   0.992798    1.068703   0.469204    2.948376  0.001
71    71.0   0.992920    1.068654   0.479938    2.919990  0.001
72    72.0   0.991089    1.068727   0.479938    2.901753  0.001
73    73.0   0.993286    1.061417   0.477294    2.930708  0.001
74    74.0   0.993530    1.055779   0.479348    2.923521  0.001
75    75.0   0.994385    1.050508   0.483782    2.884905  0.001
76    76.0   0.993408    1.055084   0.488827    2.881064  0.001
77    77.0   0.993774    1.053361   0.471312    2.943081  0.001
78    78.0   0.992554    1.059186   0.474094    2.944686  0.001
79    79.0   0.992432    1.056522   0.485216    2.905349  0.001
80    80.0   0.992188    1.061212   0.468281    2.953152  0.001
81    81.0   0.991211    1.060022   0.469700    2.970149  0.001
82    82.0   0.993530    1.049539   0.480504    2.930757  0.001
83    83.0   0.995117    1.041376   0.472373    2.950997  0.001
84    84.0   0.992920    1.042384   0.497353    2.873934  0.001
85    85.0   0.994263    1.044873   0.484650    2.868455  0.001
86    86.0   0.995728    1.040679   0.490161    2.888438  0.001
87    87.0   0.995850    1.028677   0.495067    2.882980  0.001
88    88.0   0.995361    1.025274   0.476868    2.916906  0.001
89    89.0   0.995117    1.027721   0.502274    2.852217  0.001
90    90.0   0.994507    1.035691   0.493555    2.884807  0.001
91    91.0   0.994629    1.032995   0.488068    2.888893  0.001
92    92.0   0.992798    1.039145   0.487200    2.887813  0.001
93    93.0   0.994141    1.032546   0.498941    2.884351  0.001
94    94.0   0.994995    1.026554   0.496625    2.882228  0.001
95    95.0   0.995850    1.021843   0.500841    2.866597  0.001
96    96.0   0.996094    1.021202   0.507398    2.848311  0.001
97    97.0   0.994751    1.021800   0.490037    2.881457  0.001
98    98.0   0.995972    1.019711   0.504615    2.853470  0.001
99    99.0   0.994873    1.022305   0.501128    2.854255  0.001
100  100.0   0.995728    1.016207   0.495508    2.884153  0.001
101  101.0   0.995850    1.016159   0.495469    2.885237  0.001
102  102.0   0.995605    1.018058   0.508684    2.828340  0.001
103  103.0   0.995972    1.015127   0.500678    2.877578  0.001
104  104.0   0.996460    1.010109   0.501833    2.868827  0.001
105  105.0   0.995117    1.013175   0.499050    2.894842  0.001
106  106.0   0.996460    1.010726   0.508475    2.841751  0.001
107  107.0   0.996338    1.007233   0.504313    2.862438  0.001
108  108.0   0.996094    1.009145   0.512706    2.810833  0.001
109  109.0   0.996216    1.009867   0.509110    2.851424  0.001
110  110.0   0.997070    1.003903   0.502647    2.864068  0.001
111  111.0   0.996216    1.004996   0.505444    2.851319  0.001
112  112.0   0.997070    1.003833   0.508296    2.863418  0.001
113  113.0   0.997559    0.995925   0.508668    2.872889  0.001
114  114.0   0.996582    0.999644   0.511645    2.858582  0.001
115  115.0   0.996948    0.996166   0.519248    2.858511  0.001
116  116.0   0.996704    0.997465   0.519016    2.833048  0.001
117  117.0   0.996826    0.994119   0.515489    2.850729  0.001
118  118.0   0.996826    0.997730   0.510405    2.847361  0.001
119  119.0   0.996704    0.997594   0.510544    2.848778  0.001
120  120.0   0.997070    0.993936   0.515186    2.840980  0.001
121  121.0   0.996582    0.991978   0.515017    2.841542  0.001
122  122.0   0.996582    0.994370   0.517195    2.836996  0.001
123  123.0   0.997070    0.989665   0.507428    2.892785  0.001

EXPERIMENT: C-5
================

NAME: ghost_20200918064112

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'True',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': "{'RandomErasing': {'p': 0.5, 'scale': [0.02, 0.25]}}",
 'weight_decay': 0.0}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.006470    5.431705   0.005208    8.618626  0.001
1      1.0   0.009766    5.409154   0.008681   55.182423  0.001
2      2.0   0.012817    5.358506   0.014028    9.962071  0.001
3      3.0   0.015503    5.269935   0.016330   22.811537  0.001
4      4.0   0.018921    5.241997   0.018299    6.503686  0.001
5      5.0   0.025024    5.139419   0.021399    5.332875  0.001
6      6.0   0.024170    5.073783   0.026374    5.400389  0.001
7      7.0   0.033447    5.001432   0.025794    8.958171  0.001
8      8.0   0.035767    4.948357   0.031885    5.194358  0.001
9      9.0   0.044678    4.863403   0.045100    5.301066  0.001
10    10.0   0.054688    4.817403   0.055020    5.417908  0.001
11    11.0   0.057617    4.738419   0.053339    5.318017  0.001
12    12.0   0.071167    4.624986   0.053091    4.917684  0.001
13    13.0   0.082153    4.536403   0.076706    5.609649  0.001
14    14.0   0.084961    4.495120   0.061484    4.952764  0.001
15    15.0   0.095093    4.418910   0.079326    4.808749  0.001
16    16.0   0.110718    4.298693   0.091494    4.693946  0.001
17    17.0   0.130859    4.184436   0.101623    4.770575  0.001
18    18.0   0.147583    4.092299   0.105150    5.221366  0.001
19    19.0   0.156982    4.016981   0.106568    4.681072  0.001
20    20.0   0.179688    3.902024   0.122635    4.766198  0.001
21    21.0   0.197754    3.816487   0.128339    4.654786  0.001
22    22.0   0.215332    3.744639   0.152675    6.566657  0.001
23    23.0   0.238770    3.609552   0.166782    4.754946  0.001
24    24.0   0.272095    3.473966   0.169292    6.394293  0.001
25    25.0   0.297363    3.362415   0.182685    4.419939  0.001
26    26.0   0.323853    3.251696   0.197139    4.224945  0.001
27    27.0   0.358154    3.109503   0.197303    4.481409  0.001
28    28.0   0.383301    3.015599   0.222268    4.036371  0.001
29    29.0   0.420776    2.869341   0.228002    4.146175  0.001
30    30.0   0.459717    2.740894   0.231281    4.047822  0.001
31    31.0   0.501709    2.609958   0.243558    4.013594  0.001
32    32.0   0.526611    2.523492   0.258632    3.857522  0.001
33    33.0   0.557129    2.429235   0.258175    4.034222  0.001
34    34.0   0.601807    2.316900   0.277288    3.797875  0.001
35    35.0   0.629272    2.228211   0.286782    3.782781  0.001
36    36.0   0.662720    2.133475   0.284728    3.762789  0.001
37    37.0   0.695068    2.046250   0.287774    3.911958  0.001
38    38.0   0.715576    1.979378   0.301748    3.781587  0.001
39    39.0   0.739502    1.919471   0.316713    3.638106  0.001
40    40.0   0.757812    1.860542   0.313513    3.670480  0.001
41    41.0   0.779785    1.795383   0.308692    3.771662  0.001
42    42.0   0.792969    1.754555   0.324595    4.090706  0.001
43    43.0   0.822510    1.693478   0.324402    3.753667  0.001
44    44.0   0.832397    1.647914   0.315280    3.675172  0.001
45    45.0   0.834961    1.643905   0.336058    3.581326  0.001
46    46.0   0.852051    1.591119   0.332982    3.613282  0.001
47    47.0   0.858521    1.572897   0.340701    3.538678  0.001
48    48.0   0.872681    1.527568   0.328781    3.605570  0.001
49    49.0   0.884521    1.500747   0.325751    3.636576  0.001
50    50.0   0.885010    1.492610   0.325339    3.772043  0.001
51    51.0   0.891846    1.473598   0.341941    3.539918  0.001
52    52.0   0.897339    1.461042   0.333950    3.683937  0.001
53    53.0   0.903198    1.435122   0.339570    3.577673  0.001
54    54.0   0.913574    1.405218   0.340592    3.578959  0.001
55    55.0   0.910645    1.405508   0.346986    3.554110  0.001
56    56.0   0.916748    1.389660   0.347398    3.531107  0.001
57    57.0   0.921509    1.380248   0.342685    3.514325  0.001
58    58.0   0.920898    1.370518   0.352884    3.494060  0.001
59    59.0   0.933472    1.342871   0.349382    3.530068  0.001
60    60.0   0.935425    1.332330   0.351435    3.472062  0.001
61    61.0   0.938843    1.316973   0.362542    3.434156  0.001
62    62.0   0.930908    1.334321   0.350731    3.493557  0.001
63    63.0   0.933350    1.319653   0.351738    4.060183  0.001
64    64.0   0.940063    1.300951   0.359466    3.419486  0.001
65    65.0   0.940674    1.296294   0.345716    3.555350  0.001
66    66.0   0.944702    1.286230   0.353489    3.489306  0.001
67    67.0   0.948120    1.276088   0.358558    3.434916  0.001
68    68.0   0.947021    1.271635   0.348638    3.544938  0.001
69    69.0   0.950195    1.262417   0.352497    3.451421  0.001
70    70.0   0.955444    1.253145   0.349134    3.488273  0.001
71    71.0   0.954956    1.252591   0.347452    3.512783  0.001
72    72.0   0.953003    1.253913   0.361317    3.444492  0.001
73    73.0   0.956665    1.245135   0.347800    3.614779  0.001
74    74.0   0.956543    1.243995   0.366936    3.416096  0.001
75    75.0   0.959229    1.235507   0.358489    3.469679  0.001
76    76.0   0.956787    1.235171   0.347135    3.530640  0.001
77    77.0   0.957520    1.229062   0.370161    3.367200  0.001
78    78.0   0.959106    1.219589   0.360821    3.470368  0.001
79    79.0   0.959351    1.222047   0.357799    3.431357  0.001
80    80.0   0.960205    1.214447   0.357566    3.408756  0.001
81    81.0   0.964355    1.200425   0.365037    3.423304  0.001
82    82.0   0.969116    1.197085   0.364774    3.418787  0.001
83    83.0   0.967407    1.188094   0.366867    3.477770  0.001
84    84.0   0.967896    1.188748   0.368107    3.405532  0.001
85    85.0   0.965576    1.194836   0.361907    3.459777  0.001
86    86.0   0.968750    1.185459   0.367998    3.418853  0.001
87    87.0   0.968628    1.182740   0.359000    3.466952  0.001
88    88.0   0.971313    1.177906   0.358186    3.429643  0.001
89    89.0   0.973267    1.166252   0.363380    3.397691  0.001
90    90.0   0.971802    1.173125   0.375106    3.357915  0.001
91    91.0   0.973511    1.170081   0.378663    3.338593  0.001
92    92.0   0.972900    1.161954   0.371540    3.374270  0.001
93    93.0   0.976318    1.150417   0.374486    3.351014  0.001
94    94.0   0.972290    1.152902   0.368533    3.377241  0.001
95    95.0   0.972778    1.158531   0.369734    3.390492  0.001
96    96.0   0.973022    1.154974   0.371113    3.392007  0.001
97    97.0   0.974854    1.161768   0.374571    3.367164  0.001
98    98.0   0.975586    1.145619   0.377656    3.370015  0.001
99    99.0   0.976562    1.145198   0.351366    3.458452  0.001
100  100.0   0.977905    1.132981   0.368836    3.400445  0.001
101  101.0   0.975830    1.136408   0.366480    3.390783  0.001
102  102.0   0.979248    1.135713   0.380771    3.361339  0.001
103  103.0   0.976929    1.136860   0.374338    3.390346  0.001
104  104.0   0.975952    1.138499   0.374323    3.370737  0.001
105  105.0   0.977173    1.139652   0.370533    3.380933  0.001
106  106.0   0.981201    1.122264   0.373370    3.407712  0.001

EXPERIMENT: C-6
================

NAME: ghost_20200918084418

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'True',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': "{'RandomErasing': {'p': 0.5, 'scale': [0.02, 0.25]}}",
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.005005    5.459475   0.005580    5.320951  0.001
1     1.0   0.006592    5.476928   0.012773   24.539845  0.001
2     2.0   0.008545    5.437794   0.015997    5.687128  0.001
3     3.0   0.012939    5.348305   0.018849    8.750640  0.001
4     4.0   0.016968    5.247908   0.019345    5.613522  0.001
5     5.0   0.018311    5.222527   0.020709    5.257682  0.001
6     6.0   0.020508    5.162135   0.018477    7.684744  0.001
7     7.0   0.025513    5.086081   0.031498    5.155905  0.001
8     8.0   0.029297    5.026153   0.029886    6.724453  0.001
9     9.0   0.030762    4.981938   0.031126    7.041797  0.001
10   10.0   0.039795    4.931331   0.036706    5.269064  0.001
11   11.0   0.040771    4.877355   0.046325   11.335662  0.001
12   12.0   0.038330    4.924245   0.037093    5.044043  0.001
13   13.0   0.049316    4.820688   0.048160    5.003651  0.001
14   14.0   0.056519    4.741050   0.050347    5.443281  0.001
15   15.0   0.060913    4.692543   0.061221    5.094218  0.001
16   16.0   0.074341    4.605232   0.061453    6.232272  0.001
17   17.0   0.078613    4.567897   0.060422    5.598373  0.001
18   18.0   0.091309    4.471908   0.072614    4.908795  0.001
19   19.0   0.088013    4.452366   0.088632    4.615955  0.001
20   20.0   0.102051    4.351555   0.075233    5.015048  0.001
21   21.0   0.111328    4.285941   0.088765    5.227499  0.001
22   22.0   0.127319    4.202360   0.099669    4.564867  0.001
23   23.0   0.140259    4.103829   0.116558    4.709773  0.001
24   24.0   0.155029    4.016131   0.121355    5.131240  0.001
25   25.0   0.181519    3.918444   0.124525    6.068792  0.001
26   26.0   0.194702    3.823875   0.150086    5.322725  0.001
27   27.0   0.218506    3.719560   0.157248    5.042780  0.001
28   28.0   0.241821    3.611430   0.171252    6.852537  0.001
29   29.0   0.260742    3.516082   0.188226    5.095197  0.001
30   30.0   0.295410    3.388896   0.173587    4.589674  0.001
31   31.0   0.320801    3.271328   0.204371    4.369833  0.001
32   32.0   0.343506    3.177720   0.225846    5.749091  0.001
33   33.0   0.383545    3.045142   0.217144    4.365376  0.001
34   34.0   0.411133    2.946492   0.224887    5.198431  0.001
35   35.0   0.444336    2.820678   0.249471    5.322788  0.001
36   36.0   0.480835    2.698683   0.258082    4.378734  0.001
37   37.0   0.508789    2.596405   0.269808    3.982958  0.001
38   38.0   0.542358    2.498403   0.273132    5.767209  0.001
39   39.0   0.569336    2.414869   0.294564    3.734607  0.001
40   40.0   0.592163    2.334870   0.286951    3.899287  0.001
41   41.0   0.630737    2.234892   0.308746    3.639251  0.001
42   42.0   0.659668    2.153733   0.309660    5.600717  0.001
43   43.0   0.677490    2.090181   0.308042    4.173383  0.001
44   44.0   0.703735    2.013978   0.319714    4.075181  0.001
45   45.0   0.713135    1.994630   0.303190    5.386250  0.001
46   46.0   0.747681    1.901486   0.323192    4.143929  0.001
47   47.0   0.769531    1.824611   0.349273    4.226950  0.001
48   48.0   0.781738    1.785034   0.337934    4.012174  0.001
49   49.0   0.796753    1.758348   0.339888    4.661942  0.001
50   50.0   0.818481    1.703996   0.344143    3.950817  0.001
51   51.0   0.823364    1.670754   0.333727    3.746991  0.001
52   52.0   0.830322    1.647379   0.347903    3.833689  0.001
53   53.0   0.848633    1.600236   0.355567    4.134887  0.001
54   54.0   0.853516    1.584435   0.372680    3.490432  0.001
55   55.0   0.864746    1.558782   0.367532    3.492620  0.001
56   56.0   0.867554    1.549297   0.372735    3.599507  0.001
57   57.0   0.883545    1.497717   0.369656    3.481207  0.001
58   58.0   0.884888    1.490991   0.372695    3.452376  0.001
59   59.0   0.894775    1.466473   0.370339    3.889741  0.001
60   60.0   0.902588    1.440701   0.376291    3.879732  0.001
61   61.0   0.896851    1.449065   0.376446    3.586492  0.001
62   62.0   0.904785    1.426095   0.378469    3.792041  0.001
63   63.0   0.907959    1.412373   0.364744    4.281588  0.001
64   64.0   0.915161    1.398437   0.365246    4.602056  0.001
65   65.0   0.912231    1.401935   0.363836    3.840884  0.001
66   66.0   0.921631    1.368271   0.375082    3.691522  0.001
67   67.0   0.925537    1.367080   0.375880    3.714222  0.001
68   68.0   0.924316    1.366265   0.385220    3.669213  0.001
69   69.0   0.936646    1.330063   0.380112    3.695102  0.001
70   70.0   0.934692    1.328662   0.381004    3.622965  0.001
71   71.0   0.936768    1.314811   0.386793    3.675424  0.001
72   72.0   0.936523    1.315230   0.375532    3.841490  0.001
73   73.0   0.933960    1.316368   0.383315    3.557041  0.001

EXPERIMENT: C-7
================

NAME: ghost_20200918105227

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.007812    5.423789   0.012897    6.830509  0.001
1      1.0   0.011597    5.356699   0.011781    6.915704  0.001
2      2.0   0.013062    5.347097   0.015501    5.246624  0.001
3      3.0   0.016724    5.290235   0.019841    7.754148  0.001
4      4.0   0.022827    5.203609   0.025273    5.145411  0.001
5      5.0   0.029175    5.108306   0.030095    5.323426  0.001
6      6.0   0.032959    5.039948   0.035660    6.083528  0.001
7      7.0   0.042603    4.922546   0.041047    5.446425  0.001
8      8.0   0.050049    4.776648   0.051013    5.217811  0.001
9      9.0   0.057495    4.681333   0.057857    4.982926  0.001
10    10.0   0.068848    4.573017   0.064003    4.806262  0.001
11    11.0   0.087402    4.459341   0.063631    5.131130  0.001
12    12.0   0.089600    4.436603   0.071746    5.900247  0.001
13    13.0   0.102051    4.334929   0.083200    4.718290  0.001
14    14.0   0.123291    4.189861   0.108583    4.405200  0.001
15    15.0   0.141357    4.050386   0.111513    4.441833  0.001
16    16.0   0.161865    3.948713   0.117342    4.415579  0.001
17    17.0   0.177979    3.866515   0.129455    4.242622  0.001
18    18.0   0.192505    3.790809   0.153776    4.122041  0.001
19    19.0   0.223999    3.629401   0.170106    4.174814  0.001
20    20.0   0.253784    3.494108   0.171772    4.274932  0.001
21    21.0   0.289307    3.353281   0.196937    4.186425  0.001
22    22.0   0.327393    3.211887   0.215771    3.860883  0.001
23    23.0   0.366943    3.049473   0.228861    3.971240  0.001
24    24.0   0.396484    2.915653   0.262815    3.738001  0.001
25    25.0   0.443726    2.777773   0.248687    4.398669  0.001
26    26.0   0.492188    2.600003   0.289020    4.113207  0.001
27    27.0   0.538574    2.469540   0.297056    3.677993  0.001
28    28.0   0.572632    2.348168   0.294461    3.860878  0.001
29    29.0   0.616699    2.224971   0.328902    3.459264  0.001
30    30.0   0.653687    2.112159   0.342567    3.516114  0.001
31    31.0   0.694092    2.005417   0.358401    3.459205  0.001
32    32.0   0.719604    1.924257   0.359287    3.351938  0.001
33    33.0   0.763428    1.829767   0.376174    3.268406  0.001
34    34.0   0.792847    1.749523   0.385707    3.387256  0.001
35    35.0   0.814819    1.676032   0.387582    3.220569  0.001
36    36.0   0.834839    1.613859   0.400013    3.239433  0.001
37    37.0   0.848511    1.576283   0.407756    3.238776  0.001
38    38.0   0.883667    1.502594   0.389125    3.218631  0.001
39    39.0   0.890137    1.473598   0.408440    3.199309  0.001
40    40.0   0.902466    1.432043   0.419359    3.112056  0.001
41    41.0   0.911499    1.403177   0.415672    3.108828  0.001
42    42.0   0.926758    1.363494   0.429651    3.063403  0.001
43    43.0   0.935913    1.332614   0.426923    3.058443  0.001
44    44.0   0.942871    1.308239   0.432364    3.068506  0.001
45    45.0   0.949219    1.290991   0.437588    3.068693  0.001
46    46.0   0.946777    1.288649   0.430008    3.075874  0.001
47    47.0   0.954468    1.276319   0.446308    3.053431  0.001
48    48.0   0.955078    1.259296   0.433495    3.060856  0.001
49    49.0   0.962891    1.235926   0.443471    3.016496  0.001
50    50.0   0.969360    1.222148   0.449477    2.992284  0.001
51    51.0   0.967773    1.215487   0.453793    2.976606  0.001
52    52.0   0.968994    1.206672   0.440370    3.008654  0.001
53    53.0   0.970093    1.197413   0.440945    3.047235  0.001
54    54.0   0.972046    1.184881   0.460569    2.950652  0.001
55    55.0   0.971558    1.191313   0.449780    2.998825  0.001
56    56.0   0.975586    1.181181   0.469170    2.928002  0.001
57    57.0   0.980225    1.159720   0.466218    2.917619  0.001
58    58.0   0.981689    1.149844   0.465574    2.929215  0.001
59    59.0   0.981445    1.146317   0.452617    2.980951  0.001
60    60.0   0.984863    1.129361   0.471412    2.918556  0.001
61    61.0   0.983521    1.128218   0.473852    2.893778  0.001
62    62.0   0.983154    1.137751   0.456655    2.965598  0.001
63    63.0   0.985718    1.124955   0.472240    2.911564  0.001
64    64.0   0.980591    1.138816   0.461437    2.958444  0.001
65    65.0   0.986450    1.124140   0.471784    2.951023  0.001
66    66.0   0.984741    1.129835   0.476348    2.893687  0.001
67    67.0   0.986084    1.116213   0.476938    2.884652  0.001
68    68.0   0.987061    1.110782   0.476239    2.901475  0.001
69    69.0   0.989014    1.108976   0.475589    2.919235  0.001
70    70.0   0.989380    1.097344   0.479254    2.882535  0.001
71    71.0   0.989014    1.095940   0.479890    2.905405  0.001
72    72.0   0.989502    1.093851   0.477697    2.923501  0.001
73    73.0   0.989624    1.086266   0.480246    2.911691  0.001
74    74.0   0.990479    1.087159   0.485316    2.873343  0.001
75    75.0   0.987183    1.091025   0.494438    2.863615  0.001
76    76.0   0.991455    1.082635   0.493189    2.855314  0.001
77    77.0   0.989258    1.081966   0.480292    2.918989  0.001
78    78.0   0.989136    1.081273   0.484865    2.901639  0.001
79    79.0   0.993042    1.068030   0.487632    2.879636  0.001
80    80.0   0.991699    1.067282   0.477836    2.937509  0.001
81    81.0   0.993652    1.063368   0.497042    2.849724  0.001
82    82.0   0.993652    1.066183   0.490361    2.858097  0.001
83    83.0   0.991943    1.062619   0.495306    2.852796  0.001
84    84.0   0.991699    1.062334   0.497112    2.840591  0.001
85    85.0   0.993164    1.058656   0.499949    2.882745  0.001
86    86.0   0.992554    1.062716   0.507450    2.820331  0.001
87    87.0   0.991943    1.056586   0.488446    2.883703  0.001
88    88.0   0.993286    1.053565   0.510807    2.814212  0.001
89    89.0   0.994019    1.050902   0.493004    2.859087  0.001
90    90.0   0.993896    1.053643   0.503257    2.822701  0.001
91    91.0   0.992676    1.055319   0.484269    2.912736  0.001
92    92.0   0.993530    1.050114   0.509443    2.818748  0.001
93    93.0   0.994995    1.046706   0.516983    2.782268  0.001
94    94.0   0.994263    1.042681   0.509899    2.792420  0.001
95    95.0   0.995361    1.042337   0.508094    2.808594  0.001
96    96.0   0.993774    1.040784   0.502513    2.858086  0.001
97    97.0   0.994507    1.040441   0.510108    2.825957  0.001
98    98.0   0.994873    1.036627   0.513317    2.804967  0.001
99    99.0   0.994507    1.038959   0.509225    2.847046  0.001
100  100.0   0.995239    1.036314   0.516923    2.792500  0.001
101  101.0   0.994995    1.032154   0.516572    2.815626  0.001
102  102.0   0.995850    1.030734   0.525896    2.757885  0.001
103  103.0   0.995361    1.029112   0.516293    2.810876  0.001
104  104.0   0.994263    1.029017   0.505505    2.866972  0.001
105  105.0   0.995483    1.027084   0.511914    2.832689  0.001
106  106.0   0.995972    1.021665   0.516759    2.824125  0.001
107  107.0   0.996094    1.020743   0.524439    2.794836  0.001
108  108.0   0.995605    1.020387   0.526586    2.746697  0.001
109  109.0   0.995361    1.019200   0.532042    2.782863  0.001
110  110.0   0.995605    1.024880   0.519890    2.822359  0.001
111  111.0   0.995483    1.021731   0.512906    2.851366  0.001
112  112.0   0.995117    1.025098   0.518223    2.832674  0.001
113  113.0   0.996216    1.015625   0.531849    2.744354  0.001
114  114.0   0.996216    1.015168   0.520262    2.814043  0.001
115  115.0   0.995361    1.019942   0.525152    2.796823  0.001
116  116.0   0.995483    1.016239   0.527454    2.786278  0.001
117  117.0   0.995850    1.013244   0.518634    2.819306  0.001
118  118.0   0.995239    1.017965   0.536328    2.769777  0.001
119  119.0   0.996460    1.009408   0.526516    2.779734  0.001
120  120.0   0.996094    1.006843   0.523967    2.815977  0.001
121  121.0   0.996094    1.013400   0.528966    2.788048  0.001
122  122.0   0.996094    1.013601   0.522618    2.829409  0.001
123  123.0   0.996826    1.010414   0.534275    2.774037  0.001
124  124.0   0.995850    1.006949   0.522718    2.828648  0.001
125  125.0   0.995972    1.003600   0.533670    2.789901  0.001
126  126.0   0.996460    1.006469   0.537127    2.772603  0.001
127  127.0   0.996338    1.006878   0.526477    2.818578  0.001
128  128.0   0.997314    1.003376   0.542762    2.748195  0.001

EXPERIMENT: C-8
================

NAME: ghost_20200918141836

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'False',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'True',
 'image_augmentations': 'None',
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': 'None',
 'normalize': 'False',
 'num_params': 4152804.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.006714    5.470583   0.005332   23.122660  0.001
1     1.0   0.011230    5.462960   0.007192    7.442928  0.001
2     2.0   0.012573    5.377624   0.014137    7.230180  0.001
3     3.0   0.015137    5.288579   0.014881    6.472108  0.001
4     4.0   0.020996    5.196102   0.021072    5.941271  0.001
5     5.0   0.026733    5.084447   0.029565    5.610568  0.001
6     6.0   0.039429    4.923915   0.022391    6.482081  0.001
7     7.0   0.050781    4.780357   0.031776    5.423011  0.001
8     8.0   0.072632    4.590136   0.035209    5.289838  0.001
9     9.0   0.090698    4.413429   0.035557    5.367686  0.001
10   10.0   0.118896    4.218332   0.039014    5.226730  0.001
11   11.0   0.158081    3.982313   0.048067    5.080396  0.001
12   12.0   0.201904    3.760509   0.051702    5.430696  0.001
13   13.0   0.258545    3.486759   0.048517    5.422409  0.001
14   14.0   0.344360    3.188770   0.054198    5.756992  0.001
15   15.0   0.412598    2.935182   0.057531    5.498957  0.001
16   16.0   0.493652    2.634994   0.056678    6.142309  0.001
17   17.0   0.613281    2.273258   0.052610    5.946273  0.001
18   18.0   0.719116    1.995624   0.048681    6.134017  0.001
19   19.0   0.829834    1.689140   0.065815    6.096978  0.001
20   20.0   0.909058    1.470952   0.059336    5.793037  0.001
21   21.0   0.948364    1.342317   0.057942    5.766651  0.001
22   22.0   0.976929    1.223680   0.063265    5.669284  0.001
23   23.0   0.986084    1.177976   0.062367    5.505361  0.001
24   24.0   0.989990    1.142984   0.063607    5.540793  0.001
25   25.0   0.994873    1.106227   0.064669    5.387815  0.001
26   26.0   0.994873    1.089105   0.061212    5.344160  0.001

EXPERIMENT: C-9
================

NAME: ghost_20200918155129

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'False',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'True',
 'image_augmentations': 'None',
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': "{'mean': [0.4627], 'std': [0.2545]}",
 'norm_params_rgb': 'None',
 'normalize': 'True',
 'num_params': 4152804.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.004639    5.479936   0.009177    9.838858  0.001
1     1.0   0.010498    5.444108   0.012029   22.322916  0.001
2     2.0   0.012939    5.385576   0.011657    7.828223  0.001
3     3.0   0.018677    5.305152   0.017376    6.042882  0.001
4     4.0   0.025269    5.201563   0.022969    6.257030  0.001
5     5.0   0.030151    5.045723   0.012988    6.263273  0.001
6     6.0   0.038330    4.857925   0.028622    6.176616  0.001
7     7.0   0.052124    4.709769   0.029475    5.135077  0.001
8     8.0   0.067261    4.541743   0.053810    4.966052  0.001
9     9.0   0.092773    4.328806   0.039891    4.990361  0.001
10   10.0   0.129272    4.138951   0.063598    4.745642  0.001
11   11.0   0.172363    3.900930   0.046905    5.356222  0.001
12   12.0   0.219360    3.660214   0.063870    5.034630  0.001
13   13.0   0.267700    3.439363   0.059421    5.116571  0.001
14   14.0   0.344238    3.153628   0.062530    5.215391  0.001
15   15.0   0.422485    2.870011   0.072148    5.502146  0.001
16   16.0   0.514282    2.580155   0.073984    5.510864  0.001
17   17.0   0.617676    2.266095   0.076147    5.629403  0.001
18   18.0   0.715210    1.992433   0.070561    5.811615  0.001
19   19.0   0.807495    1.745716   0.076722    5.484990  0.001
20   20.0   0.880371    1.545670   0.085487    5.392884  0.001
21   21.0   0.923950    1.413421   0.084277    5.270107  0.001
22   22.0   0.947266    1.323929   0.080309    5.401593  0.001
23   23.0   0.957764    1.280899   0.080348    5.288489  0.001
24   24.0   0.962402    1.236553   0.086757    5.225760  0.001
25   25.0   0.971313    1.207076   0.083627    5.288832  0.001

EXPERIMENT: C-10
================

NAME: ghost_20200918141437

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'True',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.004883    5.485001   0.006696    5.619728  0.001
1     1.0   0.006592    5.465528   0.006766   11.843990  0.001
2     2.0   0.008667    5.436234   0.008557    5.481352  0.001
3     3.0   0.009155    5.397369   0.006324   22.140327  0.001
4     4.0   0.007812    5.339178   0.008681    6.438395  0.001
5     5.0   0.010986    5.320844   0.007316    6.824864  0.001
6     6.0   0.012451    5.281994   0.009549    5.232718  0.001
7     7.0   0.015137    5.250947   0.009370    5.422732  0.001
8     8.0   0.013672    5.209312   0.015005    5.318941  0.001
9     9.0   0.013428    5.197742   0.016369    5.263297  0.001
10   10.0   0.015503    5.179026   0.014881    5.204898  0.001
11   11.0   0.015015    5.156552   0.017981    5.312144  0.001
12   12.0   0.017578    5.134349   0.017183    5.277436  0.001
13   13.0   0.020996    5.120617   0.017857    5.168840  0.001
14   14.0   0.022461    5.106397   0.021329    5.347131  0.001
15   15.0   0.023560    5.087378   0.018795    5.373761  0.001
16   16.0   0.021484    5.076038   0.022391    5.232367  0.001
17   17.0   0.024658    5.061090   0.019469    5.243363  0.001
18   18.0   0.027222    5.040787   0.024623    5.181732  0.001
19   19.0   0.026855    5.025491   0.024499    5.173792  0.001
20   20.0   0.027466    5.009772   0.026042    5.381930  0.001
21   21.0   0.029053    4.999712   0.030452    5.162601  0.001
22   22.0   0.027100    4.977689   0.020461    5.228913  0.001
23   23.0   0.025269    4.999258   0.027366    5.144000  0.001
24   24.0   0.029541    4.982195   0.027103    5.141973  0.001
25   25.0   0.034546    4.970389   0.026483    5.208669  0.001
26   26.0   0.032837    4.950446   0.027847    5.145701  0.001
27   27.0   0.036377    4.919949   0.024871    5.349466  0.001
28   28.0   0.033569    4.962920   0.027227    5.199702  0.001
29   29.0   0.036255    4.902846   0.032257    5.145130  0.001
30   30.0   0.040771    4.861288   0.028343    5.423996  0.001
31   31.0   0.040649    4.867937   0.032009    5.146933  0.001
32   32.0   0.047241    4.818889   0.030095    5.231778  0.001
33   33.0   0.047729    4.772897   0.032188    5.241595  0.001
34   34.0   0.052734    4.739482   0.035233    5.233582  0.001
35   35.0   0.055664    4.711755   0.030893    5.794448  0.001
36   36.0   0.058594    4.678667   0.038155    5.318707  0.001
37   37.0   0.063843    4.646827   0.032381    5.376264  0.001
38   38.0   0.071289    4.607943   0.033001    5.450357  0.001
39   39.0   0.075806    4.569676   0.039078    5.242525  0.001

EXPERIMENT: C-11
================

NAME: ghost_20200918141630

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'True',
 'dropout': 0.2,
 'erase_background': 'True',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.005981    5.463923   0.005084    5.354785  0.001
1     1.0   0.006348    5.467912   0.006076    5.350529  0.001
2     2.0   0.007324    5.457730   0.007882   20.104151  0.001
3     3.0   0.007935    5.425858   0.006696    5.529812  0.001
4     4.0   0.006714    5.362312   0.007812    5.309980  0.001
5     5.0   0.010742    5.351423   0.006890    5.942167  0.001
6     6.0   0.009277    5.327190   0.004340   27.827324  0.001
7     7.0   0.010498    5.288680   0.009053    5.288560  0.001
8     8.0   0.008301    5.254216   0.009425    6.068656  0.001
9     9.0   0.010620    5.236046   0.007316    7.290512  0.001
10   10.0   0.011108    5.220918   0.011230    5.577558  0.001
11   11.0   0.010376    5.195941   0.008750    7.638970  0.001
12   12.0   0.012207    5.188783   0.006200    6.845624  0.001
13   13.0   0.013916    5.181145   0.013021    7.150334  0.001
14   14.0   0.013672    5.178813   0.008681    6.826929  0.001
15   15.0   0.012695    5.179420   0.008557    6.220959  0.001
16   16.0   0.013916    5.167533   0.010789    5.789257  0.001
17   17.0   0.015015    5.158785   0.008572    9.413090  0.001
18   18.0   0.015625    5.148605   0.010734   11.082758  0.001
19   19.0   0.015869    5.147326   0.008929    5.672309  0.001
20   20.0   0.016357    5.144720   0.013517    5.289077  0.001
21   21.0   0.016357    5.129812   0.013393    6.250109  0.001
22   22.0   0.016602    5.131394   0.014524    5.259003  0.001
23   23.0   0.017456    5.123381   0.016191    6.693768  0.001
24   24.0   0.014282    5.195494   0.011657    5.320937  0.001
25   25.0   0.014038    5.161750   0.013145    5.257469  0.001
26   26.0   0.017944    5.128680   0.016617    5.251497  0.001
27   27.0   0.017822    5.114366   0.013765    5.208095  0.001
28   28.0   0.019531    5.098780   0.015377    5.195699  0.001
29   29.0   0.018188    5.085842   0.016811    5.201885  0.001
30   30.0   0.020264    5.074399   0.015144    5.307059  0.001
31   31.0   0.027100    5.056726   0.017803    5.262593  0.001
32   32.0   0.022095    5.051196   0.019787    5.344688  0.001
33   33.0   0.023315    5.030403   0.019608    5.335444  0.001
34   34.0   0.026123    5.017601   0.023507    5.303881  0.001
35   35.0   0.029053    4.998322   0.022089    5.238195  0.001
36   36.0   0.029785    4.981444   0.024747    5.229210  0.001
37   37.0   0.030151    4.965133   0.023189    5.364013  0.001
38   38.0   0.033325    4.945208   0.025754    5.272446  0.001
39   39.0   0.034668    4.932286   0.024747    5.374704  0.001
40   40.0   0.037842    4.903005   0.026925    5.477488  0.001
41   41.0   0.040527    4.876950   0.030700    5.417430  0.001
42   42.0   0.043579    4.860471   0.026925    5.484652  0.001
43   43.0   0.042603    4.822314   0.029157    5.458646  0.001

EXPERIMENT: C-12
================

NAME: ghost_20200918162955

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'True',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': "{'mean': [0.4627], 'std': [0.2545]}",
 'norm_params_rgb': 'None',
 'normalize': 'True',
 'num_params': 4152804.0,
 'optimizer': 'Adam',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.0}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.004395    5.490562   0.005332   20.745226  0.001
1      1.0   0.005737    5.506803   0.009425    6.571481  0.001
2      2.0   0.009399    5.460087   0.010045    5.926360  0.001
3      3.0   0.012207    5.451762   0.014082    6.297807  0.001
4      4.0   0.015747    5.341673   0.013641    6.774528  0.001
5      5.0   0.018799    5.263966   0.016121   11.385105  0.001
6      6.0   0.020142    5.187335   0.015616    7.466463  0.001
7      7.0   0.022827    5.116260   0.022709    5.588329  0.001
8      8.0   0.027222    5.057246   0.026607    5.389940  0.001
9      9.0   0.035645    4.966884   0.028924    5.083446  0.001
10    10.0   0.037964    4.873650   0.035760    5.503383  0.001
11    11.0   0.048462    4.759586   0.027754    7.356065  0.001
12    12.0   0.067383    4.663761   0.058151    5.266762  0.001
13    13.0   0.073608    4.544118   0.070960    4.666406  0.001
14    14.0   0.090210    4.411651   0.070155    5.437828  0.001
15    15.0   0.105347    4.283511   0.086222    5.064416  0.001
16    16.0   0.132568    4.148180   0.078385    6.051191  0.001
17    17.0   0.153442    4.014050   0.125765    4.590986  0.001
18    18.0   0.177979    3.870481   0.143329    4.255028  0.001
19    19.0   0.216431    3.706161   0.149496    4.167088  0.001
20    20.0   0.223511    3.675683   0.095362    4.976120  0.001
21    21.0   0.276978    3.457980   0.206080    3.950875  0.001
22    22.0   0.311768    3.286643   0.190627    4.377297  0.001
23    23.0   0.350464    3.135988   0.217552    5.256140  0.001
24    24.0   0.400269    2.950103   0.221142    4.043247  0.001
25    25.0   0.443604    2.798391   0.269418    4.036833  0.001
26    26.0   0.489990    2.635443   0.270628   17.971611  0.001
27    27.0   0.534790    2.490340   0.305658    3.934616  0.001
28    28.0   0.573486    2.364053   0.300033    4.987436  0.001
29    29.0   0.616211    2.236091   0.335623    4.412070  0.001
30    30.0   0.656372    2.110627   0.357448    4.186444  0.001
31    31.0   0.691772    2.018705   0.343807    3.451406  0.001
32    32.0   0.729492    1.916849   0.352394    5.101917  0.001
33    33.0   0.761353    1.823719   0.370726    3.580698  0.001
34    34.0   0.792480    1.753680   0.390737    3.242337  0.001
35    35.0   0.826416    1.650649   0.389814    3.235201  0.001
36    36.0   0.849609    1.601789   0.391659    3.262429  0.001
37    37.0   0.859497    1.558689   0.406734    3.149317  0.001
38    38.0   0.883667    1.490229   0.397890    3.203917  0.001
39    39.0   0.900513    1.450880   0.401650    3.220445  0.001
40    40.0   0.912598    1.412740   0.395534    3.242622  0.001
41    41.0   0.924927    1.385823   0.397790    3.217263  0.001
42    42.0   0.932861    1.349361   0.405315    3.157679  0.001
43    43.0   0.942505    1.317543   0.422428    3.104376  0.001
44    44.0   0.945312    1.308186   0.419407    3.113862  0.001
45    45.0   0.952515    1.286501   0.418400    3.132858  0.001
46    46.0   0.956665    1.270980   0.427047    3.066096  0.001
47    47.0   0.961304    1.250358   0.434512    3.042225  0.001
48    48.0   0.962646    1.240247   0.439805    3.053770  0.001
49    49.0   0.968140    1.222004   0.437884    3.042146  0.001
50    50.0   0.973877    1.211508   0.447687    3.033524  0.001
51    51.0   0.970093    1.199127   0.430791    3.049487  0.001
52    52.0   0.972290    1.202090   0.449904    2.984450  0.001
53    53.0   0.978027    1.188190   0.431163    3.066187  0.001
54    54.0   0.978271    1.171037   0.443480    3.030064  0.001
55    55.0   0.979248    1.163044   0.453724    2.989917  0.001
56    56.0   0.980103    1.155024   0.442990    3.017679  0.001
57    57.0   0.981812    1.149990   0.453585    2.973648  0.001
58    58.0   0.983521    1.144140   0.445524    3.011970  0.001
59    59.0   0.983032    1.138270   0.447741    3.013294  0.001
60    60.0   0.984741    1.144137   0.443108    2.984026  0.001
61    61.0   0.985107    1.133418   0.458064    2.941293  0.001
62    62.0   0.986084    1.125345   0.454477    2.977621  0.001
63    63.0   0.987671    1.119735   0.462964    2.947763  0.001
64    64.0   0.986450    1.110328   0.462126    2.938475  0.001
65    65.0   0.988770    1.104095   0.470668    2.943146  0.001
66    66.0   0.987427    1.110640   0.467428    2.929680  0.001
67    67.0   0.987793    1.105531   0.418103    3.152461  0.001
68    68.0   0.990234    1.095343   0.471139    2.941033  0.001
69    69.0   0.990845    1.084885   0.463614    2.949375  0.001
70    70.0   0.993286    1.074597   0.479348    2.912434  0.001
71    71.0   0.991577    1.084045   0.471962    2.926436  0.001
72    72.0   0.993164    1.076161   0.467806    2.966699  0.001
73    73.0   0.993530    1.074188   0.484045    2.897561  0.001
74    74.0   0.993042    1.076270   0.482355    2.901791  0.001
75    75.0   0.991943    1.070476   0.480773    2.910970  0.001
76    76.0   0.994141    1.061723   0.458391    2.979796  0.001
77    77.0   0.993164    1.068863   0.471923    2.954821  0.001
78    78.0   0.993896    1.062079   0.476898    2.929636  0.001
79    79.0   0.992432    1.063425   0.477046    2.935840  0.001
80    80.0   0.993530    1.058521   0.473828    2.922401  0.001
81    81.0   0.992554    1.064622   0.490454    2.867352  0.001
82    82.0   0.993042    1.059854   0.476417    2.926082  0.001
83    83.0   0.993286    1.058798   0.472389    2.940853  0.001
84    84.0   0.993530    1.051118   0.493555    2.868397  0.001
85    85.0   0.994751    1.048491   0.483316    2.898721  0.001
86    86.0   0.993530    1.046849   0.475930    2.940690  0.001
87    87.0   0.994263    1.044407   0.477721    2.949131  0.001
88    88.0   0.994995    1.046753   0.461204    3.018192  0.001
89    89.0   0.995239    1.043396   0.505087    2.843196  0.001
90    90.0   0.994385    1.040463   0.486332    2.911019  0.001
91    91.0   0.995361    1.033616   0.488842    2.902434  0.001
92    92.0   0.995483    1.035558   0.481913    2.951905  0.001
93    93.0   0.995605    1.035641   0.499483    2.877424  0.001
94    94.0   0.995728    1.033961   0.499265    2.858835  0.001
95    95.0   0.994873    1.036849   0.491446    2.918366  0.001
96    96.0   0.995972    1.033919   0.493307    2.913029  0.001
97    97.0   0.994751    1.031690   0.496724    2.901205  0.001
98    98.0   0.996704    1.027816   0.502305    2.879913  0.001
99    99.0   0.995728    1.022476   0.499329    2.890459  0.001
100  100.0   0.995605    1.023214   0.495569    2.893454  0.001
101  101.0   0.995850    1.021769   0.487626    2.906805  0.001
102  102.0   0.995972    1.024653   0.498367    2.895883  0.001
103  103.0   0.995605    1.024704   0.502405    2.868533  0.001
104  104.0   0.995972    1.016934   0.505039    2.890769  0.001

EXPERIMENT: C-13
================

NAME: ghost_20200918184244

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.1}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.1}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.006592    5.445792   0.008681   17.160057  0.001
1      1.0   0.010010    5.418169   0.016493   10.111145  0.001
2      2.0   0.011597    5.382802   0.010665    6.346486  0.001
3      3.0   0.012573    5.269672   0.016369    6.976832  0.001
4      4.0   0.017944    5.194145   0.021329    5.295397  0.001
5      5.0   0.022949    5.139536   0.024554    5.912764  0.001
6      6.0   0.024170    5.072550   0.024926    6.187244  0.001
7      7.0   0.030151    5.019659   0.026042    5.346576  0.001
8      8.0   0.032959    4.958475   0.039450    5.225513  0.001
9      9.0   0.038818    4.887226   0.032257    6.077888  0.001
10    10.0   0.047485    4.817233   0.030878    5.265643  0.001
11    11.0   0.054810    4.727204   0.040620    5.064271  0.001
12    12.0   0.066406    4.623140   0.061097    6.802234  0.001
13    13.0   0.076538    4.518026   0.069777    5.165895  0.001
14    14.0   0.086182    4.402438   0.071692    5.128315  0.001
15    15.0   0.103516    4.303494   0.086642    4.765903  0.001
16    16.0   0.125244    4.175733   0.103181    4.990865  0.001
17    17.0   0.148804    4.033173   0.108250    5.319735  0.001
18    18.0   0.168335    3.912319   0.151326    4.279798  0.001
19    19.0   0.198120    3.789229   0.124973    5.755716  0.001
20    20.0   0.230103    3.627244   0.185102    4.840435  0.001
21    21.0   0.255127    3.489827   0.204997    4.665112  0.001
22    22.0   0.299072    3.309956   0.228752    3.897459  0.001
23    23.0   0.340454    3.179190   0.249631    3.766759  0.001
24    24.0   0.381592    3.020187   0.270900    3.561861  0.001
25    25.0   0.415039    2.878863   0.284014    3.497325  0.001
26    26.0   0.460449    2.722162   0.319665    3.368805  0.001
27    27.0   0.498535    2.589360   0.310767    3.399891  0.001
28    28.0   0.534912    2.487210   0.353287    3.225705  0.001
29    29.0   0.574829    2.363868   0.378088    3.163493  0.001
30    30.0   0.615356    2.245362   0.387349    3.107611  0.001
31    31.0   0.643188    2.133917   0.416065    3.047959  0.001
32    32.0   0.680054    2.035939   0.418103    3.015659  0.001
33    33.0   0.714722    1.951627   0.434215    2.935568  0.001
34    34.0   0.751953    1.849031   0.417151    3.064015  0.001
35    35.0   0.767456    1.789586   0.447895    2.898154  0.001
36    36.0   0.794189    1.725316   0.453594    2.892539  0.001
37    37.0   0.813232    1.654661   0.477558    2.807546  0.001
38    38.0   0.845947    1.579881   0.494166    2.740275  0.001
39    39.0   0.872314    1.523728   0.473263    2.819612  0.001
40    40.0   0.886597    1.474259   0.508675    2.685740  0.001
41    41.0   0.899170    1.432170   0.498064    2.713021  0.001
42    42.0   0.897827    1.429062   0.510326    2.689511  0.001
43    43.0   0.912964    1.382627   0.503133    2.706838  0.001
44    44.0   0.926270    1.348375   0.512821    2.707666  0.001
45    45.0   0.937500    1.320558   0.501536    2.726916  0.001
46    46.0   0.941650    1.298526   0.519820    2.666020  0.001
47    47.0   0.950195    1.266458   0.520579    2.664709  0.001
48    48.0   0.948975    1.272296   0.524463    2.666935  0.001
49    49.0   0.953735    1.254219   0.542474    2.596797  0.001
50    50.0   0.956909    1.238490   0.527826    2.621179  0.001
51    51.0   0.964355    1.217261   0.543055    2.580346  0.001
52    52.0   0.967041    1.196954   0.522890    2.653974  0.001
53    53.0   0.971191    1.185944   0.545847    2.570214  0.001
54    54.0   0.966919    1.189399   0.541428    2.583664  0.001
55    55.0   0.971069    1.186990   0.553689    2.522712  0.001
56    56.0   0.972290    1.174021   0.554721    2.532362  0.001
57    57.0   0.976440    1.160791   0.547559    2.592339  0.001
58    58.0   0.974731    1.152017   0.540212    2.574154  0.001
59    59.0   0.977051    1.143654   0.547032    2.560306  0.001
60    60.0   0.973755    1.160682   0.555782    2.554157  0.001
61    61.0   0.977539    1.147706   0.551615    2.561131  0.001
62    62.0   0.977783    1.138944   0.564021    2.534124  0.001
63    63.0   0.981079    1.136593   0.558511    2.528451  0.001
64    64.0   0.983032    1.119055   0.567306    2.513644  0.001
65    65.0   0.981567    1.120084   0.576716    2.466188  0.001
66    66.0   0.980103    1.123074   0.558813    2.526299  0.001
67    67.0   0.981323    1.122997   0.555628    2.545225  0.001
68    68.0   0.984863    1.101212   0.588705    2.442634  0.001
69    69.0   0.982910    1.103316   0.592062    2.447058  0.001
70    70.0   0.989258    1.095409   0.569532    2.508708  0.001
71    71.0   0.984497    1.106766   0.566532    2.555566  0.001
72    72.0   0.985352    1.106414   0.567618    2.532956  0.001
73    73.0   0.985596    1.094903   0.576313    2.488918  0.001
74    74.0   0.988647    1.080921   0.589280    2.438436  0.001
75    75.0   0.987549    1.085893   0.571562    2.505222  0.001
76    76.0   0.987549    1.084619   0.573863    2.509920  0.001
77    77.0   0.985474    1.089757   0.570152    2.516894  0.001
78    78.0   0.984497    1.099181   0.580227    2.480268  0.001
79    79.0   0.985840    1.094065   0.573376    2.510671  0.001
80    80.0   0.986450    1.080622   0.587846    2.435735  0.001
81    81.0   0.991089    1.064861   0.608114    2.407896  0.001
82    82.0   0.991455    1.053065   0.603169    2.399764  0.001
83    83.0   0.992554    1.057212   0.600788    2.412595  0.001
84    84.0   0.990479    1.068754   0.564874    2.534632  0.001
85    85.0   0.989502    1.075962   0.578446    2.509697  0.001
86    86.0   0.989624    1.067854   0.599929    2.418532  0.001
87    87.0   0.991333    1.066690   0.586700    2.477920  0.001
88    88.0   0.987915    1.070036   0.579002    2.517137  0.001
89    89.0   0.985718    1.080162   0.591621    2.427960  0.001
90    90.0   0.989868    1.064044   0.603997    2.411824  0.001
91    91.0   0.989990    1.061722   0.598898    2.438050  0.001
92    92.0   0.989868    1.059731   0.591869    2.470157  0.001
93    93.0   0.994385    1.044032   0.601789    2.446907  0.001
94    94.0   0.991699    1.042673   0.603347    2.412027  0.001
95    95.0   0.994019    1.041296   0.622692    2.349232  0.001
96    96.0   0.992065    1.047245   0.606774    2.411759  0.001
97    97.0   0.987915    1.065959   0.592365    2.440019  0.001
98    98.0   0.990845    1.063349   0.595752    2.456750  0.001
99    99.0   0.990356    1.055844   0.607563    2.387965  0.001
100  100.0   0.992065    1.046051   0.617484    2.383532  0.001
101  101.0   0.991089    1.048511   0.633923    2.314566  0.001
102  102.0   0.992432    1.040388   0.618546    2.371113  0.001
103  103.0   0.992920    1.040550   0.614081    2.367299  0.001
104  104.0   0.991333    1.050033   0.628381    2.318144  0.001
105  105.0   0.991211    1.043525   0.622057    2.375096  0.001
106  106.0   0.991943    1.042371   0.610316    2.399708  0.001
107  107.0   0.992432    1.036295   0.616655    2.395936  0.001
108  108.0   0.994019    1.023537   0.631031    2.314807  0.001
109  109.0   0.993652    1.023397   0.629002    2.362400  0.001
110  110.0   0.994385    1.024503   0.622583    2.359046  0.001
111  111.0   0.994263    1.029528   0.619662    2.379345  0.001
112  112.0   0.991333    1.029396   0.618506    2.399821  0.001
113  113.0   0.991821    1.039022   0.602092    2.460256  0.001
114  114.0   0.992676    1.039379   0.616398    2.394631  0.001
115  115.0   0.990601    1.039608   0.626769    2.361413  0.001
116  116.0   0.992920    1.037787   0.601928    2.423297  0.001

EXPERIMENT: C-14
================

NAME: ghost_20200919004258

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.008057    5.442338   0.012153   10.828816  0.001
1      1.0   0.011353    5.402149   0.017694    5.215696  0.001
2      2.0   0.014038    5.349668   0.014261    6.627111  0.001
3      3.0   0.017944    5.262504   0.023631    6.562974  0.001
4      4.0   0.019165    5.166745   0.027723    5.216635  0.001
5      5.0   0.026001    5.087626   0.029281    5.331025  0.001
6      6.0   0.029297    5.016714   0.027932    5.201330  0.001
7      7.0   0.035400    4.931495   0.035923    5.155684  0.001
8      8.0   0.040649    4.826596   0.043790    5.124328  0.001
9      9.0   0.050659    4.725021   0.036101    5.116211  0.001
10    10.0   0.059570    4.593351   0.061469    4.658823  0.001
11    11.0   0.075439    4.454968   0.075482    4.715270  0.001
12    12.0   0.095947    4.322073   0.066235    4.861986  0.001
13    13.0   0.110107    4.205419   0.097237    4.926468  0.001
14    14.0   0.136719    4.054417   0.108096    4.340870  0.001
15    15.0   0.161499    3.896675   0.155303    4.299799  0.001
16    16.0   0.191650    3.766465   0.155606    4.237042  0.001
17    17.0   0.230347    3.602701   0.196295    3.811535  0.001
18    18.0   0.259033    3.451224   0.171588    4.189559  0.001
19    19.0   0.308960    3.272794   0.222367    3.763558  0.001
20    20.0   0.350342    3.109123   0.291594    3.395171  0.001
21    21.0   0.384766    2.969373   0.299555    3.335740  0.001
22    22.0   0.428589    2.831676   0.338291    3.201633  0.001
23    23.0   0.463379    2.690488   0.336113    3.218203  0.001
24    24.0   0.504517    2.569046   0.382313    3.080963  0.001
25    25.0   0.546265    2.435730   0.383554    3.014025  0.001
26    26.0   0.579102    2.340502   0.417299    2.975047  0.001
27    27.0   0.611084    2.231264   0.448177    2.802627  0.001
28    28.0   0.637817    2.155410   0.466010    2.750876  0.001
29    29.0   0.667725    2.058500   0.460136    2.815506  0.001
30    30.0   0.708984    1.962010   0.493594    2.653136  0.001
31    31.0   0.728271    1.879466   0.499655    2.645224  0.001
32    32.0   0.748413    1.833621   0.492106    2.691959  0.001
33    33.0   0.769165    1.766611   0.513009    2.630100  0.001
34    34.0   0.794189    1.690981   0.543832    2.523328  0.001
35    35.0   0.820435    1.640445   0.527602    2.558492  0.001
36    36.0   0.835083    1.592347   0.552474    2.492913  0.001
37    37.0   0.850098    1.543286   0.560380    2.480593  0.001
38    38.0   0.857910    1.522155   0.552676    2.476991  0.001
39    39.0   0.874023    1.469158   0.563882    2.438467  0.001
40    40.0   0.886353    1.440903   0.578570    2.406383  0.001
41    41.0   0.897339    1.404270   0.586878    2.347143  0.001
42    42.0   0.910889    1.362490   0.586288    2.380422  0.001
43    43.0   0.920166    1.348059   0.595078    2.355588  0.001
44    44.0   0.923706    1.334925   0.580539    2.404331  0.001
45    45.0   0.931396    1.306959   0.568401    2.430186  0.001
46    46.0   0.930664    1.307406   0.602627    2.347569  0.001
47    47.0   0.942749    1.275004   0.610152    2.314914  0.001
48    48.0   0.941406    1.270335   0.616516    2.323826  0.001
49    49.0   0.944458    1.257116   0.600821    2.335458  0.001
50    50.0   0.952148    1.241996   0.628754    2.236773  0.001
51    51.0   0.952271    1.230979   0.627111    2.258804  0.001
52    52.0   0.949707    1.241523   0.569517    2.452052  0.001
53    53.0   0.953247    1.233847   0.625430    2.238758  0.001
54    54.0   0.951172    1.225635   0.621685    2.274449  0.001
55    55.0   0.954712    1.225736   0.593148    2.371094  0.001
56    56.0   0.957031    1.214392   0.606998    2.326123  0.001
57    57.0   0.960571    1.200773   0.621972    2.274535  0.001
58    58.0   0.951782    1.215871   0.611362    2.309186  0.001
59    59.0   0.961426    1.190505   0.585420    2.404935  0.001
60    60.0   0.965820    1.182488   0.616462    2.309533  0.001
61    61.0   0.967407    1.169947   0.626437    2.269961  0.001
62    62.0   0.970947    1.161881   0.614338    2.301095  0.001
63    63.0   0.974243    1.144218   0.607960    2.344271  0.001
64    64.0   0.973267    1.145118   0.640053    2.224545  0.001
65    65.0   0.959839    1.189029   0.570052    2.508489  0.001
66    66.0   0.956909    1.192387   0.616229    2.303969  0.001
67    67.0   0.965088    1.179494   0.630862    2.270126  0.001
68    68.0   0.965576    1.160262   0.638357    2.244198  0.001
69    69.0   0.974731    1.133979   0.640316    2.205250  0.001
70    70.0   0.974487    1.141826   0.636496    2.233839  0.001
71    71.0   0.970459    1.153255   0.603069    2.341911  0.001
72    72.0   0.965698    1.168186   0.616198    2.320506  0.001
73    73.0   0.969727    1.156140   0.658213    2.198383  0.001
74    74.0   0.964111    1.167379   0.622220    2.297992  0.001
75    75.0   0.973511    1.145011   0.622360    2.301514  0.001
76    76.0   0.978394    1.123449   0.640713    2.221803  0.001
77    77.0   0.979248    1.122889   0.647990    2.191301  0.001
78    78.0   0.977783    1.109843   0.653764    2.192583  0.001
79    79.0   0.977539    1.117771   0.591427    2.375059  0.001
80    80.0   0.977539    1.111343   0.642301    2.224576  0.001
81    81.0   0.978638    1.113260   0.636094    2.257424  0.001
82    82.0   0.973389    1.131348   0.636868    2.271868  0.001
83    83.0   0.971924    1.141438   0.643852    2.217927  0.001
84    84.0   0.971436    1.139621   0.639046    2.246570  0.001
85    85.0   0.978882    1.118062   0.635589    2.240901  0.001
86    86.0   0.975098    1.125808   0.653858    2.178088  0.001
87    87.0   0.974976    1.137843   0.628847    2.257980  0.001
88    88.0   0.972534    1.138886   0.643278    2.221069  0.001
89    89.0   0.975342    1.122898   0.617910    2.334441  0.001
90    90.0   0.980103    1.100122   0.649765    2.227676  0.001
91    91.0   0.985107    1.085325   0.669126    2.151765  0.001
92    92.0   0.988403    1.071140   0.637945    2.243556  0.001
93    93.0   0.978027    1.103371   0.654880    2.227678  0.001
94    94.0   0.981689    1.100226   0.635520    2.272087  0.001
95    95.0   0.979248    1.115717   0.654245    2.206428  0.001
96    96.0   0.980591    1.107515   0.608114    2.351986  0.001
97    97.0   0.977905    1.119946   0.587722    2.443316  0.001
98    98.0   0.979370    1.109710   0.665281    2.179987  0.001
99    99.0   0.980103    1.101086   0.661189    2.138197  0.001
100  100.0   0.976318    1.117457   0.609354    2.345103  0.001
101  101.0   0.980103    1.104984   0.645107    2.226071  0.001
102  102.0   0.980591    1.102066   0.648044    2.232369  0.001
103  103.0   0.985229    1.078443   0.668862    2.128300  0.001
104  104.0   0.985229    1.079434   0.652121    2.247218  0.001
105  105.0   0.981201    1.093450   0.645455    2.222183  0.001
106  106.0   0.984497    1.086852   0.649765    2.228551  0.001
107  107.0   0.984253    1.090807   0.671040    2.119343  0.001
108  108.0   0.981689    1.089967   0.662499    2.186252  0.001
109  109.0   0.984863    1.086759   0.628862    2.283432  0.001
110  110.0   0.981689    1.094083   0.640549    2.232015  0.001
111  111.0   0.978760    1.110536   0.640271    2.247419  0.001
112  112.0   0.979736    1.099760   0.647548    2.278916  0.001
113  113.0   0.982056    1.093735   0.662236    2.159302  0.001
114  114.0   0.981934    1.090104   0.668451    2.170442  0.001
115  115.0   0.982422    1.084986   0.655911    2.177469  0.001
116  116.0   0.984131    1.088440   0.654493    2.229833  0.001
117  117.0   0.983276    1.088603   0.663848    2.200157  0.001
118  118.0   0.982788    1.091249   0.684968    2.088785  0.001
119  119.0   0.982300    1.088465   0.683387    2.118475  0.001
120  120.0   0.984741    1.074886   0.649076    2.229113  0.001
121  121.0   0.984985    1.079235   0.661368    2.186821  0.001
122  122.0   0.985840    1.079547   0.677489    2.112761  0.001
123  123.0   0.986572    1.070606   0.680728    2.115417  0.001
124  124.0   0.983154    1.083044   0.622375    2.351007  0.001
125  125.0   0.982788    1.085031   0.653625    2.194202  0.001
126  126.0   0.984497    1.086126   0.661065    2.206817  0.001
127  127.0   0.983887    1.083966   0.632543    2.317061  0.001
128  128.0   0.984009    1.079522   0.664964    2.192200  0.001
129  129.0   0.984619    1.082635   0.674954    2.157363  0.001
130  130.0   0.985229    1.076597   0.626785    2.360257  0.001
131  131.0   0.984741    1.072508   0.644448    2.272212  0.001
132  132.0   0.984497    1.078896   0.674512    2.145001  0.001
133  133.0   0.978149    1.102917   0.662305    2.216975  0.001

EXPERIMENT: C-15
================

NAME: ghost_20200919004532

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.3}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.3}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.005615    5.467622   0.007937    5.427189  0.001
1     1.0   0.006958    5.456586   0.010293    6.613689  0.001
2     2.0   0.010620    5.364616   0.012897    5.266049  0.001
3     3.0   0.012207    5.297658   0.013641    7.589386  0.001
4     4.0   0.014771    5.189625   0.015268    5.525669  0.001
5     5.0   0.019775    5.128297   0.017733    5.771059  0.001
6     6.0   0.020020    5.071706   0.022887    6.095231  0.001
7     7.0   0.025269    5.027680   0.022089    5.137804  0.001
8     8.0   0.025513    4.985510   0.024058    6.589257  0.001
9     9.0   0.034668    4.927705   0.017555    5.473654  0.001
10   10.0   0.037476    4.885030   0.025065    5.206562  0.001
11   11.0   0.044312    4.789968   0.040387    4.855501  0.001
12   12.0   0.050659    4.706546   0.046077    4.747838  0.001
13   13.0   0.062744    4.609816   0.050145    4.726928  0.001
14   14.0   0.066528    4.507549   0.052749    4.767413  0.001
15   15.0   0.087402    4.395429   0.080442    4.443155  0.001
16   16.0   0.108154    4.293763   0.096648    4.325345  0.001
17   17.0   0.125854    4.149470   0.081325    4.532245  0.001
18   18.0   0.140747    4.047178   0.116806    4.241822  0.001
19   19.0   0.164673    3.926307   0.124782    4.178957  0.001
20   20.0   0.189331    3.806230   0.144267    4.053129  0.001
21   21.0   0.216797    3.677653   0.167650    3.977927  0.001
22   22.0   0.244507    3.528703   0.200829    3.800992  0.001
23   23.0   0.272339    3.414298   0.232358    3.586764  0.001
24   24.0   0.306396    3.286399   0.251509    3.517485  0.001
25   25.0   0.347778    3.131473   0.264817    3.496861  0.001
26   26.0   0.384766    2.998024   0.311103    3.313094  0.001
27   27.0   0.416382    2.852505   0.337338    3.188987  0.001
28   28.0   0.459595    2.729625   0.381654    3.055131  0.001
29   29.0   0.490845    2.614668   0.355334    3.116655  0.001
30   30.0   0.534546    2.492287   0.408177    2.963470  0.001
31   31.0   0.569092    2.369706   0.387839    3.053374  0.001
32   32.0   0.595093    2.293370   0.462259    2.793874  0.001
33   33.0   0.640991    2.162139   0.411114    2.988508  0.001
34   34.0   0.659790    2.099868   0.440062    2.862003  0.001
35   35.0   0.684448    2.015596   0.436272    2.896067  0.001
36   36.0   0.718872    1.941805   0.513837    2.604375  0.001
37   37.0   0.727295    1.893289   0.528331    2.560632  0.001
38   38.0   0.763306    1.790877   0.536888    2.540463  0.001
39   39.0   0.778809    1.752255   0.477760    2.757754  0.001
40   40.0   0.799561    1.700073   0.539577    2.520574  0.001
41   41.0   0.815430    1.656854   0.555435    2.480606  0.001
42   42.0   0.827759    1.602394   0.530618    2.569510  0.001
43   43.0   0.845703    1.568321   0.580926    2.395646  0.001
44   44.0   0.857422    1.535877   0.520891    2.625467  0.001
45   45.0   0.869019    1.487758   0.582538    2.402080  0.001
46   46.0   0.879395    1.466491   0.547692    2.487401  0.001
47   47.0   0.898071    1.422512   0.586164    2.382031  0.001
48   48.0   0.904663    1.389913   0.583491    2.384290  0.001
49   49.0   0.905518    1.387025   0.558798    2.473748  0.001
50   50.0   0.895020    1.415409   0.558852    2.512443  0.001
51   51.0   0.914917    1.366233   0.582598    2.386819  0.001
52   52.0   0.914551    1.355494   0.580043    2.419406  0.001
53   53.0   0.927246    1.333428   0.600767    2.327479  0.001
54   54.0   0.929077    1.308688   0.584304    2.396260  0.001
55   55.0   0.926147    1.321185   0.593535    2.384231  0.001
56   56.0   0.934448    1.299114   0.557751    2.500913  0.001
57   57.0   0.934448    1.303025   0.615966    2.284549  0.001
58   58.0   0.942871    1.271015   0.606199    2.311861  0.001
59   59.0   0.947510    1.264289   0.585832    2.381505  0.001
60   60.0   0.933594    1.292603   0.573872    2.441396  0.001
61   61.0   0.943481    1.263705   0.604820    2.326336  0.001
62   62.0   0.938843    1.278437   0.594666    2.361066  0.001
63   63.0   0.947266    1.254951   0.590520    2.392191  0.001
64   64.0   0.941895    1.267147   0.594186    2.378504  0.001
65   65.0   0.943115    1.262733   0.608277    2.320059  0.001
66   66.0   0.950317    1.241701   0.575717    2.439409  0.001
67   67.0   0.943237    1.255719   0.600356    2.351309  0.001
68   68.0   0.950073    1.237309   0.618391    2.298877  0.001
69   69.0   0.950195    1.232834   0.589010    2.397701  0.001
70   70.0   0.958252    1.216462   0.594676    2.343844  0.001
71   71.0   0.953247    1.213161   0.614865    2.290310  0.001
72   72.0   0.952515    1.227976   0.579631    2.440384  0.001

EXPERIMENT: C-16
================

NAME: ghost_20200919024636

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.4}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.4}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.007812    5.434570   0.011037    6.348656  0.001
1     1.0   0.008911    5.360934   0.009177    9.964504  0.001
2     2.0   0.009155    5.331388   0.015377    5.903151  0.001
3     3.0   0.013184    5.257117   0.013765    5.272419  0.001
4     4.0   0.016357    5.185784   0.021329    5.191445  0.001
5     5.0   0.020874    5.120031   0.017733    5.245128  0.001
6     6.0   0.023071    5.064429   0.026042    5.062146  0.001
7     7.0   0.028564    5.007500   0.025794    5.074954  0.001
8     8.0   0.028076    4.935425   0.031002    5.096181  0.001
9     9.0   0.040405    4.848724   0.040923    5.328816  0.001
10   10.0   0.047119    4.726917   0.046518    4.795119  0.001
11   11.0   0.059570    4.619987   0.047247    4.756310  0.001
12   12.0   0.071777    4.495515   0.077575    4.487015  0.001
13   13.0   0.087402    4.355313   0.070660    4.505260  0.001
14   14.0   0.101440    4.239026   0.104693    4.231583  0.001
15   15.0   0.123413    4.119279   0.117381    4.178876  0.001
16   16.0   0.147705    3.977570   0.120572    4.205818  0.001
17   17.0   0.160889    3.879543   0.145531    4.045102  0.001
18   18.0   0.194824    3.729985   0.201340    3.732024  0.001
19   19.0   0.226074    3.595229   0.182095    3.872304  0.001
20   20.0   0.259888    3.463218   0.210169    3.692666  0.001
21   21.0   0.294067    3.324966   0.238162    3.628597  0.001
22   22.0   0.319824    3.216918   0.296709    3.322631  0.001
23   23.0   0.359375    3.061020   0.318325    3.249929  0.001
24   24.0   0.400269    2.938766   0.343559    3.180127  0.001
25   25.0   0.437012    2.820423   0.373957    3.063644  0.001
26   26.0   0.459106    2.714695   0.357914    3.138360  0.001
27   27.0   0.492798    2.622902   0.382770    2.993154  0.001
28   28.0   0.514893    2.525645   0.434978    2.836746  0.001
29   29.0   0.559937    2.405758   0.378276    3.072234  0.001
30   30.0   0.579712    2.308797   0.469806    2.731570  0.001
31   31.0   0.614746    2.227929   0.441828    2.809499  0.001
32   32.0   0.641846    2.158268   0.454214    2.774287  0.001
33   33.0   0.668579    2.059083   0.499537    2.622245  0.001
34   34.0   0.675415    2.020539   0.485657    2.697112  0.001
35   35.0   0.703369    1.957042   0.499943    2.595850  0.001
36   36.0   0.721313    1.898089   0.532269    2.532995  0.001
37   37.0   0.746094    1.843910   0.539141    2.485785  0.001
38   38.0   0.753906    1.798824   0.576616    2.380377  0.001
39   39.0   0.786743    1.726697   0.567013    2.411990  0.001
40   40.0   0.792725    1.694633   0.560262    2.443845  0.001
41   41.0   0.799927    1.675143   0.538197    2.487321  0.001
42   42.0   0.806763    1.640438   0.566293    2.428596  0.001
43   43.0   0.821533    1.608522   0.530004    2.564170  0.001
44   44.0   0.837402    1.570974   0.584498    2.364404  0.001
45   45.0   0.860474    1.517093   0.511887    2.638899  0.001
46   46.0   0.848877    1.540965   0.587063    2.359540  0.001
47   47.0   0.866699    1.496498   0.590093    2.326138  0.001
48   48.0   0.859985    1.496662   0.590489    2.339146  0.001
49   49.0   0.867676    1.478015   0.561333    2.457628  0.001
50   50.0   0.888550    1.425184   0.599261    2.332500  0.001
51   51.0   0.893799    1.411768   0.562712    2.422448  0.001
52   52.0   0.896118    1.402526   0.549349    2.478250  0.001
53   53.0   0.891235    1.396083   0.602325    2.336404  0.001
54   54.0   0.896851    1.404907   0.587740    2.358255  0.001
55   55.0   0.901489    1.377086   0.591070    2.318658  0.001
56   56.0   0.904541    1.375717   0.564245    2.452548  0.001
57   57.0   0.912842    1.358411   0.530028    2.583575  0.001
58   58.0   0.896729    1.386514   0.597216    2.325512  0.001
59   59.0   0.913086    1.355111   0.608794    2.292946  0.001
60   60.0   0.924683    1.312358   0.591134    2.356415  0.001
61   61.0   0.919189    1.324438   0.621020    2.254299  0.001
62   62.0   0.909180    1.360563   0.554875    2.490663  0.001
63   63.0   0.914307    1.349402   0.561581    2.447771  0.001
64   64.0   0.920532    1.325527   0.584646    2.373125  0.001
65   65.0   0.929810    1.300112   0.600704    2.339147  0.001
66   66.0   0.925537    1.315140   0.577206    2.395195  0.001
67   67.0   0.917114    1.330313   0.568531    2.463699  0.001
68   68.0   0.925415    1.315169   0.579353    2.456733  0.001
69   69.0   0.937744    1.276937   0.538467    2.586328  0.001
70   70.0   0.928101    1.289616   0.583606    2.422990  0.001
71   71.0   0.934204    1.281244   0.615778    2.309033  0.001
72   72.0   0.938721    1.268255   0.576852    2.435570  0.001
73   73.0   0.929810    1.287120   0.539592    2.539331  0.001
74   74.0   0.935181    1.287149   0.612533    2.286905  0.001
75   75.0   0.926392    1.301948   0.626382    2.276390  0.001
76   76.0   0.931641    1.297909   0.629150    2.207598  0.001
77   77.0   0.931641    1.281002   0.492299    2.731648  0.001
78   78.0   0.932861    1.275745   0.585856    2.413882  0.001
79   79.0   0.938354    1.268694   0.595737    2.362425  0.001
80   80.0   0.937744    1.267689   0.651423    2.173687  0.001
81   81.0   0.939697    1.258929   0.604206    2.345059  0.001
82   82.0   0.944458    1.244705   0.604200    2.333025  0.001
83   83.0   0.941284    1.250685   0.610276    2.300260  0.001
84   84.0   0.946411    1.241111   0.539416    2.603054  0.001
85   85.0   0.933350    1.279848   0.601076    2.367975  0.001
86   86.0   0.943604    1.249899   0.620460    2.293812  0.001
87   87.0   0.935791    1.273335   0.634984    2.252399  0.001
88   88.0   0.939453    1.262476   0.624017    2.313097  0.001
89   89.0   0.932373    1.280661   0.607539    2.335312  0.001
90   90.0   0.942749    1.241742   0.623784    2.252155  0.001
91   91.0   0.952637    1.209155   0.623010    2.331195  0.001
92   92.0   0.959473    1.192756   0.604702    2.362002  0.001
93   93.0   0.952271    1.221040   0.614880    2.296365  0.001
94   94.0   0.950195    1.216319   0.563891    2.490605  0.001
95   95.0   0.942627    1.240742   0.601254    2.362781  0.001

EXPERIMENT: C-17
================

NAME: ghost_20200919093001

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.5}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.5}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.008057    5.436094   0.010913    5.284986  0.001
1     1.0   0.010010    5.377795   0.012773    5.240065  0.001
2     2.0   0.012207    5.331464   0.011657    8.286401  0.001
3     3.0   0.015869    5.253327   0.014509    6.237370  0.001
4     4.0   0.017700    5.157627   0.017981    5.316266  0.001
5     5.0   0.023071    5.106009   0.020089    5.328292  0.001
6     6.0   0.026123    5.035954   0.025546    5.177608  0.001
7     7.0   0.028320    4.986261   0.028522    5.468179  0.001
8     8.0   0.036377    4.903819   0.036349    5.987338  0.001
9     9.0   0.039185    4.829384   0.034226    6.731777  0.001
10   10.0   0.043823    4.737698   0.038264    6.991948  0.001
11   11.0   0.054077    4.626845   0.049013    6.222505  0.001
12   12.0   0.067505    4.497721   0.064599    4.695022  0.001
13   13.0   0.081055    4.400533   0.063259    4.514861  0.001
14   14.0   0.095093    4.284534   0.084673    4.412606  0.001
15   15.0   0.112305    4.175057   0.097748    4.271049  0.001
16   16.0   0.129639    4.048451   0.129113    4.092163  0.001
17   17.0   0.154907    3.924737   0.145979    3.972139  0.001
18   18.0   0.185669    3.788344   0.155542    3.980580  0.001
19   19.0   0.209106    3.687451   0.148217    3.995433  0.001
20   20.0   0.237671    3.555234   0.191347    3.810695  0.001
21   21.0   0.268921    3.408572   0.184754    3.819312  0.001
22   22.0   0.305298    3.271660   0.219739    3.669813  0.001
23   23.0   0.344116    3.133818   0.296591    3.335603  0.001
24   24.0   0.382446    3.002047   0.300462    3.294492  0.001
25   25.0   0.426270    2.837154   0.295215    3.378827  0.001
26   26.0   0.456787    2.752516   0.348607    3.145785  0.001
27   27.0   0.485352    2.623602   0.417320    2.906420  0.001
28   28.0   0.518433    2.514674   0.373212    3.059134  0.001
29   29.0   0.543213    2.438566   0.414041    2.917905  0.001
30   30.0   0.591675    2.321222   0.399726    2.998986  0.001
31   31.0   0.612427    2.237619   0.427528    2.831271  0.001
32   32.0   0.635742    2.168711   0.471433    2.696289  0.001
33   33.0   0.657349    2.098447   0.510891    2.570478  0.001
34   34.0   0.674805    2.047841   0.486029    2.628084  0.001
35   35.0   0.697876    1.979905   0.474551    2.675594  0.001
36   36.0   0.718506    1.912667   0.477612    2.703975  0.001
37   37.0   0.734009    1.868936   0.539244    2.490569  0.001
38   38.0   0.741943    1.852730   0.527905    2.530583  0.001
39   39.0   0.757202    1.797721   0.560362    2.458394  0.001
40   40.0   0.777710    1.735626   0.561178    2.432239  0.001
41   41.0   0.783936    1.725718   0.450554    2.842698  0.001
42   42.0   0.792969    1.699057   0.539652    2.500961  0.001
43   43.0   0.803833    1.685426   0.518411    2.589353  0.001
44   44.0   0.811768    1.643741   0.540965    2.529720  0.001
45   45.0   0.818970    1.623969   0.484859    2.693586  0.001
46   46.0   0.835938    1.590848   0.491422    2.709427  0.001
47   47.0   0.836426    1.568373   0.565083    2.447800  0.001
48   48.0   0.837280    1.570000   0.510816    2.657681  0.001
49   49.0   0.849609    1.553654   0.585693    2.357384  0.001
50   50.0   0.861450    1.513198   0.519627    2.588650  0.001
51   51.0   0.858276    1.529064   0.501406    2.700532  0.001
52   52.0   0.852295    1.534355   0.599457    2.301491  0.001
53   53.0   0.865234    1.498564   0.547855    2.511500  0.001
54   54.0   0.868896    1.487925   0.597479    2.333902  0.001
55   55.0   0.865601    1.486109   0.575034    2.429611  0.001
56   56.0   0.871094    1.476862   0.583010    2.386796  0.001
57   57.0   0.876831    1.457900   0.571640    2.453793  0.001
58   58.0   0.883301    1.449533   0.541422    2.525906  0.001
59   59.0   0.871948    1.470262   0.542483    2.540887  0.001
60   60.0   0.882446    1.444108   0.582142    2.391446  0.001
61   61.0   0.885620    1.441245   0.579761    2.452955  0.001
62   62.0   0.903809    1.379931   0.584483    2.405977  0.001
63   63.0   0.903320    1.378304   0.526919    2.593964  0.001
64   64.0   0.900879    1.395858   0.548203    2.484046  0.001
65   65.0   0.899170    1.395359   0.547831    2.520179  0.001
66   66.0   0.898560    1.395249   0.523673    2.621264  0.001
67   67.0   0.898071    1.392585   0.525013    2.609882  0.001

EXPERIMENT: C-18
================

NAME: ghost_20200919111723

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.1,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.005005    5.480509   0.008557    5.512994  0.001
1      1.0   0.007202    5.457059   0.009425   16.860352  0.001
2      2.0   0.010254    5.366024   0.011781    5.479494  0.001
3      3.0   0.013672    5.252083   0.019484    5.162034  0.001
4      4.0   0.019531    5.176038   0.019221    5.165483  0.001
5      5.0   0.019043    5.105697   0.029227    5.147059  0.001
6      6.0   0.023315    5.050423   0.027723    5.063827  0.001
7      7.0   0.027466    4.989585   0.036404    5.086635  0.001
8      8.0   0.034546    4.936820   0.041394    5.012249  0.001
9      9.0   0.039673    4.865136   0.044619    4.952549  0.001
10    10.0   0.046997    4.779377   0.048696    5.023009  0.001
11    11.0   0.056641    4.658015   0.040511    5.095522  0.001
12    12.0   0.069580    4.546845   0.059415    4.730989  0.001
13    13.0   0.081909    4.438069   0.066126    4.818226  0.001
14    14.0   0.095459    4.330621   0.090114    4.957000  0.001
15    15.0   0.117920    4.190530   0.109490    4.415162  0.001
16    16.0   0.136353    4.066781   0.117868    4.288058  0.001
17    17.0   0.162476    3.928530   0.144336    4.153721  0.001
18    18.0   0.187866    3.783540   0.148344    4.137124  0.001
19    19.0   0.229614    3.609679   0.195001    3.889638  0.001
20    20.0   0.259033    3.448054   0.196102    4.087860  0.001
21    21.0   0.303833    3.298174   0.195597    4.012189  0.001
22    22.0   0.337769    3.156697   0.277303    3.505275  0.001
23    23.0   0.377441    3.006345   0.300716    3.406366  0.001
24    24.0   0.416748    2.848401   0.341197    3.224170  0.001
25    25.0   0.470337    2.704685   0.363162    3.154028  0.001
26    26.0   0.499756    2.581127   0.368742    3.101757  0.001
27    27.0   0.542114    2.455585   0.362487    3.160282  0.001
28    28.0   0.580811    2.317871   0.410711    3.004875  0.001
29    29.0   0.626465    2.206978   0.438171    2.884110  0.001
30    30.0   0.644775    2.129764   0.425731    2.944597  0.001
31    31.0   0.692505    1.999700   0.452826    2.834166  0.001
32    32.0   0.716553    1.929698   0.473450    2.768605  0.001
33    33.0   0.738892    1.853392   0.471799    2.766382  0.001
34    34.0   0.773804    1.764290   0.504855    2.665396  0.001
35    35.0   0.796997    1.695945   0.493609    2.726482  0.001
36    36.0   0.829224    1.613987   0.538693    2.577894  0.001
37    37.0   0.842529    1.580655   0.531129    2.586000  0.001
38    38.0   0.867188    1.517599   0.503778    2.678909  0.001
39    39.0   0.879395    1.472014   0.532826    2.566818  0.001
40    40.0   0.890015    1.439505   0.521417    2.599316  0.001
41    41.0   0.899292    1.417101   0.539949    2.556448  0.001
42    42.0   0.915161    1.360352   0.523758    2.616897  0.001
43    43.0   0.920166    1.337049   0.547776    2.531430  0.001
44    44.0   0.932007    1.314951   0.562068    2.480060  0.001
45    45.0   0.932983    1.306448   0.563332    2.466853  0.001
46    46.0   0.941040    1.277092   0.533957    2.584491  0.001
47    47.0   0.948853    1.263666   0.579135    2.432615  0.001
48    48.0   0.954102    1.243659   0.583669    2.399057  0.001
49    49.0   0.944458    1.255524   0.559185    2.516684  0.001
50    50.0   0.958130    1.218727   0.579289    2.462881  0.001
51    51.0   0.961426    1.207618   0.586204    2.430210  0.001
52    52.0   0.957886    1.211586   0.587900    2.407474  0.001
53    53.0   0.959717    1.214441   0.579150    2.431083  0.001
54    54.0   0.968506    1.183492   0.596992    2.372906  0.001
55    55.0   0.969360    1.167437   0.594914    2.418189  0.001
56    56.0   0.964844    1.192100   0.583902    2.419257  0.001
57    57.0   0.967041    1.175115   0.561333    2.495009  0.001
58    58.0   0.968994    1.177237   0.575221    2.465752  0.001
59    59.0   0.965942    1.180137   0.599324    2.394237  0.001
60    60.0   0.961792    1.185618   0.594334    2.395711  0.001
61    61.0   0.970581    1.157818   0.578252    2.448174  0.001
62    62.0   0.974609    1.150215   0.578252    2.458088  0.001
63    63.0   0.973022    1.140118   0.584909    2.430841  0.001
64    64.0   0.970581    1.150348   0.584716    2.453501  0.001
65    65.0   0.972290    1.156003   0.597256    2.401013  0.001
66    66.0   0.976562    1.135458   0.590048    2.414012  0.001
67    67.0   0.976196    1.137378   0.613501    2.352180  0.001
68    68.0   0.974609    1.137889   0.602781    2.397862  0.001
69    69.0   0.978638    1.122786   0.603426    2.365134  0.001
70    70.0   0.974243    1.145754   0.585390    2.404558  0.001
71    71.0   0.976074    1.139279   0.590172    2.437799  0.001
72    72.0   0.983765    1.109283   0.598967    2.419557  0.001
73    73.0   0.979492    1.119196   0.590226    2.416953  0.001
74    74.0   0.980713    1.108585   0.609036    2.355954  0.001
75    75.0   0.980713    1.107720   0.582251    2.468924  0.001
76    76.0   0.976807    1.140235   0.605866    2.364204  0.001
77    77.0   0.979858    1.120078   0.605316    2.341975  0.001
78    78.0   0.982300    1.107842   0.626630    2.315329  0.001
79    79.0   0.979492    1.110085   0.603580    2.377779  0.001
80    80.0   0.981567    1.107322   0.590822    2.416339  0.001
81    81.0   0.986572    1.078568   0.637558    2.255419  0.001
82    82.0   0.982544    1.095757   0.615663    2.339472  0.001
83    83.0   0.983154    1.098881   0.599379    2.412371  0.001
84    84.0   0.981812    1.097880   0.623724    2.305922  0.001
85    85.0   0.986084    1.086368   0.630196    2.288179  0.001
86    86.0   0.980835    1.096569   0.611338    2.349835  0.001
87    87.0   0.985840    1.088639   0.618530    2.334686  0.001
88    88.0   0.983765    1.095274   0.612950    2.381736  0.001
89    89.0   0.984009    1.095581   0.643758    2.234603  0.001
90    90.0   0.989746    1.086119   0.598178    2.457857  0.001
91    91.0   0.981445    1.104049   0.627761    2.294083  0.001
92    92.0   0.976807    1.114532   0.636016    2.274221  0.001
93    93.0   0.982300    1.106284   0.593218    2.459682  0.001
94    94.0   0.984375    1.078713   0.640232    2.250669  0.001
95    95.0   0.985718    1.078238   0.624413    2.333591  0.001
96    96.0   0.984985    1.081267   0.628203    2.304588  0.001
97    97.0   0.986450    1.072864   0.655098    2.215873  0.001
98    98.0   0.986084    1.075273   0.621824    2.331480  0.001
99    99.0   0.987305    1.074585   0.644805    2.203204  0.001
100  100.0   0.981934    1.096156   0.607890    2.368938  0.001
101  101.0   0.985718    1.072501   0.622538    2.319784  0.001
102  102.0   0.989258    1.067817   0.614136    2.402849  0.001
103  103.0   0.988159    1.063872   0.639845    2.277990  0.001
104  104.0   0.984619    1.073784   0.632256    2.293925  0.001
105  105.0   0.988403    1.072372   0.639185    2.301357  0.001
106  106.0   0.987305    1.068845   0.629622    2.316231  0.001
107  107.0   0.983765    1.087068   0.622042    2.325383  0.001
108  108.0   0.984619    1.093330   0.644146    2.249103  0.001
109  109.0   0.983521    1.079913   0.608595    2.393629  0.001
110  110.0   0.989624    1.068632   0.658391    2.201730  0.001
111  111.0   0.987671    1.060683   0.607409    2.399883  0.001
112  112.0   0.986938    1.061758   0.637210    2.309133  0.001
113  113.0   0.988647    1.047789   0.666893    2.179266  0.001
114  114.0   0.991089    1.043962   0.668962    2.203963  0.001
115  115.0   0.987427    1.057352   0.643610    2.277222  0.001
116  116.0   0.988770    1.061525   0.657027    2.188703  0.001
117  117.0   0.985107    1.078826   0.627777    2.331262  0.001
118  118.0   0.980591    1.101196   0.614756    2.353562  0.001
119  119.0   0.984863    1.087111   0.646402    2.231812  0.001
120  120.0   0.985840    1.065053   0.655981    2.248269  0.001
121  121.0   0.990356    1.039002   0.658074    2.241092  0.001
122  122.0   0.990479    1.039017   0.649393    2.261938  0.001
123  123.0   0.988281    1.049768   0.628242    2.344117  0.001
124  124.0   0.989868    1.050069   0.628754    2.321661  0.001
125  125.0   0.989624    1.053133   0.624607    2.336844  0.001
126  126.0   0.984985    1.064883   0.628009    2.291623  0.001
127  127.0   0.987793    1.066279   0.642464    2.261187  0.001
128  128.0   0.985107    1.071543   0.634651    2.273426  0.001

EXPERIMENT: C-19
================

NAME: ghost_20200919112111

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.3,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.006958    5.428983   0.008750    5.674539  0.001
1      1.0   0.009766    5.388566   0.013214    5.263114  0.001
2      2.0   0.012817    5.363870   0.017679    5.268764  0.001
3      3.0   0.014771    5.326757   0.017485    5.403054  0.001
4      4.0   0.013062    5.305646   0.017431   13.184285  0.001
5      5.0   0.021362    5.186705   0.019345   22.330271  0.001
6      6.0   0.023926    5.105898   0.027118    5.872627  0.001
7      7.0   0.028442    5.018582   0.031885    9.175242  0.001
8      8.0   0.028687    4.974799   0.028963    5.038043  0.001
9      9.0   0.038574    4.921576   0.040248    4.920313  0.001
10    10.0   0.040527    4.832734   0.045208    4.806216  0.001
11    11.0   0.049683    4.735765   0.040194    4.863804  0.001
12    12.0   0.053955    4.731963   0.055005    4.743851  0.001
13    13.0   0.064331    4.593971   0.043790    4.937807  0.001
14    14.0   0.076538    4.482932   0.070908    4.603586  0.001
15    15.0   0.089722    4.390117   0.093463    4.414582  0.001
16    16.0   0.102051    4.271221   0.100477    4.374101  0.001
17    17.0   0.121826    4.152670   0.078954    4.664662  0.001
18    18.0   0.145752    4.022779   0.130626    4.198401  0.001
19    19.0   0.174072    3.881338   0.138136    4.477153  0.001
20    20.0   0.205444    3.746397   0.148374    4.101649  0.001
21    21.0   0.230713    3.597213   0.194575    3.861825  0.001
22    22.0   0.264404    3.466360   0.192373    3.914033  0.001
23    23.0   0.296265    3.321325   0.246812    3.556576  0.001
24    24.0   0.341675    3.172037   0.258097    3.561496  0.001
25    25.0   0.371948    3.052105   0.279620    3.475175  0.001
26    26.0   0.403442    2.927672   0.317899    3.322796  0.001
27    27.0   0.445190    2.790357   0.338593    3.244454  0.001
28    28.0   0.485840    2.650426   0.357884    3.138050  0.001
29    29.0   0.514282    2.535152   0.384809    3.039070  0.001
30    30.0   0.547974    2.418792   0.378236    3.098583  0.001
31    31.0   0.584717    2.308769   0.350459    3.227026  0.001
32    32.0   0.625366    2.199227   0.434025    2.892336  0.001
33    33.0   0.652588    2.114657   0.450028    2.836900  0.001
34    34.0   0.682373    2.033738   0.468435    2.782233  0.001
35    35.0   0.702759    1.973230   0.461530    2.801756  0.001
36    36.0   0.733154    1.876211   0.489021    2.707579  0.001
37    37.0   0.759521    1.802290   0.483549    2.751293  0.001
38    38.0   0.775146    1.748272   0.460995    2.833351  0.001
39    39.0   0.794922    1.694426   0.519016    2.590588  0.001
40    40.0   0.822754    1.631751   0.509249    2.645024  0.001
41    41.0   0.830444    1.589931   0.512597    2.627548  0.001
42    42.0   0.857056    1.535047   0.518341    2.603225  0.001
43    43.0   0.879761    1.481242   0.551094    2.503685  0.001
44    44.0   0.882935    1.465813   0.560310    2.465937  0.001
45    45.0   0.894287    1.432636   0.537260    2.525204  0.001
46    46.0   0.908691    1.388488   0.566308    2.435186  0.001
47    47.0   0.921021    1.357092   0.544259    2.520824  0.001
48    48.0   0.916870    1.356747   0.578763    2.422483  0.001
49    49.0   0.928833    1.323727   0.568951    2.418488  0.001
50    50.0   0.935547    1.311823   0.570068    2.413790  0.001
51    51.0   0.932007    1.311984   0.578198    2.414318  0.001
52    52.0   0.938232    1.291722   0.572687    2.433590  0.001
53    53.0   0.936157    1.298030   0.565549    2.467411  0.001
54    54.0   0.938354    1.297698   0.577865    2.408607  0.001
55    55.0   0.938599    1.285445   0.579150    2.401209  0.001
56    56.0   0.954956    1.243626   0.614066    2.283255  0.001
57    57.0   0.956421    1.225045   0.589800    2.386847  0.001
58    58.0   0.955200    1.234117   0.599448    2.362256  0.001
59    59.0   0.955933    1.227503   0.593868    2.367105  0.001
60    60.0   0.958984    1.215705   0.603138    2.316851  0.001
61    61.0   0.965576    1.195048   0.621987    2.275069  0.001
62    62.0   0.959717    1.212304   0.545544    2.553223  0.001
63    63.0   0.960815    1.206361   0.582081    2.434119  0.001
64    64.0   0.961060    1.206499   0.583421    2.410162  0.001
65    65.0   0.967773    1.182436   0.603054    2.367221  0.001
66    66.0   0.961670    1.197932   0.589637    2.400162  0.001
67    67.0   0.959961    1.207546   0.592791    2.373724  0.001
68    68.0   0.958862    1.205429   0.601085    2.374282  0.001
69    69.0   0.962280    1.197535   0.577950    2.455486  0.001
70    70.0   0.970093    1.169708   0.619825    2.285896  0.001
71    71.0   0.958862    1.198268   0.587305    2.402485  0.001
72    72.0   0.969116    1.172605   0.609656    2.324930  0.001
73    73.0   0.968872    1.168347   0.616958    2.328413  0.001
74    74.0   0.972778    1.159404   0.620251    2.265952  0.001
75    75.0   0.967285    1.174521   0.613540    2.308190  0.001
76    76.0   0.975098    1.147902   0.626204    2.262212  0.001
77    77.0   0.973511    1.152839   0.585653    2.415301  0.001
78    78.0   0.972412    1.155650   0.638124    2.236347  0.001
79    79.0   0.969971    1.170765   0.587885    2.426533  0.001
80    80.0   0.974854    1.149790   0.601169    2.389855  0.001
81    81.0   0.973511    1.150021   0.622375    2.292106  0.001
82    82.0   0.971680    1.160361   0.598589    2.345916  0.001
83    83.0   0.971680    1.151896   0.577330    2.471292  0.001
84    84.0   0.976196    1.145435   0.616531    2.309523  0.001
85    85.0   0.971069    1.158947   0.619453    2.303239  0.001
86    86.0   0.970337    1.158301   0.616159    2.314864  0.001
87    87.0   0.978638    1.125542   0.651268    2.191885  0.001
88    88.0   0.979736    1.125094   0.627196    2.250264  0.001
89    89.0   0.980957    1.120907   0.641883    2.219788  0.001
90    90.0   0.979858    1.127208   0.612260    2.323949  0.001
91    91.0   0.975708    1.142718   0.612315    2.313390  0.001
92    92.0   0.971191    1.155717   0.611477    2.329404  0.001
93    93.0   0.976807    1.141559   0.620902    2.326524  0.001
94    94.0   0.973999    1.141979   0.644270    2.208621  0.001
95    95.0   0.977905    1.133341   0.638992    2.197676  0.001
96    96.0   0.977295    1.122187   0.626382    2.257458  0.001
97    97.0   0.976929    1.128976   0.647270    2.203843  0.001
98    98.0   0.976562    1.131247   0.621933    2.321030  0.001
99    99.0   0.975952    1.137197   0.605370    2.359011  0.001
100  100.0   0.973511    1.139590   0.631536    2.264073  0.001
101  101.0   0.979980    1.116345   0.609423    2.367800  0.001
102  102.0   0.983521    1.103978   0.671234    2.145622  0.001
103  103.0   0.977905    1.123871   0.640108    2.246679  0.001
104  104.0   0.974731    1.139858   0.608610    2.385315  0.001
105  105.0   0.978149    1.125834   0.607711    2.357123  0.001
106  106.0   0.982544    1.108196   0.648788    2.226003  0.001
107  107.0   0.980103    1.103799   0.669800    2.140107  0.001
108  108.0   0.986206    1.086668   0.656422    2.204100  0.001
109  109.0   0.981445    1.101773   0.643317    2.237767  0.001
110  110.0   0.984497    1.097494   0.641030    2.264489  0.001
111  111.0   0.977783    1.125333   0.600658    2.416612  0.001
112  112.0   0.965576    1.164320   0.612315    2.350003  0.001
113  113.0   0.973389    1.147843   0.636953    2.240035  0.001
114  114.0   0.976318    1.119126   0.640891    2.265626  0.001
115  115.0   0.981689    1.112028   0.636403    2.246155  0.001
116  116.0   0.983521    1.105559   0.671412    2.132606  0.001
117  117.0   0.982788    1.098948   0.634086    2.284575  0.001
118  118.0   0.980835    1.099812   0.645207    2.212794  0.001
119  119.0   0.980957    1.102708   0.654810    2.182338  0.001
120  120.0   0.981201    1.105151   0.641278    2.274179  0.001
121  121.0   0.979858    1.111307   0.642146    2.269072  0.001
122  122.0   0.980103    1.110200   0.634225    2.284481  0.001
123  123.0   0.978394    1.126698   0.678069    2.107612  0.001
124  124.0   0.983032    1.094774   0.648912    2.244684  0.001
125  125.0   0.982666    1.097637   0.658019    2.173675  0.001
126  126.0   0.980103    1.101648   0.655167    2.201220  0.001
127  127.0   0.975342    1.127614   0.622553    2.327435  0.001
128  128.0   0.981201    1.101695   0.636472    2.262353  0.001
129  129.0   0.983887    1.095476   0.662677    2.201184  0.001
130  130.0   0.982544    1.093718   0.673303    2.141155  0.001
131  131.0   0.985474    1.088390   0.671164    2.122716  0.001
132  132.0   0.982666    1.090079   0.630877    2.318094  0.001
133  133.0   0.982666    1.095100   0.660996    2.186147  0.001
134  134.0   0.984863    1.087723   0.648401    2.232759  0.001
135  135.0   0.979004    1.117501   0.637325    2.269606  0.001
136  136.0   0.974487    1.127720   0.654438    2.197321  0.001
137  137.0   0.976929    1.119421   0.674776    2.130803  0.001
138  138.0   0.985229    1.093776   0.672900    2.155354  0.001

EXPERIMENT: C-20
================

NAME: ghost_20200919132156

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.4,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.006470    5.473466   0.013765    5.795957  0.001
1     1.0   0.008911    5.410369   0.012649    8.971790  0.001
2     2.0   0.013062    5.380570   0.014013    6.766061  0.001
3     3.0   0.016724    5.298327   0.020833    5.748191  0.001
4     4.0   0.018188    5.222896   0.017981   20.285252  0.001
5     5.0   0.022583    5.142239   0.024058    5.397996  0.001
6     6.0   0.029297    5.044713   0.032242    5.471155  0.001
7     7.0   0.031494    4.986972   0.031002    5.584818  0.001
8     8.0   0.034912    4.914282   0.037202    5.669455  0.001
9     9.0   0.043945    4.831980   0.049059    5.075706  0.001
10   10.0   0.050293    4.731478   0.056151    5.022705  0.001
11   11.0   0.057129    4.636376   0.058175    4.888971  0.001
12   12.0   0.071899    4.525304   0.054013    5.238282  0.001
13   13.0   0.084717    4.422603   0.081147    4.818893  0.001
14   14.0   0.097168    4.338613   0.092422    5.162826  0.001
15   15.0   0.110596    4.240148   0.094654    4.928063  0.001
16   16.0   0.128174    4.127762   0.122967    4.567266  0.001
17   17.0   0.149902    4.009353   0.130230    6.490671  0.001
18   18.0   0.170044    3.874857   0.134183    4.999508  0.001
19   19.0   0.195190    3.739811   0.152342    3.970368  0.001
20   20.0   0.216064    3.619419   0.205145    3.725360  0.001
21   21.0   0.247803    3.486565   0.222071    3.679590  0.001
22   22.0   0.282227    3.340969   0.250315    3.570445  0.001
23   23.0   0.319824    3.203019   0.276114    3.418261  0.001
24   24.0   0.357910    3.065217   0.287299    3.388449  0.001
25   25.0   0.392334    2.962553   0.288796    3.450446  0.001
26   26.0   0.430908    2.831765   0.342335    3.179051  0.001
27   27.0   0.457397    2.718406   0.371098    3.064263  0.001
28   28.0   0.491211    2.613502   0.378947    3.092378  0.001
29   29.0   0.527832    2.508073   0.426729    2.878651  0.001
30   30.0   0.549072    2.414955   0.427334    2.864360  0.001
31   31.0   0.577881    2.327227   0.433728    2.859195  0.001
32   32.0   0.605591    2.235884   0.456537    2.766673  0.001
33   33.0   0.631470    2.168715   0.449577    2.816264  0.001
34   34.0   0.650146    2.092748   0.467489    2.741694  0.001
35   35.0   0.679199    2.013770   0.495197    2.624404  0.001
36   36.0   0.703369    1.943915   0.482339    2.717658  0.001
37   37.0   0.729370    1.876631   0.527548    2.547099  0.001
38   38.0   0.737549    1.843147   0.542855    2.488035  0.001
39   39.0   0.767334    1.765668   0.500508    2.646212  0.001
40   40.0   0.789795    1.702575   0.541422    2.472136  0.001
41   41.0   0.806396    1.668517   0.554473    2.441164  0.001
42   42.0   0.818726    1.627848   0.557851    2.468909  0.001
43   43.0   0.829590    1.588743   0.555504    2.437965  0.001
44   44.0   0.837891    1.573025   0.586140    2.356120  0.001
45   45.0   0.856201    1.521579   0.586358    2.351904  0.001
46   46.0   0.870239    1.473113   0.577072    2.397860  0.001
47   47.0   0.876343    1.461584   0.574438    2.402092  0.001
48   48.0   0.882568    1.440417   0.591497    2.354100  0.001
49   49.0   0.898315    1.410629   0.602122    2.310065  0.001
50   50.0   0.889282    1.415685   0.576700    2.398379  0.001
51   51.0   0.900513    1.393686   0.607137    2.286763  0.001
52   52.0   0.907349    1.375177   0.600014    2.294354  0.001
53   53.0   0.912354    1.353268   0.572043    2.431805  0.001
54   54.0   0.920654    1.332126   0.598417    2.323290  0.001
55   55.0   0.924927    1.321803   0.567037    2.450846  0.001
56   56.0   0.914062    1.339726   0.591914    2.361827  0.001
57   57.0   0.924316    1.323914   0.571825    2.413511  0.001
58   58.0   0.920288    1.320923   0.622172    2.241697  0.001
59   59.0   0.928711    1.305416   0.611268    2.291280  0.001
60   60.0   0.935547    1.276564   0.598163    2.336158  0.001
61   61.0   0.937500    1.289260   0.620693    2.246051  0.001
62   62.0   0.939209    1.280520   0.616849    2.270659  0.001
63   63.0   0.932739    1.292454   0.625436    2.258516  0.001
64   64.0   0.942383    1.260692   0.628645    2.220427  0.001
65   65.0   0.947266    1.243006   0.613283    2.295204  0.001
66   66.0   0.950439    1.238921   0.626110    2.242559  0.001
67   67.0   0.952637    1.231985   0.625802    2.244707  0.001
68   68.0   0.946533    1.236599   0.613927    2.290953  0.001
69   69.0   0.950195    1.236270   0.622057    2.245519  0.001
70   70.0   0.954956    1.221175   0.610624    2.292516  0.001
71   71.0   0.947144    1.236356   0.621755    2.281542  0.001
72   72.0   0.958374    1.214975   0.612763    2.277348  0.001
73   73.0   0.951294    1.232527   0.599518    2.338286  0.001
74   74.0   0.958252    1.206021   0.609066    2.305833  0.001
75   75.0   0.958862    1.206059   0.648834    2.163501  0.001
76   76.0   0.961792    1.197703   0.635138    2.222558  0.001
77   77.0   0.965210    1.180808   0.616259    2.308426  0.001
78   78.0   0.960815    1.191237   0.613119    2.266757  0.001
79   79.0   0.958496    1.199876   0.636521    2.213345  0.001
80   80.0   0.950195    1.222667   0.609602    2.318025  0.001
81   81.0   0.956665    1.211242   0.635287    2.220824  0.001
82   82.0   0.948120    1.226825   0.611283    2.297971  0.001
83   83.0   0.957886    1.203770   0.612499    2.314523  0.001
84   84.0   0.964844    1.173229   0.640510    2.212495  0.001
85   85.0   0.964233    1.176501   0.617251    2.290149  0.001
86   86.0   0.960083    1.191646   0.616809    2.296770  0.001
87   87.0   0.958984    1.200214   0.620723    2.266404  0.001
88   88.0   0.959229    1.195780   0.628009    2.244087  0.001
89   89.0   0.961548    1.186053   0.626437    2.271558  0.001
90   90.0   0.956909    1.208222   0.615857    2.338022  0.001

EXPERIMENT: C-21
================

NAME: ghost_20200919153600

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.5,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.004761    5.460871   0.011037    5.957318  0.001
1      1.0   0.007812    5.407085   0.010665   17.504591  0.001
2      2.0   0.011108    5.348975   0.013145    5.178681  0.001
3      3.0   0.016113    5.300962   0.019221    5.122268  0.001
4      4.0   0.017944    5.245675   0.024678    5.068602  0.001
5      5.0   0.024170    5.137544   0.029018    5.111300  0.001
6      6.0   0.028442    5.045459   0.034350    5.685771  0.001
7      7.0   0.030396    4.954335   0.036032    4.917165  0.001
8      8.0   0.037720    4.848702   0.043899    4.921113  0.001
9      9.0   0.048706    4.737515   0.051463    4.755130  0.001
10    10.0   0.053467    4.620260   0.071280    4.702050  0.001
11    11.0   0.069214    4.511774   0.072505    4.998612  0.001
12    12.0   0.077637    4.419291   0.066072    4.787789  0.001
13    13.0   0.089355    4.320652   0.069653    4.929706  0.001
14    14.0   0.111694    4.204374   0.108156    4.341523  0.001
15    15.0   0.126709    4.094873   0.070809    4.848565  0.001
16    16.0   0.146729    3.979889   0.127317    4.228079  0.001
17    17.0   0.169434    3.862060   0.157641    4.089687  0.001
18    18.0   0.191895    3.738955   0.171588    4.138690  0.001
19    19.0   0.219604    3.621875   0.201706    3.789392  0.001
20    20.0   0.237549    3.525153   0.242596    3.540458  0.001
21    21.0   0.276001    3.391548   0.253012    3.545600  0.001
22    22.0   0.302979    3.274086   0.288805    3.375260  0.001
23    23.0   0.341675    3.136030   0.308653    3.296774  0.001
24    24.0   0.367554    3.026721   0.320945    3.257993  0.001
25    25.0   0.401733    2.914757   0.375445    3.033055  0.001
26    26.0   0.435669    2.805814   0.367944    3.054694  0.001
27    27.0   0.454590    2.707098   0.392249    2.998447  0.001
28    28.0   0.480103    2.647662   0.397255    2.944181  0.001
29    29.0   0.522339    2.500077   0.397657    2.942266  0.001
30    30.0   0.544067    2.434299   0.448788    2.786711  0.001
31    31.0   0.568726    2.354042   0.451089    2.776290  0.001
32    32.0   0.590210    2.301651   0.447418    2.797015  0.001
33    33.0   0.620728    2.200859   0.494719    2.641518  0.001
34    34.0   0.644409    2.138382   0.522503    2.548571  0.001
35    35.0   0.656006    2.073398   0.453851    2.768452  0.001
36    36.0   0.693604    1.985999   0.522796    2.546118  0.001
37    37.0   0.703125    1.972014   0.541760    2.492352  0.001
38    38.0   0.713745    1.908003   0.524687    2.540390  0.001
39    39.0   0.731689    1.864948   0.546125    2.493563  0.001
40    40.0   0.749146    1.816261   0.560504    2.417108  0.001
41    41.0   0.765137    1.772344   0.558208    2.428371  0.001
42    42.0   0.782837    1.724500   0.555852    2.431127  0.001
43    43.0   0.801392    1.675165   0.558163    2.434897  0.001
44    44.0   0.807617    1.646852   0.569547    2.383214  0.001
45    45.0   0.822388    1.607452   0.544761    2.491027  0.001
46    46.0   0.833862    1.591203   0.587250    2.329508  0.001
47    47.0   0.835815    1.573446   0.581615    2.357544  0.001
48    48.0   0.848877    1.539280   0.591536    2.305333  0.001
49    49.0   0.865723    1.490085   0.593798    2.339385  0.001
50    50.0   0.865723    1.482188   0.616770    2.249449  0.001
51    51.0   0.877563    1.454137   0.622807    2.228387  0.001
52    52.0   0.880493    1.447836   0.613600    2.262314  0.001
53    53.0   0.880249    1.437249   0.610724    2.273585  0.001
54    54.0   0.887817    1.425794   0.626412    2.242290  0.001
55    55.0   0.906494    1.381471   0.628853    2.244319  0.001
56    56.0   0.899048    1.384627   0.608873    2.268484  0.001
57    57.0   0.908813    1.365457   0.613640    2.272024  0.001
58    58.0   0.908325    1.366568   0.614175    2.259434  0.001
59    59.0   0.916504    1.341526   0.635232    2.195979  0.001
60    60.0   0.911377    1.354956   0.627033    2.200313  0.001
61    61.0   0.919312    1.333437   0.592425    2.336296  0.001
62    62.0   0.926147    1.318242   0.604626    2.311568  0.001
63    63.0   0.934937    1.294956   0.636194    2.221570  0.001
64    64.0   0.930176    1.308080   0.640386    2.193822  0.001
65    65.0   0.928467    1.304355   0.646199    2.181731  0.001
66    66.0   0.926636    1.303293   0.639324    2.209944  0.001
67    67.0   0.929077    1.309301   0.627226    2.216256  0.001
68    68.0   0.929688    1.293626   0.622955    2.250829  0.001
69    69.0   0.933228    1.282308   0.631706    2.232117  0.001
70    70.0   0.938354    1.276337   0.629498    2.245982  0.001
71    71.0   0.937500    1.265919   0.622335    2.263966  0.001
72    72.0   0.940674    1.265678   0.648501    2.163061  0.001
73    73.0   0.949829    1.243748   0.628481    2.228322  0.001
74    74.0   0.949951    1.241314   0.649750    2.167501  0.001
75    75.0   0.953613    1.232548   0.634201    2.231486  0.001
76    76.0   0.943115    1.262187   0.630638    2.207864  0.001
77    77.0   0.945679    1.254092   0.627459    2.239112  0.001
78    78.0   0.941895    1.256908   0.654081    2.150901  0.001
79    79.0   0.949219    1.237302   0.639294    2.219170  0.001
80    80.0   0.943848    1.252284   0.648005    2.151520  0.001
81    81.0   0.943481    1.257604   0.614438    2.298949  0.001
82    82.0   0.946655    1.244970   0.611656    2.260076  0.001
83    83.0   0.946533    1.249382   0.638233    2.231608  0.001
84    84.0   0.945923    1.246831   0.646363    2.170594  0.001
85    85.0   0.948730    1.240772   0.662723    2.112637  0.001
86    86.0   0.958374    1.204327   0.640108    2.197079  0.001
87    87.0   0.955688    1.214776   0.662831    2.121244  0.001
88    88.0   0.953125    1.216856   0.593257    2.365135  0.001
89    89.0   0.954346    1.216827   0.666025    2.095552  0.001
90    90.0   0.958008    1.206464   0.666691    2.140273  0.001
91    91.0   0.955322    1.215956   0.667514    2.108643  0.001
92    92.0   0.959473    1.205738   0.643441    2.192979  0.001
93    93.0   0.953979    1.215350   0.606565    2.346710  0.001
94    94.0   0.957642    1.207024   0.659051    2.136938  0.001
95    95.0   0.954102    1.215033   0.647509    2.178919  0.001
96    96.0   0.959961    1.200651   0.635039    2.230194  0.001
97    97.0   0.959961    1.211379   0.646844    2.193739  0.001
98    98.0   0.959961    1.197361   0.660902    2.124889  0.001
99    99.0   0.956299    1.206874   0.625986    2.256534  0.001
100  100.0   0.962769    1.192357   0.624592    2.256035  0.001
101  101.0   0.960938    1.194912   0.648695    2.170110  0.001
102  102.0   0.952026    1.218945   0.648912    2.187332  0.001
103  103.0   0.957153    1.206478   0.623932    2.266588  0.001
104  104.0   0.960815    1.191061   0.629815    2.279690  0.001

EXPERIMENT: C-22
================

NAME: ghost_20200919214406

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.009766    5.319703   0.010734    6.418532  0.001
1      1.0   0.008667    5.243111   0.012773    5.286970  0.001
2      2.0   0.011963    5.200691   0.014633    5.187885  0.001
3      3.0   0.014404    5.146661   0.016384    5.166508  0.001
4      4.0   0.018555    5.106002   0.020957    5.071770  0.001
5      5.0   0.024536    5.026799   0.029955    4.968497  0.001
6      6.0   0.027222    4.954870   0.030079    4.969392  0.001
7      7.0   0.034302    4.859400   0.018864    5.212802  0.001
8      8.0   0.042358    4.745280   0.041558    4.796919  0.001
9      9.0   0.051147    4.628640   0.059981    4.592114  0.001
10    10.0   0.063965    4.503036   0.060407    4.586096  0.001
11    11.0   0.078735    4.394482   0.074589    4.426069  0.001
12    12.0   0.093506    4.289023   0.095531    4.315509  0.001
13    13.0   0.109131    4.185298   0.111722    4.203553  0.001
14    14.0   0.128662    4.072672   0.116985    4.162717  0.001
15    15.0   0.148438    3.963170   0.145606    3.986145  0.001
16    16.0   0.167603    3.857975   0.155775    3.991069  0.001
17    17.0   0.195801    3.736550   0.164232    3.898383  0.001
18    18.0   0.214478    3.632397   0.192342    3.772788  0.001
19    19.0   0.246704    3.517700   0.174183    3.838271  0.001
20    20.0   0.277344    3.391928   0.241341    3.548361  0.001
21    21.0   0.308472    3.279638   0.285666    3.357572  0.001
22    22.0   0.332520    3.193255   0.298236    3.316935  0.001
23    23.0   0.364990    3.066160   0.289625    3.328781  0.001
24    24.0   0.399170    2.938082   0.352210    3.133256  0.001
25    25.0   0.417358    2.877113   0.374710    3.021819  0.001
26    26.0   0.447876    2.763157   0.399799    2.955625  0.001
27    27.0   0.479370    2.669066   0.411570    2.889103  0.001
28    28.0   0.507324    2.570036   0.429397    2.846871  0.001
29    29.0   0.545410    2.472387   0.409308    2.958422  0.001
30    30.0   0.557251    2.414480   0.437031    2.847864  0.001
31    31.0   0.591675    2.313423   0.460221    2.744415  0.001
32    32.0   0.611328    2.243396   0.490494    2.663102  0.001
33    33.0   0.635986    2.180856   0.510583    2.571111  0.001
34    34.0   0.652954    2.113914   0.500841    2.634237  0.001
35    35.0   0.672852    2.053082   0.498772    2.620709  0.001
36    36.0   0.699951    1.979538   0.543282    2.499380  0.001
37    37.0   0.712891    1.934635   0.527255    2.542209  0.001
38    38.0   0.739502    1.873966   0.547994    2.494167  0.001
39    39.0   0.742676    1.853551   0.529983    2.550860  0.001
40    40.0   0.767822    1.777726   0.583987    2.373185  0.001
41    41.0   0.786499    1.724859   0.590365    2.323703  0.001
42    42.0   0.784668    1.714118   0.573500    2.391446  0.001
43    43.0   0.813721    1.656285   0.591730    2.352627  0.001
44    44.0   0.815308    1.649257   0.571362    2.386292  0.001
45    45.0   0.830811    1.600798   0.562960    2.444122  0.001
46    46.0   0.850586    1.558927   0.610068    2.273154  0.001
47    47.0   0.865967    1.513121   0.582266    2.372896  0.001
48    48.0   0.869019    1.499498   0.610648    2.265250  0.001
49    49.0   0.872803    1.483687   0.608322    2.293401  0.001
50    50.0   0.880127    1.467360   0.612508    2.263915  0.001
51    51.0   0.884766    1.452597   0.612648    2.264849  0.001
52    52.0   0.891968    1.427704   0.595117    2.329081  0.001
53    53.0   0.896606    1.414758   0.613059    2.283201  0.001
54    54.0   0.898560    1.409393   0.615857    2.271720  0.001
55    55.0   0.904785    1.385173   0.623684    2.230139  0.001
56    56.0   0.917236    1.360004   0.619825    2.275473  0.001
57    57.0   0.915894    1.353726   0.652176    2.131681  0.001
58    58.0   0.923950    1.339918   0.622608    2.235571  0.001
59    59.0   0.913940    1.351497   0.638069    2.193947  0.001
60    60.0   0.913696    1.347603   0.591908    2.341846  0.001
61    61.0   0.922485    1.331270   0.605718    2.321970  0.001
62    62.0   0.924805    1.319116   0.623972    2.252273  0.001
63    63.0   0.930054    1.302873   0.597628    2.322484  0.001
64    64.0   0.936523    1.294445   0.618555    2.267593  0.001
65    65.0   0.938477    1.281042   0.649820    2.158184  0.001
66    66.0   0.932861    1.292953   0.631125    2.235116  0.001
67    67.0   0.930664    1.295790   0.637062    2.199907  0.001
68    68.0   0.932495    1.293988   0.631125    2.230678  0.001
69    69.0   0.937988    1.278527   0.665514    2.110049  0.001
70    70.0   0.951782    1.240983   0.638293    2.174909  0.001
71    71.0   0.943359    1.259510   0.639999    2.196996  0.001
72    72.0   0.939819    1.268679   0.603480    2.334995  0.001
73    73.0   0.943481    1.260683   0.626536    2.236996  0.001
74    74.0   0.949341    1.251263   0.653322    2.172210  0.001
75    75.0   0.950562    1.235626   0.647727    2.154852  0.001
76    76.0   0.955078    1.225897   0.590901    2.364685  0.001
77    77.0   0.944702    1.255678   0.651547    2.156271  0.001
78    78.0   0.955200    1.225318   0.638689    2.206898  0.001
79    79.0   0.947754    1.236640   0.626491    2.268452  0.001
80    80.0   0.950439    1.237642   0.653749    2.167768  0.001
81    81.0   0.948730    1.237761   0.641526    2.191030  0.001
82    82.0   0.948242    1.240964   0.657965    2.147025  0.001
83    83.0   0.952148    1.219235   0.645068    2.166453  0.001
84    84.0   0.947388    1.234930   0.673535    2.066857  0.001
85    85.0   0.947266    1.235990   0.640038    2.213666  0.001
86    86.0   0.941406    1.253777   0.660212    2.136231  0.001
87    87.0   0.953247    1.219928   0.667707    2.120062  0.001
88    88.0   0.963867    1.190186   0.651958    2.179150  0.001
89    89.0   0.953979    1.202853   0.662910    2.105636  0.001
90    90.0   0.954834    1.219551   0.634473    2.211272  0.001
91    91.0   0.951050    1.225600   0.649175    2.181311  0.001
92    92.0   0.951416    1.220962   0.648749    2.187486  0.001
93    93.0   0.953491    1.215919   0.646695    2.195616  0.001
94    94.0   0.953979    1.215028   0.642712    2.189807  0.001
95    95.0   0.947998    1.233269   0.656973    2.141221  0.001
96    96.0   0.963745    1.193990   0.669265    2.102542  0.001
97    97.0   0.962402    1.187358   0.689285    2.061585  0.001
98    98.0   0.963867    1.170296   0.683223    2.054642  0.001
99    99.0   0.961060    1.175393   0.661422    2.130213  0.001
100  100.0   0.967529    1.164303   0.645867    2.184282  0.001
101  101.0   0.965576    1.174597   0.658833    2.135612  0.001
102  102.0   0.963745    1.177576   0.659429    2.146379  0.001
103  103.0   0.960449    1.186726   0.656244    2.156046  0.001
104  104.0   0.952515    1.216658   0.632667    2.220156  0.001
105  105.0   0.955933    1.204010   0.634210    2.253039  0.001
106  106.0   0.961914    1.191585   0.656794    2.137491  0.001
107  107.0   0.944824    1.235994   0.652509    2.160298  0.001
108  108.0   0.946533    1.222334   0.673814    2.105256  0.001
109  109.0   0.958618    1.192713   0.673233    2.075365  0.001
110  110.0   0.964355    1.166062   0.677156    2.081413  0.001
111  111.0   0.970947    1.158887   0.689121    2.064586  0.001
112  112.0   0.971313    1.139564   0.663119    2.158768  0.001
113  113.0   0.968628    1.152446   0.649191    2.191204  0.001

EXPERIMENT: C-23
================

NAME: ghost_20200919214527

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3411972.0,
 'optimizer': 'AdamW',
 'out_channels': 640.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.008179    5.381639   0.010293    6.044212  0.001
1     1.0   0.010742    5.305378   0.011657    6.062918  0.001
2     2.0   0.013550    5.253799   0.011781    5.441685  0.001
3     3.0   0.015503    5.185487   0.014385    5.281303  0.001
4     4.0   0.018799    5.120309   0.022941    5.090528  0.001
5     5.0   0.021973    5.039453   0.025546    5.015958  0.001
6     6.0   0.029663    4.956580   0.024623    5.124753  0.001
7     7.0   0.034912    4.875382   0.029514    4.999076  0.001
8     8.0   0.043335    4.787013   0.043914    4.796431  0.001
9     9.0   0.051392    4.674315   0.054083    4.684862  0.001
10   10.0   0.066162    4.549122   0.048185    4.734004  0.001
11   11.0   0.079590    4.409182   0.056811    4.834194  0.001
12   12.0   0.097900    4.286135   0.087882    4.347962  0.001
13   13.0   0.115479    4.145653   0.095547    4.292016  0.001
14   14.0   0.137817    4.029398   0.134183    4.092810  0.001
15   15.0   0.156982    3.896206   0.148831    3.958392  0.001
16   16.0   0.185425    3.776311   0.163131    3.929663  0.001
17   17.0   0.212280    3.644764   0.199302    3.728870  0.001
18   18.0   0.235962    3.534127   0.172323    3.935026  0.001
19   19.0   0.276489    3.394733   0.228994    3.622957  0.001
20   20.0   0.312012    3.272678   0.256128    3.440861  0.001
21   21.0   0.345703    3.132855   0.284456    3.388226  0.001
22   22.0   0.363892    3.044938   0.310731    3.269594  0.001
23   23.0   0.399292    2.919822   0.359333    3.110905  0.001
24   24.0   0.434814    2.811319   0.345181    3.143914  0.001
25   25.0   0.454712    2.728350   0.369486    3.065368  0.001
26   26.0   0.497437    2.611602   0.387800    3.033639  0.001
27   27.0   0.520874    2.515007   0.424467    2.887236  0.001
28   28.0   0.548340    2.417953   0.429739    2.870064  0.001
29   29.0   0.573730    2.355009   0.454547    2.797886  0.001
30   30.0   0.597168    2.259730   0.465901    2.743572  0.001
31   31.0   0.629517    2.168319   0.490276    2.653206  0.001
32   32.0   0.653442    2.095487   0.492741    2.669891  0.001
33   33.0   0.685181    2.021685   0.504978    2.628524  0.001
34   34.0   0.696655    1.961768   0.531347    2.550735  0.001
35   35.0   0.724976    1.895315   0.501645    2.636140  0.001
36   36.0   0.744385    1.851723   0.544002    2.485316  0.001
37   37.0   0.759033    1.787832   0.542275    2.493612  0.001
38   38.0   0.790039    1.712288   0.565083    2.413676  0.001
39   39.0   0.805786    1.673891   0.550916    2.493839  0.001
40   40.0   0.823608    1.621548   0.539522    2.531620  0.001
41   41.0   0.830688    1.595354   0.550241    2.510666  0.001
42   42.0   0.846436    1.556527   0.573437    2.413914  0.001
43   43.0   0.855591    1.529157   0.591705    2.356673  0.001
44   44.0   0.877930    1.483377   0.572895    2.394794  0.001
45   45.0   0.885620    1.448421   0.557488    2.468080  0.001
46   46.0   0.890503    1.435753   0.592791    2.340202  0.001
47   47.0   0.900757    1.407200   0.587032    2.388270  0.001
48   48.0   0.906616    1.384445   0.609315    2.304852  0.001
49   49.0   0.917480    1.355331   0.621537    2.253890  0.001
50   50.0   0.920532    1.337342   0.549304    2.533692  0.001
51   51.0   0.919556    1.340383   0.602890    2.337768  0.001
52   52.0   0.925171    1.316403   0.610183    2.316107  0.001
53   53.0   0.934448    1.296846   0.618972    2.267542  0.001
54   54.0   0.932007    1.300593   0.618050    2.250252  0.001
55   55.0   0.938477    1.277140   0.593620    2.366020  0.001
56   56.0   0.935547    1.285466   0.620693    2.262131  0.001
57   57.0   0.935181    1.285455   0.608873    2.297570  0.001
58   58.0   0.939331    1.273911   0.620351    2.266181  0.001
59   59.0   0.946289    1.252706   0.611105    2.291758  0.001
60   60.0   0.947144    1.249366   0.617980    2.278516  0.001
61   61.0   0.949829    1.236433   0.590613    2.342665  0.001
62   62.0   0.952637    1.231634   0.606680    2.322737  0.001
63   63.0   0.950928    1.231248   0.631264    2.257663  0.001
64   64.0   0.955688    1.225688   0.603223    2.315504  0.001
65   65.0   0.955444    1.226777   0.628645    2.265401  0.001
66   66.0   0.950928    1.223409   0.612965    2.293135  0.001
67   67.0   0.951660    1.230149   0.589304    2.409285  0.001
68   68.0   0.956787    1.212944   0.626373    2.269150  0.001
69   69.0   0.961304    1.195776   0.619383    2.287545  0.001

EXPERIMENT: C-24
================

NAME: ghost_20200919234654

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3782532.0,
 'optimizer': 'AdamW',
 'out_channels': 960.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0     0.0   0.005981    5.413719   0.009921    6.208075  0.001
1     1.0   0.012695    5.384357   0.012346    7.020339  0.001
2     2.0   0.012451    5.316631   0.013021    6.772408  0.001
3     3.0   0.019043    5.194304   0.015501    6.111758  0.001
4     4.0   0.020996    5.112588   0.025670    5.221116  0.001
5     5.0   0.029785    5.025302   0.029514    5.073890  0.001
6     6.0   0.032593    4.949262   0.030993    5.156346  0.001
7     7.0   0.037354    4.845438   0.038938    4.874766  0.001
8     8.0   0.046875    4.742359   0.049355    4.917341  0.001
9     9.0   0.061157    4.621039   0.060833    4.736598  0.001
10   10.0   0.068237    4.524816   0.067584    4.605683  0.001
11   11.0   0.085449    4.411043   0.069505    4.672610  0.001
12   12.0   0.101440    4.301467   0.089851    4.511621  0.001
13   13.0   0.117920    4.174234   0.103988    4.458918  0.001
14   14.0   0.140259    4.031995   0.128385    4.255852  0.001
15   15.0   0.173706    3.866863   0.132401    4.204988  0.001
16   16.0   0.202881    3.740786   0.177694    3.959047  0.001
17   17.0   0.230103    3.617911   0.202248    4.059395  0.001
18   18.0   0.261719    3.465315   0.216902    3.788931  0.001
19   19.0   0.303345    3.322026   0.271789    3.487144  0.001
20   20.0   0.340454    3.170746   0.281858    3.479478  0.001
21   21.0   0.367432    3.047785   0.298290    3.436953  0.001
22   22.0   0.410767    2.905018   0.345707    3.189502  0.001
23   23.0   0.445068    2.781203   0.338841    3.265844  0.001
24   24.0   0.478027    2.659938   0.386118    3.030423  0.001
25   25.0   0.514282    2.552212   0.390737    3.019290  0.001
26   26.0   0.557007    2.424427   0.411098    2.936584  0.001
27   27.0   0.579834    2.326159   0.435480    2.880869  0.001
28   28.0   0.596558    2.266592   0.432797    2.882792  0.001
29   29.0   0.627808    2.186647   0.453376    2.785292  0.001
30   30.0   0.659180    2.091223   0.485376    2.711319  0.001
31   31.0   0.684570    2.019312   0.504419    2.631824  0.001
32   32.0   0.713989    1.924577   0.508079    2.621997  0.001
33   33.0   0.736206    1.870693   0.511264    2.616455  0.001
34   34.0   0.761353    1.803766   0.504482    2.670427  0.001
35   35.0   0.775269    1.750205   0.516735    2.614184  0.001
36   36.0   0.807373    1.677317   0.543028    2.500838  0.001
37   37.0   0.819580    1.630025   0.546467    2.501449  0.001
38   38.0   0.838257    1.583545   0.544622    2.522369  0.001
39   39.0   0.847412    1.554854   0.521332    2.595331  0.001
40   40.0   0.862793    1.529174   0.554681    2.477404  0.001
41   41.0   0.879761    1.482525   0.577438    2.408282  0.001
42   42.0   0.886841    1.451190   0.551031    2.521159  0.001
43   43.0   0.899414    1.423196   0.584939    2.379623  0.001
44   44.0   0.902222    1.394503   0.575391    2.449328  0.001
45   45.0   0.914429    1.367873   0.565564    2.456888  0.001
46   46.0   0.916992    1.354300   0.558913    2.473912  0.001
47   47.0   0.917236    1.340957   0.585723    2.383014  0.001
48   48.0   0.921387    1.348270   0.541095    2.539781  0.001
49   49.0   0.926514    1.325422   0.598347    2.347247  0.001
50   50.0   0.928101    1.315386   0.589032    2.390393  0.001
51   51.0   0.934448    1.291293   0.584746    2.433415  0.001
52   52.0   0.947388    1.262151   0.588412    2.382360  0.001
53   53.0   0.942383    1.272367   0.588381    2.372241  0.001
54   54.0   0.947632    1.247412   0.619746    2.274470  0.001
55   55.0   0.953735    1.234900   0.615896    2.300379  0.001
56   56.0   0.953369    1.231555   0.601944    2.354554  0.001
57   57.0   0.955688    1.228044   0.621646    2.279131  0.001
58   58.0   0.959473    1.219971   0.600952    2.362125  0.001
59   59.0   0.952271    1.229422   0.591055    2.393994  0.001
60   60.0   0.957031    1.210429   0.591249    2.405700  0.001
61   61.0   0.961304    1.205551   0.589032    2.409702  0.001
62   62.0   0.960449    1.203550   0.596442    2.385040  0.001
63   63.0   0.962769    1.189152   0.617305    2.292460  0.001
64   64.0   0.962646    1.201569   0.617088    2.324536  0.001
65   65.0   0.951050    1.226555   0.612112    2.305948  0.001
66   66.0   0.961792    1.194476   0.631179    2.264802  0.001
67   67.0   0.964111    1.175166   0.594195    2.378476  0.001
68   68.0   0.968628    1.170135   0.619135    2.299155  0.001
69   69.0   0.964600    1.181257   0.605842    2.341226  0.001
70   70.0   0.973877    1.156112   0.623536    2.255113  0.001
71   71.0   0.974243    1.148615   0.636294    2.256796  0.001
72   72.0   0.964600    1.186486   0.620034    2.317744  0.001
73   73.0   0.966431    1.179966   0.621854    2.310792  0.001
74   74.0   0.962158    1.183196   0.609384    2.318889  0.001
75   75.0   0.960205    1.188465   0.622111    2.282357  0.001
76   76.0   0.973145    1.152107   0.628660    2.259934  0.001
77   77.0   0.972412    1.159733   0.628675    2.287462  0.001
78   78.0   0.972168    1.147627   0.646408    2.232450  0.001
79   79.0   0.978882    1.128554   0.631993    2.247048  0.001
80   80.0   0.982300    1.122631   0.646928    2.202305  0.001
81   81.0   0.979370    1.120992   0.627529    2.291113  0.001
82   82.0   0.976807    1.138839   0.643441    2.221086  0.001
83   83.0   0.973755    1.145792   0.612524    2.318403  0.001
84   84.0   0.971924    1.154561   0.615391    2.348759  0.001
85   85.0   0.965210    1.176217   0.619438    2.316278  0.001
86   86.0   0.965576    1.179598   0.631869    2.290872  0.001
87   87.0   0.970215    1.156466   0.586730    2.452030  0.001
88   88.0   0.970825    1.153576   0.649175    2.204817  0.001
89   89.0   0.980957    1.113888   0.649647    2.210285  0.001
90   90.0   0.980103    1.108189   0.645431    2.226207  0.001
91   91.0   0.980347    1.116015   0.615887    2.335062  0.001
92   92.0   0.971924    1.148987   0.646983    2.206817  0.001
93   93.0   0.969482    1.146428   0.629721    2.277123  0.001
94   94.0   0.977295    1.129264   0.640208    2.236712  0.001
95   95.0   0.977539    1.127238   0.646974    2.225876  0.001

EXPERIMENT: C-25
================

NAME: ghost_20200920074437

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'None',
 'lr_scheduler_params': 'None',
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4523652.0,
 'optimizer': 'AdamW',
 'out_channels': 1600.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss     lr
0      0.0   0.007568    5.488760   0.008433    5.355661  0.001
1      1.0   0.009155    5.476677   0.010913   22.421841  0.001
2      2.0   0.012695    5.383978   0.013021    5.622970  0.001
3      3.0   0.012817    5.337908   0.013269    5.473814  0.001
4      4.0   0.014893    5.209933   0.021949    5.149271  0.001
5      5.0   0.020142    5.161459   0.028026    6.078528  0.001
6      6.0   0.022583    5.078140   0.027530    5.611857  0.001
7      7.0   0.025879    5.019729   0.030506    5.085618  0.001
8      8.0   0.029785    4.969121   0.033606    5.030260  0.001
9      9.0   0.038452    4.898499   0.034970    5.069253  0.001
10    10.0   0.047607    4.833485   0.046999    4.900582  0.001
11    11.0   0.050903    4.750188   0.042411    4.952561  0.001
12    12.0   0.062134    4.670527   0.059896    4.985010  0.001
13    13.0   0.066650    4.565989   0.067421    5.061105  0.001
14    14.0   0.077637    4.476603   0.063438    4.647849  0.001
15    15.0   0.090820    4.370484   0.073978    4.793132  0.001
16    16.0   0.107178    4.254798   0.097555    4.534835  0.001
17    17.0   0.131470    4.129309   0.105413    5.526864  0.001
18    18.0   0.151001    4.022773   0.103513    4.612706  0.001
19    19.0   0.170898    3.906073   0.132223    4.884727  0.001
20    20.0   0.207397    3.730211   0.166370    4.276256  0.001
21    21.0   0.238281    3.561492   0.178299    4.124205  0.001
22    22.0   0.270020    3.430892   0.216494    3.883284  0.001
23    23.0   0.321655    3.246574   0.249564    3.934431  0.001
24    24.0   0.357788    3.106698   0.257096    3.790072  0.001
25    25.0   0.387939    2.969705   0.282953    5.845692  0.001
26    26.0   0.445068    2.817140   0.298257    3.561083  0.001
27    27.0   0.484619    2.651778   0.348852    3.271263  0.001
28    28.0   0.519775    2.535050   0.352963    3.260818  0.001
29    29.0   0.558838    2.419479   0.322372    4.310863  0.001
30    30.0   0.594360    2.315274   0.399968    3.091101  0.001
31    31.0   0.638062    2.168789   0.410354    2.967006  0.001
32    32.0   0.664795    2.078129   0.421926    3.049486  0.001
33    33.0   0.686768    2.024142   0.462958    2.811466  0.001
34    34.0   0.728638    1.901146   0.452106    2.860749  0.001
35    35.0   0.766602    1.805977   0.444711    2.961933  0.001
36    36.0   0.784790    1.739470   0.458336    2.864498  0.001
37    37.0   0.820557    1.650749   0.478985    2.780384  0.001
38    38.0   0.842896    1.596803   0.495896    2.693692  0.001
39    39.0   0.859985    1.537767   0.506506    2.691029  0.001
40    40.0   0.878418    1.496471   0.478519    2.785702  0.001
41    41.0   0.896362    1.442263   0.520603    2.626554  0.001
42    42.0   0.910889    1.401593   0.529771    2.591930  0.001
43    43.0   0.919312    1.372629   0.550852    2.534266  0.001
44    44.0   0.926880    1.341847   0.513000    2.667762  0.001
45    45.0   0.934937    1.324136   0.554101    2.493833  0.001
46    46.0   0.934692    1.307739   0.538312    2.579416  0.001
47    47.0   0.946533    1.284000   0.558193    2.517419  0.001
48    48.0   0.946777    1.277080   0.558635    2.508945  0.001
49    49.0   0.951782    1.251093   0.556006    2.526842  0.001
50    50.0   0.960938    1.228462   0.556433    2.507008  0.001
51    51.0   0.955322    1.238400   0.556699    2.536994  0.001
52    52.0   0.958984    1.234878   0.570122    2.484500  0.001
53    53.0   0.963501    1.224093   0.554945    2.520242  0.001
54    54.0   0.962769    1.210580   0.568377    2.485384  0.001
55    55.0   0.968994    1.190550   0.559131    2.511721  0.001
56    56.0   0.969360    1.192224   0.575539    2.464901  0.001
57    57.0   0.965820    1.193859   0.566734    2.485169  0.001
58    58.0   0.972046    1.177339   0.564517    2.498365  0.001
59    59.0   0.969238    1.177912   0.549667    2.576618  0.001
60    60.0   0.970459    1.174796   0.579692    2.446965  0.001
61    61.0   0.964600    1.198881   0.541670    2.566995  0.001
62    62.0   0.970337    1.180578   0.549116    2.585004  0.001
63    63.0   0.970337    1.180105   0.560967    2.539216  0.001
64    64.0   0.972412    1.167758   0.585683    2.437434  0.001
65    65.0   0.973633    1.163507   0.573709    2.485569  0.001
66    66.0   0.966797    1.185192   0.596775    2.388326  0.001
67    67.0   0.969116    1.173409   0.587404    2.426925  0.001
68    68.0   0.977417    1.141130   0.593451    2.396932  0.001
69    69.0   0.982788    1.121611   0.587235    2.422181  0.001
70    70.0   0.977295    1.138033   0.599851    2.380895  0.001
71    71.0   0.979126    1.129145   0.596766    2.409869  0.001
72    72.0   0.972900    1.157480   0.571894    2.488937  0.001
73    73.0   0.975952    1.134574   0.579553    2.484882  0.001
74    74.0   0.980713    1.123775   0.577484    2.474051  0.001
75    75.0   0.982056    1.114619   0.593783    2.385814  0.001
76    76.0   0.982666    1.119611   0.610555    2.372145  0.001
77    77.0   0.981079    1.124934   0.561269    2.517478  0.001
78    78.0   0.977905    1.134651   0.572654    2.475606  0.001
79    79.0   0.979248    1.130821   0.588838    2.433722  0.001
80    80.0   0.978027    1.125956   0.605873    2.390328  0.001
81    81.0   0.981323    1.117630   0.609408    2.362830  0.001
82    82.0   0.981323    1.116874   0.593883    2.408702  0.001
83    83.0   0.982056    1.122922   0.588675    2.451623  0.001
84    84.0   0.982422    1.121920   0.605216    2.353756  0.001
85    85.0   0.976929    1.141527   0.602564    2.381389  0.001
86    86.0   0.982544    1.112932   0.579250    2.485306  0.001
87    87.0   0.985107    1.101198   0.594046    2.418318  0.001
88    88.0   0.984619    1.094434   0.626382    2.305393  0.001
89    89.0   0.987061    1.083463   0.596403    2.414091  0.001
90    90.0   0.984009    1.091388   0.623754    2.293749  0.001
91    91.0   0.983032    1.102583   0.588536    2.430663  0.001
92    92.0   0.979980    1.115938   0.608834    2.361760  0.001
93    93.0   0.978149    1.122123   0.576105    2.476680  0.001
94    94.0   0.972656    1.149089   0.598039    2.417632  0.001
95    95.0   0.977417    1.124962   0.559905    2.542836  0.001
96    96.0   0.981567    1.111680   0.621428    2.378396  0.001
97    97.0   0.987305    1.090740   0.623442    2.328459  0.001
98    98.0   0.989868    1.070872   0.628868    2.311677  0.001
99    99.0   0.987549    1.075043   0.598735    2.423430  0.001
100  100.0   0.988525    1.072453   0.631085    2.315411  0.001
101  101.0   0.983032    1.085894   0.617925    2.357263  0.001
102  102.0   0.981567    1.107292   0.613561    2.387879  0.001
103  103.0   0.978516    1.121227   0.582102    2.456992  0.001
104  104.0   0.983032    1.099815   0.618546    2.362370  0.001
105  105.0   0.985474    1.090512   0.620560    2.311382  0.001

EXPERIMENT: C-26
================

NAME: ghost_20200920103637

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.008179    5.338798   0.009921    5.360194  1.000000e-03
1      1.0   0.010254    5.262289   0.009177    5.226869  1.000000e-03
2      2.0   0.013916    5.202191   0.013393    5.574546  1.000000e-03
3      3.0   0.014160    5.146494   0.019593    5.125531  1.000000e-03
4      4.0   0.018311    5.099011   0.020461    5.101171  1.000000e-03
5      5.0   0.023315    5.037823   0.027282    5.020231  1.000000e-03
6      6.0   0.028320    4.981892   0.029390    5.072357  1.000000e-03
7      7.0   0.032227    4.918103   0.037148    4.941700  1.000000e-03
8      8.0   0.040771    4.824995   0.037024    4.873784  1.000000e-03
9      9.0   0.053223    4.694360   0.043046    4.885151  1.000000e-03
10    10.0   0.062378    4.578166   0.053245    4.739239  1.000000e-03
11    11.0   0.074097    4.453693   0.054887    4.676527  1.000000e-03
12    12.0   0.089478    4.335937   0.089286    4.387362  1.000000e-03
13    13.0   0.105225    4.213319   0.061647    4.571652  1.000000e-03
14    14.0   0.125244    4.107619   0.108577    4.225321  1.000000e-03
15    15.0   0.148804    3.982662   0.099082    4.369476  1.000000e-03
16    16.0   0.173584    3.862523   0.142313    3.994473  1.000000e-03
17    17.0   0.197144    3.742653   0.172562    3.931270  1.000000e-03
18    18.0   0.222534    3.639823   0.219757    3.691189  1.000000e-03
19    19.0   0.250244    3.525725   0.191471    3.786729  1.000000e-03
20    20.0   0.278564    3.401886   0.230286    3.619325  1.000000e-03
21    21.0   0.302002    3.299161   0.277357    3.404263  1.000000e-03
22    22.0   0.329468    3.168509   0.265032    3.486413  1.000000e-03
23    23.0   0.362061    3.058644   0.305492    3.279363  1.000000e-03
24    24.0   0.398926    2.945509   0.315026    3.280324  1.000000e-03
25    25.0   0.423950    2.849886   0.362012    3.124828  1.000000e-03
26    26.0   0.457275    2.754454   0.385925    3.012423  1.000000e-03
27    27.0   0.486572    2.647128   0.404124    2.946019  1.000000e-03
28    28.0   0.501343    2.571950   0.411991    2.913707  1.000000e-03
29    29.0   0.538330    2.475351   0.426351    2.882676  1.000000e-03
30    30.0   0.570435    2.386846   0.467086    2.750071  1.000000e-03
31    31.0   0.593872    2.295374   0.454129    2.792597  1.000000e-03
32    32.0   0.619385    2.222872   0.459809    2.742926  1.000000e-03
33    33.0   0.642334    2.166121   0.483737    2.654669  1.000000e-03
34    34.0   0.656860    2.104225   0.504337    2.613170  1.000000e-03
35    35.0   0.683472    2.036869   0.534701    2.507779  1.000000e-03
36    36.0   0.707764    1.967115   0.512110    2.590318  1.000000e-03
37    37.0   0.712891    1.937366   0.563292    2.428457  1.000000e-03
38    38.0   0.742188    1.858227   0.560425    2.422321  1.000000e-03
39    39.0   0.766357    1.804272   0.570981    2.387551  1.000000e-03
40    40.0   0.774048    1.761103   0.568564    2.420994  1.000000e-03
41    41.0   0.787842    1.730680   0.569091    2.425722  1.000000e-03
42    42.0   0.801880    1.692764   0.589691    2.363555  1.000000e-03
43    43.0   0.807983    1.673531   0.548644    2.485265  1.000000e-03
44    44.0   0.823120    1.625724   0.582995    2.368603  1.000000e-03
45    45.0   0.828979    1.603050   0.560092    2.431496  1.000000e-03
46    46.0   0.845825    1.562806   0.602007    2.274467  1.000000e-03
47    47.0   0.857788    1.531220   0.606819    2.306106  1.000000e-03
48    48.0   0.874512    1.489414   0.615013    2.266570  1.000000e-03
49    49.0   0.869019    1.492334   0.589428    2.360901  1.000000e-03
50    50.0   0.881592    1.460981   0.586506    2.383101  1.000000e-03
51    51.0   0.887329    1.444110   0.598659    2.336207  1.000000e-03
52    52.0   0.892334    1.422558   0.607176    2.311013  1.000000e-03
53    53.0   0.897095    1.416723   0.616214    2.297538  1.000000e-03
54    54.0   0.914673    1.371572   0.621507    2.259623  1.000000e-03
55    55.0   0.923828    1.342036   0.581065    2.423026  1.000000e-03
56    56.0   0.913818    1.377495   0.624150    2.242936  1.000000e-03
57    57.0   0.917480    1.354577   0.620723    2.246340  1.000000e-03
58    58.0   0.920288    1.338297   0.601629    2.317295  1.000000e-03
59    59.0   0.928589    1.321920   0.634884    2.217979  1.000000e-03
60    60.0   0.927124    1.324234   0.631179    2.226789  1.000000e-03
61    61.0   0.923950    1.337148   0.625756    2.222925  1.000000e-03
62    62.0   0.928833    1.306406   0.585475    2.371663  1.000000e-03
63    63.0   0.932495    1.304487   0.609454    2.312153  1.000000e-03
64    64.0   0.938477    1.282411   0.646423    2.198082  1.000000e-03
65    65.0   0.937866    1.280743   0.634527    2.235098  1.000000e-03
66    66.0   0.948486    1.254783   0.631845    2.243996  1.000000e-03
67    67.0   0.938965    1.281925   0.611066    2.313791  1.000000e-03
68    68.0   0.936035    1.277439   0.619801    2.284457  1.000000e-03
69    69.0   0.939697    1.277801   0.590492    2.400514  1.000000e-03
70    70.0   0.937500    1.274709   0.633039    2.213869  1.000000e-03
71    71.0   0.977173    1.142630   0.719062    1.928973  1.000000e-04
72    72.0   0.989624    1.094633   0.720480    1.930138  1.000000e-04
73    73.0   0.991943    1.073421   0.725316    1.920664  1.000000e-04
74    74.0   0.992798    1.062600   0.729657    1.916837  1.000000e-04
75    75.0   0.993774    1.056339   0.729106    1.919116  1.000000e-04
76    76.0   0.993286    1.051296   0.727796    1.923712  1.000000e-04
77    77.0   0.995483    1.039912   0.732578    1.916096  1.000000e-04
78    78.0   0.994629    1.035702   0.735500    1.916428  1.000000e-04
79    79.0   0.993896    1.033873   0.737663    1.909219  1.000000e-04
80    80.0   0.996338    1.028380   0.736740    1.920441  1.000000e-04
81    81.0   0.996460    1.022049   0.741150    1.906086  1.000000e-04
82    82.0   0.996338    1.019907   0.740778    1.904139  1.000000e-04
83    83.0   0.996216    1.017685   0.742514    1.902696  1.000000e-04
84    84.0   0.996094    1.017264   0.737484    1.915915  1.000000e-04
85    85.0   0.996338    1.014291   0.738422    1.917525  1.000000e-04
86    86.0   0.996582    1.010159   0.740972    1.902596  1.000000e-04
87    87.0   0.997192    1.006172   0.741081    1.920448  1.000000e-04
88    88.0   0.996826    1.005169   0.740406    1.918622  1.000000e-04
89    89.0   0.997925    1.001057   0.743933    1.912077  1.000000e-05
90    90.0   0.998169    0.998225   0.745049    1.906371  1.000000e-05
91    91.0   0.997192    0.998590   0.745049    1.907914  1.000000e-05
92    92.0   0.997314    0.997532   0.744305    1.904002  1.000000e-05
93    93.0   0.996826    0.999431   0.744181    1.910498  1.000000e-05
94    94.0   0.997559    0.996307   0.744925    1.908047  1.000000e-05
95    95.0   0.997803    0.996547   0.745366    1.906353  1.000000e-06
96    96.0   0.997437    0.995956   0.745669    1.906266  1.000000e-06
97    97.0   0.997437    0.995991   0.746041    1.903306  1.000000e-06
98    98.0   0.997925    0.996673   0.745173    1.918131  1.000000e-06
99    99.0   0.997070    0.994776   0.743685    1.909958  1.000000e-06
100  100.0   0.997437    0.997005   0.746041    1.904957  1.000000e-06
101  101.0   0.997559    0.995103   0.744677    1.910175  1.000000e-07

EXPERIMENT: C-27
================

NAME: ghost_20200920083702

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.006714    5.344605   0.012897    5.383159  1.000000e-03
1      1.0   0.011353    5.250697   0.010045    5.310042  1.000000e-03
2      2.0   0.012695    5.197628   0.013834    5.208437  1.000000e-03
3      3.0   0.017700    5.136021   0.015323    5.186738  1.000000e-03
4      4.0   0.018799    5.078137   0.018492    5.122463  1.000000e-03
5      5.0   0.025391    5.019431   0.024817    5.045957  1.000000e-03
6      6.0   0.028687    4.949693   0.023026    5.043085  1.000000e-03
7      7.0   0.033081    4.854807   0.028095    4.956707  1.000000e-03
8      8.0   0.042114    4.748146   0.038899    4.867212  1.000000e-03
9      9.0   0.053589    4.610176   0.053106    4.653006  1.000000e-03
10    10.0   0.071777    4.494942   0.065328    4.538581  1.000000e-03
11    11.0   0.081909    4.377284   0.073016    4.426591  1.000000e-03
12    12.0   0.094482    4.259204   0.102406    4.262581  1.000000e-03
13    13.0   0.108765    4.164128   0.094098    4.239785  1.000000e-03
14    14.0   0.131958    4.042445   0.120581    4.180862  1.000000e-03
15    15.0   0.149536    3.935847   0.152124    3.965770  1.000000e-03
16    16.0   0.174927    3.818336   0.160140    3.900539  1.000000e-03
17    17.0   0.203857    3.714727   0.180864    3.835121  1.000000e-03
18    18.0   0.224487    3.585404   0.187754    3.763003  1.000000e-03
19    19.0   0.257568    3.488310   0.214764    3.639909  1.000000e-03
20    20.0   0.285278    3.360647   0.250641    3.524027  1.000000e-03
21    21.0   0.305908    3.274403   0.297710    3.332304  1.000000e-03
22    22.0   0.343140    3.137668   0.333648    3.218673  1.000000e-03
23    23.0   0.378418    3.023158   0.331773    3.200124  1.000000e-03
24    24.0   0.405518    2.920421   0.372889    3.063378  1.000000e-03
25    25.0   0.427612    2.828862   0.382809    3.017074  1.000000e-03
26    26.0   0.471191    2.713209   0.363410    3.054190  1.000000e-03
27    27.0   0.485352    2.634488   0.419546    2.882178  1.000000e-03
28    28.0   0.512207    2.550949   0.425940    2.849730  1.000000e-03
29    29.0   0.537842    2.466373   0.463018    2.721421  1.000000e-03
30    30.0   0.573608    2.373784   0.475186    2.688890  1.000000e-03
31    31.0   0.590820    2.299099   0.470553    2.686788  1.000000e-03
32    32.0   0.619873    2.223150   0.498128    2.607634  1.000000e-03
33    33.0   0.640015    2.147175   0.514194    2.550578  1.000000e-03
34    34.0   0.671753    2.069401   0.528262    2.505242  1.000000e-03
35    35.0   0.680542    2.026593   0.536129    2.463798  1.000000e-03
36    36.0   0.694092    1.981032   0.542910    2.465914  1.000000e-03
37    37.0   0.710205    1.943667   0.577523    2.355567  1.000000e-03
38    38.0   0.738037    1.870953   0.590861    2.349009  1.000000e-03
39    39.0   0.757690    1.800520   0.601481    2.273325  1.000000e-03
40    40.0   0.770996    1.775803   0.561457    2.379855  1.000000e-03
41    41.0   0.782593    1.744069   0.582538    2.347288  1.000000e-03
42    42.0   0.790283    1.704426   0.572091    2.366317  1.000000e-03
43    43.0   0.819214    1.638085   0.583887    2.333390  1.000000e-03
44    44.0   0.814209    1.646126   0.569602    2.395645  1.000000e-03
45    45.0   0.837524    1.577031   0.616695    2.235458  1.000000e-03
46    46.0   0.848511    1.558800   0.608153    2.265825  1.000000e-03
47    47.0   0.849121    1.548739   0.618415    2.228715  1.000000e-03
48    48.0   0.864990    1.504817   0.642270    2.173785  1.000000e-03
49    49.0   0.871826    1.482708   0.626715    2.208145  1.000000e-03
50    50.0   0.884277    1.449293   0.608952    2.264235  1.000000e-03
51    51.0   0.890503    1.431484   0.624785    2.220149  1.000000e-03
52    52.0   0.894531    1.412518   0.625926    2.225271  1.000000e-03
53    53.0   0.899292    1.407297   0.626219    2.197547  1.000000e-03
54    54.0   0.910645    1.372998   0.647990    2.141795  1.000000e-03
55    55.0   0.914185    1.368350   0.629676    2.216227  1.000000e-03
56    56.0   0.910034    1.364964   0.638496    2.169223  1.000000e-03
57    57.0   0.920898    1.335804   0.632622    2.191168  1.000000e-03
58    58.0   0.918701    1.350186   0.641194    2.173756  1.000000e-03
59    59.0   0.912109    1.361454   0.626397    2.219706  1.000000e-03
60    60.0   0.925415    1.328997   0.649557    2.142007  1.000000e-03
61    61.0   0.967896    1.187925   0.722325    1.897185  1.000000e-04
62    62.0   0.985107    1.128328   0.727905    1.879619  1.000000e-04
63    63.0   0.986816    1.110784   0.732191    1.874661  1.000000e-04
64    64.0   0.987671    1.102773   0.734617    1.867368  1.000000e-04
65    65.0   0.989624    1.092656   0.736477    1.866646  1.000000e-04
66    66.0   0.989868    1.084465   0.742980    1.866627  1.000000e-04
67    67.0   0.991211    1.077326   0.741368    1.867417  1.000000e-04
68    68.0   0.993164    1.072078   0.740887    1.863251  1.000000e-04
69    69.0   0.991699    1.068323   0.744840    1.853100  1.000000e-04
70    70.0   0.992188    1.065076   0.750436    1.853313  1.000000e-04
71    71.0   0.992554    1.063014   0.747142    1.852533  1.000000e-04
72    72.0   0.992920    1.057840   0.748382    1.853578  1.000000e-04
73    73.0   0.994019    1.051921   0.748506    1.848734  1.000000e-04
74    74.0   0.993774    1.047024   0.748134    1.852585  1.000000e-04
75    75.0   0.994995    1.046071   0.752211    1.844840  1.000000e-04
76    76.0   0.994995    1.045754   0.748863    1.857472  1.000000e-04
77    77.0   0.995728    1.039293   0.753645    1.844283  1.000000e-04
78    78.0   0.996704    1.035552   0.751536    1.849993  1.000000e-04
79    79.0   0.995117    1.034665   0.749979    1.851131  1.000000e-04
80    80.0   0.994995    1.032974   0.752405    1.853990  1.000000e-04
81    81.0   0.995483    1.032245   0.756993    1.845863  1.000000e-04
82    82.0   0.996094    1.026530   0.757985    1.842226  1.000000e-05
83    83.0   0.996826    1.024433   0.756993    1.842534  1.000000e-05
84    84.0   0.996582    1.023646   0.757667    1.840736  1.000000e-05
85    85.0   0.995239    1.024882   0.759915    1.841054  1.000000e-05
86    86.0   0.996826    1.023624   0.756001    1.838861  1.000000e-05
87    87.0   0.995850    1.021379   0.759046    1.841181  1.000000e-05
88    88.0   0.996826    1.019783   0.759101    1.841098  1.000000e-05
89    89.0   0.996216    1.018633   0.759349    1.839170  1.000000e-05
90    90.0   0.996338    1.020655   0.757310    1.839464  1.000000e-05
91    91.0   0.996704    1.018568   0.759155    1.838237  1.000000e-05
92    92.0   0.997437    1.015216   0.758977    1.847710  1.000000e-05
93    93.0   0.996094    1.017159   0.759845    1.840645  1.000000e-06
94    94.0   0.996826    1.015870   0.759473    1.840865  1.000000e-06
95    95.0   0.997192    1.015883   0.760093    1.842267  1.000000e-06
96    96.0   0.996704    1.019048   0.757861    1.840973  1.000000e-06
97    97.0   0.996216    1.018592   0.759543    1.835734  1.000000e-06
98    98.0   0.996582    1.020951   0.759101    1.840815  1.000000e-06
99    99.0   0.996704    1.018515   0.760287    1.838186  1.000000e-06
100  100.0   0.997803    1.016205   0.762023    1.838380  1.000000e-06
101  101.0   0.995972    1.017091   0.759225    1.845134  1.000000e-06
102  102.0   0.996826    1.016809   0.760589    1.836062  1.000000e-06
103  103.0   0.996216    1.017532   0.758109    1.843696  1.000000e-06
104  104.0   0.997314    1.016754   0.758853    1.837015  1.000000e-07
105  105.0   0.995728    1.017778   0.760589    1.841743  1.000000e-07
106  106.0   0.997437    1.017007   0.758675    1.839264  1.000000e-07
107  107.0   0.996948    1.017229   0.758977    1.846212  1.000000e-07
108  108.0   0.997559    1.015193   0.758729    1.849702  1.000000e-07
109  109.0   0.996948    1.016573   0.756621    1.841803  1.000000e-07
110  110.0   0.996948    1.014190   0.759473    1.843687  1.000000e-07
111  111.0   0.996704    1.017085   0.757737    1.844994  1.000000e-07
112  112.0   0.997559    1.014011   0.758109    1.842628  1.000000e-07

EXPERIMENT: C-28
================

NAME: ghost_20200920083703

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0      0.0   0.006592    5.341877   0.008433    5.307767  0.001000
1      1.0   0.012451    5.232094   0.012222    5.236853  0.001000
2      2.0   0.010620    5.198493   0.015571    5.293372  0.001000
3      3.0   0.013428    5.162438   0.018051    5.217777  0.001000
4      4.0   0.019409    5.116811   0.019112    5.104905  0.001000
5      5.0   0.022095    5.058050   0.024995    5.058031  0.001000
6      6.0   0.025879    4.998407   0.028909    4.971517  0.001000
7      7.0   0.034912    4.919934   0.040318    4.882009  0.001000
8      8.0   0.041748    4.815751   0.044549    4.863767  0.001000
9      9.0   0.051758    4.689357   0.052982    4.757956  0.001000
10    10.0   0.062744    4.562636   0.069048    4.562567  0.001000
11    11.0   0.074341    4.444382   0.087952    4.413666  0.001000
12    12.0   0.089600    4.330932   0.076474    4.435190  0.001000
13    13.0   0.109131    4.218907   0.101717    4.230186  0.001000
14    14.0   0.125732    4.102458   0.129386    4.096521  0.001000
15    15.0   0.145142    3.982707   0.144118    3.978280  0.001000
16    16.0   0.167725    3.872961   0.166836    3.897250  0.001000
17    17.0   0.192749    3.751758   0.165254    3.873468  0.001000
18    18.0   0.218018    3.654269   0.185607    3.754365  0.001000
19    19.0   0.251709    3.496253   0.221306    3.648269  0.001000
20    20.0   0.281738    3.395211   0.239287    3.582409  0.001000
21    21.0   0.313354    3.261451   0.269312    3.403684  0.001000
22    22.0   0.340942    3.174991   0.250602    3.489760  0.001000
23    23.0   0.367432    3.051129   0.329416    3.220170  0.001000
24    24.0   0.402344    2.949098   0.358573    3.120614  0.001000
25    25.0   0.421875    2.855864   0.385220    3.006316  0.001000
26    26.0   0.453979    2.757914   0.399526    2.988976  0.001000
27    27.0   0.482910    2.669888   0.420940    2.898096  0.001000
28    28.0   0.515137    2.543619   0.426382    2.874253  0.001000
29    29.0   0.545288    2.473926   0.422413    2.876288  0.001000
30    30.0   0.556030    2.408516   0.429149    2.842534  0.001000
31    31.0   0.587402    2.331389   0.478063    2.693463  0.001000
32    32.0   0.609375    2.247607   0.485890    2.676145  0.001000
33    33.0   0.637085    2.169691   0.493113    2.635662  0.001000
34    34.0   0.658203    2.108266   0.486773    2.683193  0.001000
35    35.0   0.675903    2.051205   0.501174    2.645468  0.001000
36    36.0   0.696167    1.993292   0.503832    2.611320  0.001000
37    37.0   0.711792    1.957085   0.519651    2.543344  0.001000
38    38.0   0.736938    1.894230   0.515202    2.583329  0.001000
39    39.0   0.734985    1.856742   0.550737    2.455439  0.001000
40    40.0   0.762207    1.786040   0.554234    2.471004  0.001000
41    41.0   0.771484    1.760716   0.579607    2.375561  0.001000
42    42.0   0.788696    1.727660   0.536957    2.484497  0.001000
43    43.0   0.806152    1.675151   0.572315    2.388984  0.001000
44    44.0   0.804565    1.669525   0.572563    2.417365  0.001000
45    45.0   0.833496    1.600631   0.598813    2.320576  0.001000
46    46.0   0.836426    1.586355   0.574027    2.401678  0.001000
47    47.0   0.847412    1.556688   0.584755    2.354296  0.001000
48    48.0   0.850952    1.537218   0.617136    2.238895  0.001000
49    49.0   0.872314    1.483582   0.618670    2.268123  0.001000
50    50.0   0.885376    1.454730   0.624428    2.219720  0.001000
51    51.0   0.884888    1.453749   0.593178    2.353885  0.001000
52    52.0   0.894165    1.432997   0.599031    2.340611  0.001000
53    53.0   0.885864    1.437897   0.619716    2.232226  0.001000
54    54.0   0.895020    1.414819   0.615004    2.278123  0.001000
55    55.0   0.900757    1.408776   0.632295    2.213046  0.001000
56    56.0   0.905518    1.383263   0.624180    2.228133  0.001000
57    57.0   0.914062    1.359593   0.622018    2.238950  0.001000
58    58.0   0.919067    1.358130   0.619979    2.249313  0.001000
59    59.0   0.919556    1.354337   0.628784    2.236736  0.001000
60    60.0   0.917725    1.346256   0.644518    2.196629  0.001000
61    61.0   0.926880    1.321173   0.611710    2.288473  0.001000
62    62.0   0.926514    1.315081   0.631993    2.217792  0.001000
63    63.0   0.927612    1.314065   0.628754    2.235214  0.001000
64    64.0   0.934326    1.299407   0.632389    2.220616  0.001000
65    65.0   0.930176    1.306146   0.630520    2.232106  0.001000
66    66.0   0.934204    1.295870   0.637062    2.199582  0.001000
67    67.0   0.974976    1.159089   0.716829    1.943287  0.000100
68    68.0   0.988770    1.106663   0.719627    1.925995  0.000100
69    69.0   0.990479    1.090309   0.723526    1.919019  0.000100
70    70.0   0.990967    1.077750   0.727370    1.911002  0.000100
71    71.0   0.992554    1.068875   0.726572    1.920554  0.000100
72    72.0   0.994385    1.061968   0.733392    1.909973  0.000100
73    73.0   0.994385    1.056582   0.732757    1.916645  0.000100
74    74.0   0.995117    1.050218   0.734066    1.913018  0.000100
75    75.0   0.994995    1.047622   0.737663    1.903617  0.000100
76    76.0   0.994873    1.044564   0.739290    1.905616  0.000100
77    77.0   0.995239    1.040124   0.740143    1.903351  0.000100
78    78.0   0.994873    1.038677   0.737236    1.906829  0.000100
79    79.0   0.995728    1.030968   0.737167    1.909830  0.000100
80    80.0   0.995239    1.031272   0.738848    1.895344  0.000100
81    81.0   0.995972    1.028993   0.736686    1.908594  0.000100
82    82.0   0.995361    1.023824   0.736051    1.907758  0.000100
83    83.0   0.994995    1.024337   0.739523    1.905325  0.000100
84    84.0   0.996216    1.019625   0.741453    1.907002  0.000100
85    85.0   0.996216    1.019215   0.744483    1.892623  0.000100
86    86.0   0.995972    1.019549   0.739468    1.904694  0.000100
87    87.0   0.996704    1.011080   0.743228    1.891642  0.000100
88    88.0   0.997192    1.011647   0.745599    1.891068  0.000100
89    89.0   0.996338    1.011604   0.744429    1.897063  0.000100
90    90.0   0.997070    1.010473   0.747102    1.888138  0.000100
91    91.0   0.996094    1.009560   0.744111    1.890056  0.000100
92    92.0   0.996338    1.009235   0.741825    1.898751  0.000100
93    93.0   0.997559    1.003440   0.747777    1.903691  0.000100
94    94.0   0.996094    1.001525   0.747901    1.895379  0.000100
95    95.0   0.997314    0.998041   0.746234    1.897994  0.000100
96    96.0   0.997070    0.997903   0.747831    1.900242  0.000100
97    97.0   0.998047    0.992560   0.747390    1.893556  0.000010
98    98.0   0.996704    0.992587   0.749568    1.895677  0.000010
99    99.0   0.996460    0.993355   0.749707    1.889577  0.000010
100  100.0   0.997803    0.991189   0.751001    1.892461  0.000010
101  101.0   0.997803    0.989949   0.751373    1.892998  0.000010
102  102.0   0.997192    0.989935   0.751304    1.896157  0.000010
103  103.0   0.997192    0.989682   0.751304    1.895273  0.000001
104  104.0   0.998291    0.987768   0.749816    1.895107  0.000001
105  105.0   0.997681    0.989173   0.751428    1.897871  0.000001

EXPERIMENT: C-29
================

NAME: ghost_20200920133231

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0     0.0   0.007568    5.360593   0.011781    5.281670  0.001000
1     1.0   0.008057    5.277685   0.012525    5.289021  0.001000
2     2.0   0.011841    5.205962   0.014757    5.428991  0.001000
3     3.0   0.017944    5.132228   0.016989    5.204909  0.001000
4     4.0   0.020386    5.079504   0.019841    5.912852  0.001000
5     5.0   0.022583    5.037016   0.025918    5.056877  0.001000
6     6.0   0.029053    4.967806   0.027902    5.039971  0.001000
7     7.0   0.034546    4.900592   0.030134    5.141024  0.001000
8     8.0   0.042847    4.807045   0.037450    4.992111  0.001000
9     9.0   0.047974    4.704005   0.048929    4.688344  0.001000
10   10.0   0.059082    4.601475   0.046533    4.790004  0.001000
11   11.0   0.071777    4.499675   0.066111    4.580364  0.001000
12   12.0   0.086548    4.400427   0.073776    4.545939  0.001000
13   13.0   0.089478    4.315210   0.086261    4.424755  0.001000
14   14.0   0.105835    4.192893   0.100785    4.286897  0.001000
15   15.0   0.125488    4.098870   0.106435    4.274473  0.001000
16   16.0   0.141968    3.999250   0.148785    4.060960  0.001000
17   17.0   0.164917    3.882882   0.147049    4.015862  0.001000
18   18.0   0.183716    3.760766   0.172903    3.861648  0.001000
19   19.0   0.209717    3.674135   0.175220    3.916324  0.001000
20   20.0   0.235229    3.555989   0.207045    3.706472  0.001000
21   21.0   0.270752    3.441969   0.220692    3.673704  0.001000
22   22.0   0.290771    3.344947   0.234272    3.588670  0.001000
23   23.0   0.320923    3.222490   0.271898    3.406439  0.001000
24   24.0   0.344360    3.118787   0.317651    3.265157  0.001000
25   25.0   0.378296    2.998621   0.329368    3.234908  0.001000
26   26.0   0.409302    2.910223   0.340955    3.161297  0.001000
27   27.0   0.441895    2.790080   0.385879    3.022466  0.001000
28   28.0   0.466553    2.700295   0.407983    2.948071  0.001000
29   29.0   0.500244    2.609207   0.412345    2.922698  0.001000
30   30.0   0.527954    2.517761   0.422776    2.902198  0.001000
31   31.0   0.548218    2.441169   0.442563    2.819632  0.001000
32   32.0   0.581299    2.353091   0.476076    2.719474  0.001000
33   33.0   0.601318    2.267940   0.462910    2.752001  0.001000
34   34.0   0.617310    2.216924   0.506542    2.604806  0.001000
35   35.0   0.639648    2.147675   0.518634    2.557898  0.001000
36   36.0   0.667236    2.075028   0.519557    2.563854  0.001000
37   37.0   0.697876    1.994950   0.537211    2.532008  0.001000
38   38.0   0.711426    1.940992   0.535242    2.504652  0.001000
39   39.0   0.731201    1.888873   0.543397    2.475410  0.001000
40   40.0   0.747437    1.846632   0.568455    2.411099  0.001000
41   41.0   0.757080    1.808816   0.553992    2.453344  0.001000
42   42.0   0.775146    1.751736   0.548000    2.505219  0.001000
43   43.0   0.788818    1.712301   0.560843    2.432070  0.001000
44   44.0   0.812866    1.663567   0.575996    2.380845  0.001000
45   45.0   0.829224    1.623484   0.595093    2.309053  0.001000
46   46.0   0.820923    1.621391   0.605153    2.288452  0.001000
47   47.0   0.835815    1.586418   0.613337    2.273432  0.001000
48   48.0   0.853516    1.541108   0.611571    2.272964  0.001000
49   49.0   0.861084    1.515741   0.579111    2.384845  0.001000
50   50.0   0.871948    1.482300   0.622320    2.233384  0.001000
51   51.0   0.879150    1.464575   0.600673    2.308249  0.001000
52   52.0   0.888550    1.444527   0.605564    2.295250  0.001000
53   53.0   0.894653    1.420418   0.616818    2.233043  0.001000
54   54.0   0.903076    1.393706   0.629885    2.227613  0.001000
55   55.0   0.904053    1.386176   0.619568    2.257905  0.001000
56   56.0   0.904419    1.379977   0.599061    2.316963  0.001000
57   57.0   0.906372    1.377558   0.596681    2.343617  0.001000
58   58.0   0.914795    1.356517   0.621328    2.244992  0.001000
59   59.0   0.915771    1.350529   0.640401    2.171700  0.001000
60   60.0   0.926392    1.317935   0.640961    2.182544  0.001000
61   61.0   0.920166    1.332649   0.625901    2.228934  0.001000
62   62.0   0.919067    1.339891   0.628233    2.210416  0.001000
63   63.0   0.930542    1.305449   0.623203    2.232687  0.001000
64   64.0   0.935913    1.297780   0.627777    2.231342  0.001000
65   65.0   0.939209    1.278542   0.629613    2.241319  0.001000
66   66.0   0.975830    1.156592   0.718333    1.917915  0.000100
67   67.0   0.988647    1.104193   0.724231    1.900665  0.000100
68   68.0   0.990845    1.086700   0.724835    1.904226  0.000100
69   69.0   0.994141    1.074483   0.729369    1.890623  0.000100
70   70.0   0.993164    1.070506   0.732485    1.890416  0.000100
71   71.0   0.992432    1.065241   0.727524    1.895636  0.000100
72   72.0   0.993774    1.059980   0.731989    1.884138  0.000100
73   73.0   0.994507    1.051168   0.736577    1.884287  0.000100
74   74.0   0.994263    1.048343   0.737391    1.884830  0.000100
75   75.0   0.994629    1.044715   0.735158    1.887460  0.000100
76   76.0   0.994507    1.042447   0.737817    1.876856  0.000100
77   77.0   0.995605    1.033404   0.738933    1.879618  0.000100
78   78.0   0.996460    1.032358   0.738616    1.874291  0.000100
79   79.0   0.995728    1.030604   0.741413    1.883294  0.000100
80   80.0   0.994995    1.029702   0.741592    1.877389  0.000100
81   81.0   0.995728    1.025505   0.738507    1.887335  0.000100
82   82.0   0.995483    1.023133   0.740724    1.876612  0.000100
83   83.0   0.995850    1.022503   0.739484    1.890154  0.000100
84   84.0   0.996216    1.018929   0.743134    1.884333  0.000100
85   85.0   0.996704    1.011860   0.745366    1.878685  0.000010
86   86.0   0.996948    1.010115   0.748219    1.879139  0.000010
87   87.0   0.997070    1.008718   0.748219    1.883407  0.000010
88   88.0   0.997070    1.008438   0.746482    1.883235  0.000010
89   89.0   0.997314    1.008732   0.745118    1.884506  0.000010
90   90.0   0.997803    1.005984   0.745118    1.882877  0.000010
91   91.0   0.997437    1.007375   0.743382    1.883417  0.000001
92   92.0   0.997681    1.005636   0.744816    1.883930  0.000001
93   93.0   0.997070    1.006704   0.743010    1.891797  0.000001

EXPERIMENT: C-30
================

NAME: ghost_20200920160024

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [28, 48, 68, 88]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0     0.0   0.007080    5.343027   0.011354    5.600918  1.000000e-03
1     1.0   0.008789    5.257970   0.012029    5.220878  1.000000e-03
2     2.0   0.010986    5.206611   0.013021    5.383962  1.000000e-03
3     3.0   0.014648    5.152643   0.019608    5.133911  1.000000e-03
4     4.0   0.021973    5.086344   0.021151    5.095118  1.000000e-03
5     5.0   0.026367    5.014835   0.029335    4.984531  1.000000e-03
6     6.0   0.032837    4.919905   0.029390    4.966087  1.000000e-03
7     7.0   0.039429    4.828239   0.045720    4.801709  1.000000e-03
8     8.0   0.051880    4.717458   0.045828    4.823032  1.000000e-03
9     9.0   0.056641    4.604928   0.062282    4.658780  1.000000e-03
10   10.0   0.066406    4.493235   0.076915    4.487425  1.000000e-03
11   11.0   0.079956    4.397038   0.064832    4.617523  1.000000e-03
12   12.0   0.093872    4.280466   0.086781    4.398355  1.000000e-03
13   13.0   0.110107    4.197718   0.101414    4.260506  1.000000e-03
14   14.0   0.130371    4.075089   0.098686    4.282431  1.000000e-03
15   15.0   0.149658    3.972074   0.143592    4.027220  1.000000e-03
16   16.0   0.167847    3.873179   0.143166    4.014775  1.000000e-03
17   17.0   0.190186    3.777830   0.168742    3.917309  1.000000e-03
18   18.0   0.217896    3.641355   0.190165    3.743823  1.000000e-03
19   19.0   0.243286    3.519549   0.225785    3.599291  1.000000e-03
20   20.0   0.269897    3.408693   0.233583    3.540596  1.000000e-03
21   21.0   0.289795    3.309726   0.273444    3.441639  1.000000e-03
22   22.0   0.327148    3.209343   0.266638    3.445956  1.000000e-03
23   23.0   0.352417    3.104506   0.319000    3.219457  1.000000e-03
24   24.0   0.385132    2.995519   0.344049    3.155237  1.000000e-03
25   25.0   0.416626    2.870639   0.370687    3.043049  1.000000e-03
26   26.0   0.445190    2.770234   0.378593    3.019750  1.000000e-03
27   27.0   0.470947    2.684720   0.404124    2.928314  1.000000e-03
28   28.0   0.574951    2.397277   0.502631    2.606195  1.000000e-04
29   29.0   0.623291    2.271559   0.509080    2.581290  1.000000e-04
30   30.0   0.642334    2.218798   0.520752    2.548880  1.000000e-04
31   31.0   0.646118    2.194046   0.527270    2.532094  1.000000e-04
32   32.0   0.660522    2.156970   0.531501    2.515184  1.000000e-04
33   33.0   0.674438    2.121501   0.533966    2.519013  1.000000e-04
34   34.0   0.676880    2.111787   0.537042    2.492614  1.000000e-04
35   35.0   0.688110    2.083685   0.539259    2.491062  1.000000e-04
36   36.0   0.685547    2.069919   0.537895    2.482885  1.000000e-04
37   37.0   0.709961    2.036567   0.543475    2.461291  1.000000e-04
38   38.0   0.712646    2.011744   0.544289    2.450624  1.000000e-04
39   39.0   0.708252    1.996488   0.547707    2.449868  1.000000e-04
40   40.0   0.719849    1.983067   0.552900    2.444579  1.000000e-04
41   41.0   0.731445    1.961238   0.552861    2.437209  1.000000e-04
42   42.0   0.736938    1.942205   0.555822    2.422051  1.000000e-04
43   43.0   0.738770    1.924206   0.561705    2.405048  1.000000e-04
44   44.0   0.752930    1.895043   0.562270    2.403058  1.000000e-04
45   45.0   0.747559    1.892074   0.565618    2.405796  1.000000e-04
46   46.0   0.760132    1.863626   0.564696    2.399093  1.000000e-04
47   47.0   0.762939    1.845576   0.565425    2.398832  1.000000e-04
48   48.0   0.782349    1.809511   0.566998    2.395955  1.000000e-05
49   49.0   0.780762    1.816259   0.567851    2.391509  1.000000e-05
50   50.0   0.788940    1.799677   0.569160    2.394087  1.000000e-05
51   51.0   0.790405    1.792664   0.569284    2.384644  1.000000e-05
52   52.0   0.788452    1.793872   0.568897    2.382555  1.000000e-05
53   53.0   0.792603    1.791737   0.569850    2.383432  1.000000e-05
54   54.0   0.790161    1.794682   0.569215    2.384771  1.000000e-05
55   55.0   0.788940    1.791678   0.573376    2.379168  1.000000e-05
56   56.0   0.791504    1.792274   0.570524    2.380858  1.000000e-05
57   57.0   0.785522    1.789471   0.573748    2.371952  1.000000e-05
58   58.0   0.790161    1.784199   0.572012    2.380452  1.000000e-05
59   59.0   0.791016    1.785447   0.570896    2.373446  1.000000e-05
60   60.0   0.788452    1.789558   0.569726    2.385358  1.000000e-05
61   61.0   0.795410    1.779263   0.575291    2.369551  1.000000e-05
62   62.0   0.791626    1.777687   0.573996    2.374554  1.000000e-05
63   63.0   0.795166    1.772832   0.571943    2.372791  1.000000e-05
64   64.0   0.798462    1.770748   0.570966    2.379853  1.000000e-05
65   65.0   0.800049    1.766904   0.575112    2.373235  1.000000e-05
66   66.0   0.801636    1.757454   0.574244    2.366648  1.000000e-05
67   67.0   0.803467    1.754578   0.572136    2.371371  1.000000e-05
68   68.0   0.797607    1.760525   0.572384    2.375891  1.000000e-06
69   69.0   0.797485    1.765119   0.578213    2.366017  1.000000e-06
70   70.0   0.802734    1.764094   0.573500    2.373445  1.000000e-06
71   71.0   0.801147    1.765136   0.576973    2.364037  1.000000e-06
72   72.0   0.798584    1.766970   0.573376    2.371621  1.000000e-06
73   73.0   0.795532    1.757867   0.573074    2.371427  1.000000e-06
74   74.0   0.800171    1.760147   0.575608    2.363789  1.000000e-06
75   75.0   0.795654    1.767269   0.573128    2.369676  1.000000e-06
76   76.0   0.799561    1.758364   0.574740    2.372914  1.000000e-06
77   77.0   0.800171    1.764326   0.573872    2.370736  1.000000e-06
78   78.0   0.799561    1.764126   0.572632    2.377629  1.000000e-06
79   79.0   0.797241    1.769761   0.573872    2.374370  1.000000e-06
80   80.0   0.802246    1.762139   0.570896    2.386390  1.000000e-06
81   81.0   0.800293    1.760925   0.574244    2.369125  1.000000e-06
82   82.0   0.802734    1.751274   0.574244    2.371427  1.000000e-06
83   83.0   0.802979    1.757446   0.574616    2.367232  1.000000e-06
84   84.0   0.803101    1.754702   0.575485    2.366077  1.000000e-06
85   85.0   0.800049    1.760639   0.572632    2.367674  1.000000e-06
86   86.0   0.804077    1.751760   0.575733    2.369421  1.000000e-06
87   87.0   0.796631    1.756677   0.576725    2.366021  1.000000e-06
88   88.0   0.793945    1.768752   0.575043    2.371404  1.000000e-07
89   89.0   0.806641    1.749178   0.577469    2.367111  1.000000e-07

EXPERIMENT: C-31
================

NAME: ghost_20200920160208

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [36, 56, 76, 96]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0     0.0   0.006836    5.345545   0.011850    5.655421  1.000000e-03
1     1.0   0.012817    5.256295   0.013021    5.201596  1.000000e-03
2     2.0   0.016846    5.182981   0.015377    5.274029  1.000000e-03
3     3.0   0.020386    5.138399   0.020035    5.175746  1.000000e-03
4     4.0   0.021118    5.077941   0.022445    5.078528  1.000000e-03
5     5.0   0.027100    5.018517   0.031017    5.010129  1.000000e-03
6     6.0   0.037109    4.946774   0.036845    4.940938  1.000000e-03
7     7.0   0.038818    4.876253   0.031444    5.020919  1.000000e-03
8     8.0   0.045288    4.789040   0.048859    4.829408  1.000000e-03
9     9.0   0.055298    4.678034   0.052649    4.729710  1.000000e-03
10   10.0   0.066772    4.580436   0.058175    4.669270  1.000000e-03
11   11.0   0.078735    4.458946   0.073815    4.553939  1.000000e-03
12   12.0   0.090454    4.371274   0.075536    4.435821  1.000000e-03
13   13.0   0.098633    4.263683   0.096772    4.289349  1.000000e-03
14   14.0   0.118408    4.158924   0.088657    4.368711  1.000000e-03
15   15.0   0.129761    4.057566   0.131935    4.075836  1.000000e-03
16   16.0   0.154419    3.954169   0.137570    4.040305  1.000000e-03
17   17.0   0.171631    3.854686   0.150125    3.983558  1.000000e-03
18   18.0   0.191406    3.751256   0.168587    3.917433  1.000000e-03
19   19.0   0.216919    3.645137   0.162139    3.920675  1.000000e-03
20   20.0   0.238892    3.531797   0.190900    3.798210  1.000000e-03
21   21.0   0.275146    3.401693   0.233637    3.554624  1.000000e-03
22   22.0   0.308228    3.277953   0.252253    3.487499  1.000000e-03
23   23.0   0.342285    3.155824   0.320403    3.232419  1.000000e-03
24   24.0   0.363403    3.049939   0.289867    3.367041  1.000000e-03
25   25.0   0.405273    2.938315   0.341445    3.170188  1.000000e-03
26   26.0   0.430664    2.809170   0.374362    3.020013  1.000000e-03
27   27.0   0.467896    2.710345   0.418841    2.904530  1.000000e-03
28   28.0   0.494019    2.624846   0.400086    2.993165  1.000000e-03
29   29.0   0.511230    2.542482   0.439224    2.821477  1.000000e-03
30   30.0   0.550049    2.427037   0.445464    2.811673  1.000000e-03
31   31.0   0.575806    2.354685   0.490851    2.658228  1.000000e-03
32   32.0   0.610718    2.259074   0.456144    2.740311  1.000000e-03
33   33.0   0.631714    2.196503   0.472334    2.685752  1.000000e-03
34   34.0   0.643188    2.143543   0.511366    2.551354  1.000000e-03
35   35.0   0.677368    2.054830   0.525255    2.543353  1.000000e-03
36   36.0   0.781738    1.781778   0.611347    2.254486  1.000000e-04
37   37.0   0.819214    1.693940   0.612602    2.233696  1.000000e-04
38   38.0   0.832886    1.646799   0.617384    2.229215  1.000000e-04
39   39.0   0.848511    1.612760   0.620926    2.216306  1.000000e-04
40   40.0   0.857300    1.582542   0.627746    2.199628  1.000000e-04
41   41.0   0.868164    1.562742   0.628614    2.184414  1.000000e-04
42   42.0   0.866699    1.563076   0.633947    2.184310  1.000000e-04
43   43.0   0.869019    1.543522   0.637240    2.177135  1.000000e-04
44   44.0   0.878906    1.521371   0.630738    2.184829  1.000000e-04
45   45.0   0.886719    1.513441   0.634140    2.168879  1.000000e-04
46   46.0   0.883789    1.501507   0.639349    2.168923  1.000000e-04
47   47.0   0.890381    1.484321   0.640891    2.160479  1.000000e-04
48   48.0   0.902588    1.462920   0.636620    2.167798  1.000000e-04
49   49.0   0.896973    1.459835   0.632474    2.173999  1.000000e-04
50   50.0   0.901123    1.448579   0.639651    2.163502  1.000000e-04
51   51.0   0.913818    1.431039   0.638922    2.161230  1.000000e-04
52   52.0   0.913330    1.427394   0.641705    2.154099  1.000000e-04
53   53.0   0.920166    1.413757   0.645867    2.150279  1.000000e-04
54   54.0   0.918823    1.401757   0.642131    2.154886  1.000000e-04
55   55.0   0.922119    1.392701   0.640976    2.155018  1.000000e-04
56   56.0   0.933350    1.364422   0.645619    2.143514  1.000000e-05
57   57.0   0.935059    1.362199   0.645301    2.150436  1.000000e-05
58   58.0   0.934937    1.363016   0.643069    2.145060  1.000000e-05
59   59.0   0.932129    1.366350   0.647161    2.145349  1.000000e-05
60   60.0   0.938721    1.353995   0.644185    2.145794  1.000000e-05
61   61.0   0.936768    1.352366   0.644061    2.140490  1.000000e-05
62   62.0   0.938965    1.347119   0.647727    2.135862  1.000000e-05
63   63.0   0.937622    1.347581   0.645797    2.140170  1.000000e-05
64   64.0   0.940796    1.345158   0.645549    2.138842  1.000000e-05
65   65.0   0.939453    1.348269   0.646293    2.135975  1.000000e-05
66   66.0   0.938599    1.340863   0.646789    2.137686  1.000000e-05
67   67.0   0.936768    1.352140   0.645743    2.134099  1.000000e-05
68   68.0   0.938965    1.344015   0.646913    2.134912  1.000000e-05
69   69.0   0.941528    1.346475   0.644433    2.139842  1.000000e-05
70   70.0   0.938721    1.342580   0.644433    2.143711  1.000000e-05
71   71.0   0.942261    1.334764   0.647905    2.133311  1.000000e-05
72   72.0   0.937988    1.345460   0.645921    2.137290  1.000000e-05
73   73.0   0.942139    1.335462   0.647533    2.139394  1.000000e-05
74   74.0   0.941528    1.345783   0.646541    2.134143  1.000000e-05
75   75.0   0.944458    1.335788   0.645301    2.144371  1.000000e-05
76   76.0   0.949341    1.328629   0.646293    2.137564  1.000000e-06
77   77.0   0.941406    1.332976   0.643813    2.140366  1.000000e-06
78   78.0   0.941040    1.338357   0.647409    2.138723  1.000000e-06
79   79.0   0.941284    1.335530   0.644061    2.142008  1.000000e-06
80   80.0   0.942139    1.342491   0.647161    2.137478  1.000000e-06
81   81.0   0.943115    1.329446   0.645177    2.140154  1.000000e-06
82   82.0   0.940186    1.326495   0.647161    2.134475  1.000000e-06
83   83.0   0.943970    1.337347   0.646417    2.130344  1.000000e-06
84   84.0   0.942261    1.334182   0.646417    2.134632  1.000000e-06
85   85.0   0.944214    1.329469   0.644309    2.146562  1.000000e-06
86   86.0   0.942993    1.335269   0.647781    2.135747  1.000000e-06
87   87.0   0.944824    1.335461   0.647285    2.136797  1.000000e-06
88   88.0   0.944458    1.328338   0.646541    2.132544  1.000000e-06
89   89.0   0.947021    1.327902   0.649269    2.136319  1.000000e-06
90   90.0   0.943359    1.335245   0.642325    2.141288  1.000000e-06
91   91.0   0.945190    1.330912   0.647285    2.141528  1.000000e-06
92   92.0   0.949219    1.329258   0.647161    2.135787  1.000000e-06
93   93.0   0.943115    1.336642   0.648401    2.137828  1.000000e-06
94   94.0   0.944092    1.331602   0.646665    2.135921  1.000000e-06
95   95.0   0.950317    1.321336   0.643689    2.140815  1.000000e-06
96   96.0   0.946777    1.325571   0.646045    2.136774  1.000000e-07
97   97.0   0.945557    1.327426   0.644061    2.145484  1.000000e-07
98   98.0   0.942383    1.334700   0.644433    2.146689  1.000000e-07

EXPERIMENT: C-32
================

NAME: ghost_20200920180652

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [44, 64, 84, 104]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0     0.0   0.008789    5.325912   0.009301    5.337111  0.001000
1     1.0   0.012085    5.234790   0.014509    5.275358  0.001000
2     2.0   0.014771    5.183204   0.017113    5.192231  0.001000
3     3.0   0.017334    5.127605   0.017361    5.136129  0.001000
4     4.0   0.021973    5.067053   0.026910    5.078277  0.001000
5     5.0   0.027100    5.006644   0.026910    5.037300  0.001000
6     6.0   0.030273    4.931996   0.034598    4.909678  0.001000
7     7.0   0.040405    4.838410   0.042287    4.868711  0.001000
8     8.0   0.045166    4.748507   0.047371    4.770670  0.001000
9     9.0   0.056519    4.642295   0.056176    4.700608  0.001000
10   10.0   0.067749    4.529470   0.074613    4.493778  0.001000
11   11.0   0.081421    4.426087   0.063988    4.581454  0.001000
12   12.0   0.095581    4.300415   0.093750    4.367352  0.001000
13   13.0   0.112671    4.199784   0.108380    4.218938  0.001000
14   14.0   0.132690    4.081760   0.111643    4.231105  0.001000
15   15.0   0.151733    3.963675   0.114961    4.197556  0.001000
16   16.0   0.175415    3.854653   0.154937    3.978525  0.001000
17   17.0   0.198120    3.737299   0.173732    3.861294  0.001000
18   18.0   0.224121    3.621006   0.191680    3.789250  0.001000
19   19.0   0.254028    3.503380   0.197874    3.761919  0.001000
20   20.0   0.284180    3.367406   0.244719    3.574593  0.001000
21   21.0   0.307251    3.275002   0.233643    3.595588  0.001000
22   22.0   0.345947    3.147022   0.306212    3.317570  0.001000
23   23.0   0.361084    3.065641   0.288185    3.381884  0.001000
24   24.0   0.396484    2.948295   0.351060    3.121006  0.001000
25   25.0   0.426392    2.855528   0.325315    3.224319  0.001000
26   26.0   0.455811    2.768604   0.372151    3.012204  0.001000
27   27.0   0.473999    2.668201   0.376080    3.027519  0.001000
28   28.0   0.510864    2.568753   0.404214    2.947911  0.001000
29   29.0   0.522705    2.509586   0.415708    2.911348  0.001000
30   30.0   0.556763    2.408773   0.431372    2.880193  0.001000
31   31.0   0.574951    2.346712   0.449789    2.797319  0.001000
32   32.0   0.607056    2.246641   0.480558    2.663958  0.001000
33   33.0   0.626831    2.201367   0.470217    2.712984  0.001000
34   34.0   0.647339    2.125461   0.514566    2.592849  0.001000
35   35.0   0.671021    2.052317   0.507682    2.617601  0.001000
36   36.0   0.701294    1.985650   0.524230    2.545921  0.001000
37   37.0   0.701050    1.969834   0.514766    2.577442  0.001000
38   38.0   0.734863    1.893859   0.508396    2.612167  0.001000
39   39.0   0.746948    1.853238   0.537553    2.533015  0.001000
40   40.0   0.764893    1.784647   0.566695    2.410600  0.001000
41   41.0   0.780518    1.745545   0.566006    2.419400  0.001000
42   42.0   0.792480    1.707010   0.592558    2.350379  0.001000
43   43.0   0.801270    1.673226   0.562092    2.411987  0.001000
44   44.0   0.899902    1.445053   0.656810    2.099673  0.000100
45   45.0   0.923828    1.364745   0.661824    2.078809  0.000100
46   46.0   0.937744    1.331821   0.668645    2.066985  0.000100
47   47.0   0.943359    1.315466   0.672117    2.063663  0.000100
48   48.0   0.946533    1.299569   0.678511    2.049787  0.000100
49   49.0   0.952515    1.279059   0.680619    2.052140  0.000100
50   50.0   0.954712    1.265487   0.675054    2.048985  0.000100
51   51.0   0.964478    1.253639   0.673814    2.048937  0.000100
52   52.0   0.965454    1.248721   0.681076    2.043287  0.000100
53   53.0   0.964966    1.243158   0.680937    2.041530  0.000100
54   54.0   0.968018    1.228359   0.675396    2.048662  0.000100
55   55.0   0.965088    1.230102   0.678992    2.040950  0.000100
56   56.0   0.968384    1.225628   0.679542    2.042129  0.000100
57   57.0   0.967896    1.221969   0.676457    2.045601  0.000100
58   58.0   0.970337    1.214413   0.678798    2.043740  0.000100
59   59.0   0.974976    1.205876   0.680425    2.037778  0.000100
60   60.0   0.972656    1.198527   0.681929    2.037566  0.000100
61   61.0   0.974731    1.189084   0.679061    2.042761  0.000100
62   62.0   0.978394    1.184067   0.684200    2.037688  0.000100
63   63.0   0.979004    1.181872   0.681046    2.052152  0.000100
64   64.0   0.982422    1.164696   0.686006    2.040501  0.000010
65   65.0   0.979736    1.169708   0.686943    2.031267  0.000010
66   66.0   0.979126    1.164471   0.682960    2.037207  0.000010
67   67.0   0.984009    1.164608   0.686378    2.032037  0.000010
68   68.0   0.982666    1.159046   0.687246    2.027514  0.000010
69   69.0   0.984009    1.156188   0.684270    2.029768  0.000010
70   70.0   0.986206    1.154550   0.685386    2.032420  0.000010
71   71.0   0.983521    1.159028   0.686571    2.027527  0.000010
72   72.0   0.984253    1.151378   0.683402    2.035068  0.000010
73   73.0   0.984253    1.157354   0.686556    2.025221  0.000010
74   74.0   0.983154    1.152721   0.687122    2.027918  0.000010
75   75.0   0.982666    1.154564   0.686323    2.022110  0.000010
76   76.0   0.982178    1.156140   0.683774    2.030217  0.000010
77   77.0   0.985718    1.149156   0.685331    2.034290  0.000010
78   78.0   0.984253    1.152912   0.685262    2.029111  0.000010
79   79.0   0.985596    1.148250   0.684959    2.029708  0.000010
80   80.0   0.982544    1.153493   0.687618    2.031573  0.000010
81   81.0   0.982300    1.149858   0.685138    2.032717  0.000010
82   82.0   0.983398    1.150651   0.686184    2.032303  0.000010
83   83.0   0.984619    1.151867   0.685634    2.029229  0.000010
84   84.0   0.985596    1.147318   0.686626    2.035021  0.000001
85   85.0   0.986084    1.142474   0.684518    2.036158  0.000001
86   86.0   0.985229    1.146217   0.683650    2.037717  0.000001
87   87.0   0.986816    1.145830   0.683526    2.031063  0.000001
88   88.0   0.985352    1.149507   0.687936    2.030154  0.000001
89   89.0   0.985352    1.143232   0.685440    2.029616  0.000001
90   90.0   0.984497    1.150958   0.684572    2.035008  0.000001

EXPERIMENT: C-33
================

NAME: ghost_20200920223823

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.2}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [52, 72, 92, 112]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.2}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss       lr
0     0.0   0.006348    5.359603   0.009053    5.271472  0.00100
1     1.0   0.010498    5.280432   0.012773    5.182036  0.00100
2     2.0   0.014160    5.189049   0.015005    5.258764  0.00100
3     3.0   0.019165    5.121609   0.022321    5.090848  0.00100
4     4.0   0.021973    5.064173   0.023313    5.101925  0.00100
5     5.0   0.025757    5.008610   0.027902    4.977236  0.00100
6     6.0   0.030151    4.955729   0.030630    4.950215  0.00100
7     7.0   0.038086    4.860544   0.040318    4.859853  0.00100
8     8.0   0.047974    4.731487   0.044271    4.797227  0.00100
9     9.0   0.059570    4.601146   0.054067    4.686525  0.00100
10   10.0   0.070312    4.494932   0.068343    4.568933  0.00100
11   11.0   0.083130    4.380674   0.085387    4.381491  0.00100
12   12.0   0.097412    4.268679   0.091587    4.330858  0.00100
13   13.0   0.112549    4.168351   0.098042    4.312632  0.00100
14   14.0   0.137939    4.051977   0.127084    4.115511  0.00100
15   15.0   0.158203    3.928128   0.148211    3.979090  0.00100
16   16.0   0.175293    3.825405   0.147282    4.029858  0.00100
17   17.0   0.209595    3.699606   0.164416    3.937864  0.00100
18   18.0   0.233887    3.593050   0.203806    3.711381  0.00100
19   19.0   0.259644    3.464642   0.225670    3.636988  0.00100
20   20.0   0.286621    3.341489   0.292507    3.356596  0.00100
21   21.0   0.329468    3.218578   0.288960    3.367038  0.00100
22   22.0   0.364746    3.090398   0.325890    3.249242  0.00100
23   23.0   0.379517    2.987887   0.348232    3.139606  0.00100
24   24.0   0.426147    2.862207   0.351187    3.111325  0.00100
25   25.0   0.449707    2.767635   0.399124    2.971002  0.00100
26   26.0   0.480835    2.671421   0.369913    3.077652  0.00100
27   27.0   0.509888    2.575056   0.381346    3.007325  0.00100
28   28.0   0.538940    2.481156   0.443525    2.825100  0.00100
29   29.0   0.566528    2.410701   0.479363    2.694100  0.00100
30   30.0   0.582642    2.329563   0.485524    2.696894  0.00100
31   31.0   0.616577    2.246631   0.469715    2.700555  0.00100
32   32.0   0.632812    2.178958   0.499894    2.641555  0.00100
33   33.0   0.664917    2.095794   0.517682    2.577863  0.00100
34   34.0   0.680420    2.040333   0.503560    2.643092  0.00100
35   35.0   0.697144    1.988628   0.539840    2.516460  0.00100
36   36.0   0.718018    1.921638   0.549280    2.460897  0.00100
37   37.0   0.735962    1.870102   0.542011    2.504802  0.00100
38   38.0   0.755127    1.823354   0.549721    2.477096  0.00100
39   39.0   0.778564    1.758127   0.545444    2.480709  0.00100
40   40.0   0.798096    1.718011   0.556000    2.437089  0.00100
41   41.0   0.803467    1.694432   0.561224    2.458222  0.00100
42   42.0   0.809937    1.667114   0.579522    2.388057  0.00100
43   43.0   0.841064    1.604306   0.601556    2.310952  0.00100
44   44.0   0.838501    1.590128   0.591203    2.367099  0.00100
45   45.0   0.849731    1.562203   0.598511    2.329327  0.00100
46   46.0   0.855591    1.541015   0.603913    2.334836  0.00100
47   47.0   0.858276    1.521933   0.609547    2.273159  0.00100
48   48.0   0.877930    1.478721   0.583491    2.390285  0.00100
49   49.0   0.876953    1.473968   0.600123    2.313788  0.00100
50   50.0   0.887207    1.442211   0.601076    2.318244  0.00100
51   51.0   0.909058    1.395641   0.613724    2.277777  0.00100
52   52.0   0.958740    1.249773   0.688005    2.026266  0.00010
53   53.0   0.975098    1.193291   0.696740    2.017842  0.00010
54   54.0   0.980225    1.167285   0.696066    2.004158  0.00010
55   55.0   0.981567    1.157096   0.702088    1.989499  0.00010
56   56.0   0.981689    1.147831   0.702196    2.002435  0.00010
57   57.0   0.987061    1.137510   0.699964    1.998018  0.00010
58   58.0   0.984741    1.134660   0.705366    1.990316  0.00010
59   59.0   0.987427    1.126921   0.705297    1.985142  0.00010
60   60.0   0.987671    1.123353   0.705297    1.991027  0.00010
61   61.0   0.988770    1.114452   0.703685    1.995235  0.00010
62   62.0   0.989502    1.112620   0.709141    1.982650  0.00010
63   63.0   0.989258    1.105785   0.709513    1.981214  0.00010
64   64.0   0.990845    1.100410   0.707281    1.986138  0.00010
65   65.0   0.990479    1.102065   0.708521    1.987179  0.00010
66   66.0   0.990845    1.097331   0.708521    1.986919  0.00010
67   67.0   0.991211    1.093280   0.705049    1.986858  0.00010
68   68.0   0.993164    1.088082   0.707846    1.987803  0.00010
69   69.0   0.991943    1.087788   0.709458    1.981537  0.00010
70   70.0   0.992310    1.082034   0.708412    1.986865  0.00010
71   71.0   0.991577    1.080750   0.708838    1.982777  0.00010
72   72.0   0.992920    1.069921   0.709032    1.982828  0.00001
73   73.0   0.993286    1.069378   0.712063    1.980904  0.00001
74   74.0   0.992065    1.072228   0.711319    1.979485  0.00001
75   75.0   0.994995    1.065551   0.712008    1.981471  0.00001
76   76.0   0.995361    1.063496   0.713744    1.970543  0.00001
77   77.0   0.994873    1.066339   0.712187    1.977169  0.00001
78   78.0   0.993164    1.068458   0.711884    1.986929  0.00001
79   79.0   0.994873    1.066409   0.712504    1.975196  0.00001
80   80.0   0.993896    1.064473   0.712628    1.975493  0.00001
81   81.0   0.993774    1.060065   0.715852    1.976678  0.00001
82   82.0   0.994263    1.061834   0.715039    1.981057  0.00001
83   83.0   0.993774    1.062425   0.713248    1.980385  0.00001
84   84.0   0.994385    1.060209   0.714349    1.983371  0.00001
85   85.0   0.995239    1.058522   0.715163    1.977697  0.00001
86   86.0   0.994385    1.060329   0.712435    1.977524  0.00001
87   87.0   0.994873    1.060514   0.713620    1.982512  0.00001
88   88.0   0.995972    1.059480   0.715232    1.981093  0.00001
89   89.0   0.995361    1.060921   0.714845    1.983586  0.00001
90   90.0   0.994873    1.061485   0.711125    1.987021  0.00001
91   91.0   0.994629    1.060181   0.714473    1.975766  0.00001

EXPERIMENT: C-36
================

NAME: ghost_20200921013438

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.5}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.5}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0     0.0   0.007324    5.335375   0.011161    5.231695  1.000000e-03
1     1.0   0.008179    5.250867   0.012525    5.192266  1.000000e-03
2     2.0   0.010132    5.200721   0.016617    5.170504  1.000000e-03
3     3.0   0.014771    5.150893   0.017609    5.187801  1.000000e-03
4     4.0   0.017822    5.111002   0.019221    5.118995  1.000000e-03
5     5.0   0.020020    5.049959   0.022073    5.059541  1.000000e-03
6     6.0   0.026489    4.977922   0.026786    4.961382  1.000000e-03
7     7.0   0.029541    4.897172   0.027971    4.880714  1.000000e-03
8     8.0   0.039307    4.795714   0.050021    4.723148  1.000000e-03
9     9.0   0.047974    4.682780   0.036915    4.845803  1.000000e-03
10   10.0   0.051880    4.592228   0.057803    4.555093  1.000000e-03
11   11.0   0.068237    4.474842   0.054067    4.672733  1.000000e-03
12   12.0   0.080933    4.380875   0.092005    4.325099  1.000000e-03
13   13.0   0.091919    4.265862   0.090223    4.305985  1.000000e-03
14   14.0   0.106934    4.163864   0.114901    4.172388  1.000000e-03
15   15.0   0.124878    4.063474   0.112403    4.135063  1.000000e-03
16   16.0   0.141479    3.954024   0.129014    4.032465  1.000000e-03
17   17.0   0.166260    3.846002   0.158195    3.882727  1.000000e-03
18   18.0   0.186890    3.764988   0.186097    3.809232  1.000000e-03
19   19.0   0.209351    3.647224   0.186024    3.795965  1.000000e-03
20   20.0   0.237061    3.536700   0.211258    3.674955  1.000000e-03
21   21.0   0.260498    3.441182   0.254171    3.482856  1.000000e-03
22   22.0   0.290161    3.331085   0.268746    3.393986  1.000000e-03
23   23.0   0.310669    3.235624   0.261617    3.468088  1.000000e-03
24   24.0   0.347778    3.123356   0.322372    3.215772  1.000000e-03
25   25.0   0.366577    3.056503   0.351907    3.122252  1.000000e-03
26   26.0   0.390747    2.963325   0.301491    3.312049  1.000000e-03
27   27.0   0.418457    2.856510   0.370170    3.021633  1.000000e-03
28   28.0   0.447754    2.772072   0.382825    2.981840  1.000000e-03
29   29.0   0.467773    2.707017   0.379368    3.014827  1.000000e-03
30   30.0   0.480713    2.646414   0.366313    3.055430  1.000000e-03
31   31.0   0.508057    2.571580   0.395241    2.939501  1.000000e-03
32   32.0   0.532471    2.492164   0.420118    2.877701  1.000000e-03
33   33.0   0.562500    2.410368   0.439644    2.811071  1.000000e-03
34   34.0   0.565552    2.398229   0.400334    2.916492  1.000000e-03
35   35.0   0.588379    2.324960   0.422746    2.886607  1.000000e-03
36   36.0   0.597290    2.275082   0.401284    2.994312  1.000000e-03
37   37.0   0.621216    2.211091   0.516932    2.558717  1.000000e-03
38   38.0   0.636108    2.166254   0.420121    2.894319  1.000000e-03
39   39.0   0.650635    2.125958   0.488071    2.674815  1.000000e-03
40   40.0   0.665161    2.079342   0.492735    2.636624  1.000000e-03
41   41.0   0.670532    2.072379   0.486991    2.622267  1.000000e-03
42   42.0   0.690430    2.005727   0.531190    2.514150  1.000000e-03
43   43.0   0.701172    1.971463   0.576576    2.349850  1.000000e-03
44   44.0   0.708984    1.937813   0.459679    2.770718  1.000000e-03
45   45.0   0.721191    1.915854   0.442243    2.825372  1.000000e-03
46   46.0   0.723633    1.897167   0.501754    2.627901  1.000000e-03
47   47.0   0.738525    1.841430   0.420097    2.907909  1.000000e-03
48   48.0   0.750122    1.833439   0.499716    2.589643  1.000000e-03
49   49.0   0.755981    1.821874   0.556535    2.454459  1.000000e-03
50   50.0   0.876953    1.492388   0.749552    1.800325  1.000000e-04
51   51.0   0.920044    1.370352   0.759310    1.768622  1.000000e-04
52   52.0   0.924561    1.339519   0.766735    1.758027  1.000000e-04
53   53.0   0.931396    1.306453   0.769339    1.739454  1.000000e-04
54   54.0   0.943604    1.284172   0.773555    1.728050  1.000000e-04
55   55.0   0.947632    1.269283   0.772703    1.727794  1.000000e-04
56   56.0   0.954834    1.249671   0.774354    1.726518  1.000000e-04
57   57.0   0.955566    1.241470   0.778035    1.711279  1.000000e-04
58   58.0   0.960815    1.228455   0.774920    1.720286  1.000000e-04
59   59.0   0.964600    1.215021   0.779438    1.714840  1.000000e-04
60   60.0   0.961792    1.209033   0.782360    1.698026  1.000000e-04
61   61.0   0.968628    1.193075   0.785406    1.696247  1.000000e-04
62   62.0   0.968140    1.188567   0.781810    1.707486  1.000000e-04
63   63.0   0.968262    1.184803   0.781174    1.706075  1.000000e-04
64   64.0   0.973022    1.173678   0.782662    1.711574  1.000000e-04
65   65.0   0.975220    1.166713   0.783283    1.700801  1.000000e-04
66   66.0   0.974976    1.161916   0.785460    1.701476  1.000000e-04
67   67.0   0.973999    1.152415   0.782662    1.695478  1.000000e-04
68   68.0   0.981445    1.130137   0.786328    1.685220  1.000000e-05
69   69.0   0.982178    1.123370   0.789925    1.678256  1.000000e-05
70   70.0   0.983032    1.123911   0.790545    1.672495  1.000000e-05
71   71.0   0.982178    1.121092   0.786948    1.685700  1.000000e-05
72   72.0   0.983154    1.116573   0.788684    1.677008  1.000000e-05
73   73.0   0.983765    1.116271   0.788436    1.676150  1.000000e-05
74   74.0   0.984619    1.111943   0.791978    1.670005  1.000000e-05
75   75.0   0.984985    1.111913   0.789428    1.677671  1.000000e-05
76   76.0   0.984375    1.111599   0.790227    1.673683  1.000000e-05
77   77.0   0.985596    1.110073   0.788064    1.680358  1.000000e-05
78   78.0   0.985352    1.108543   0.792529    1.673930  1.000000e-05
79   79.0   0.987305    1.105642   0.793714    1.674565  1.000000e-05
80   80.0   0.985352    1.111575   0.790862    1.668515  1.000000e-05
81   81.0   0.985840    1.102574   0.794017    1.673413  1.000000e-06
82   82.0   0.986572    1.104806   0.792970    1.674919  1.000000e-06
83   83.0   0.987061    1.104334   0.793025    1.677008  1.000000e-06
84   84.0   0.984863    1.108415   0.792529    1.672134  1.000000e-06
85   85.0   0.985962    1.108879   0.791289    1.678708  1.000000e-06
86   86.0   0.988403    1.106623   0.790173    1.675088  1.000000e-06
87   87.0   0.984375    1.107521   0.792901    1.674690  1.000000e-07
88   88.0   0.985962    1.105491   0.790421    1.679137  1.000000e-07
89   89.0   0.984131    1.108160   0.792777    1.674598  1.000000e-07
90   90.0   0.985962    1.104767   0.793273    1.671012  1.000000e-07
91   91.0   0.987549    1.102987   0.792405    1.672565  1.000000e-07
92   92.0   0.986450    1.104527   0.792281    1.671600  1.000000e-07
93   93.0   0.985718    1.107199   0.792529    1.676743  1.000000e-07
94   94.0   0.985474    1.103670   0.793397    1.670203  1.000000e-07
95   95.0   0.986816    1.103693   0.793397    1.673600  1.000000e-07

EXPERIMENT: C-37
================

NAME: ghost_20200921033417

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.3}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.3}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.006104    5.360750   0.010293    5.260613  1.000000e-03
1      1.0   0.010620    5.269605   0.010541    5.222180  1.000000e-03
2      2.0   0.012817    5.199790   0.016315    5.247507  1.000000e-03
3      3.0   0.018066    5.139043   0.013517    5.148623  1.000000e-03
4      4.0   0.022339    5.082321   0.025382    5.261509  1.000000e-03
5      5.0   0.023315    5.020099   0.024995    5.042516  1.000000e-03
6      6.0   0.032227    4.956017   0.035055    4.946160  1.000000e-03
7      7.0   0.039307    4.843974   0.033195    5.005202  1.000000e-03
8      8.0   0.047974    4.733355   0.045596    4.817818  1.000000e-03
9      9.0   0.057617    4.614250   0.057128    4.707985  1.000000e-03
10    10.0   0.071167    4.494869   0.060655    4.590070  1.000000e-03
11    11.0   0.076782    4.390672   0.070784    4.514389  1.000000e-03
12    12.0   0.094604    4.276322   0.091702    4.301086  1.000000e-03
13    13.0   0.110962    4.178871   0.111017    4.179777  1.000000e-03
14    14.0   0.133301    4.059474   0.130393    4.111006  1.000000e-03
15    15.0   0.154663    3.933490   0.148235    4.008316  1.000000e-03
16    16.0   0.173462    3.822899   0.129897    4.077482  1.000000e-03
17    17.0   0.201782    3.703961   0.197179    3.737137  1.000000e-03
18    18.0   0.226562    3.587174   0.180755    3.810317  1.000000e-03
19    19.0   0.248047    3.481308   0.250929    3.521540  1.000000e-03
20    20.0   0.293335    3.349881   0.259059    3.463350  1.000000e-03
21    21.0   0.313599    3.252641   0.286573    3.371188  1.000000e-03
22    22.0   0.337280    3.155121   0.311629    3.284885  1.000000e-03
23    23.0   0.368774    3.037862   0.319124    3.257606  1.000000e-03
24    24.0   0.402832    2.920855   0.339174    3.146428  1.000000e-03
25    25.0   0.423706    2.852850   0.361232    3.072599  1.000000e-03
26    26.0   0.448608    2.756610   0.373594    3.023142  1.000000e-03
27    27.0   0.481323    2.658422   0.399169    2.960371  1.000000e-03
28    28.0   0.510620    2.565629   0.429234    2.865461  1.000000e-03
29    29.0   0.535034    2.492509   0.424948    2.841756  1.000000e-03
30    30.0   0.556763    2.405921   0.475132    2.694671  1.000000e-03
31    31.0   0.578369    2.332792   0.464824    2.705505  1.000000e-03
32    32.0   0.600342    2.276004   0.482711    2.657496  1.000000e-03
33    33.0   0.628540    2.185144   0.494299    2.661631  1.000000e-03
34    34.0   0.634644    2.159898   0.475295    2.694178  1.000000e-03
35    35.0   0.671875    2.070514   0.515271    2.571329  1.000000e-03
36    36.0   0.679199    2.021766   0.512041    2.552067  1.000000e-03
37    37.0   0.695557    1.987313   0.493679    2.655764  1.000000e-03
38    38.0   0.716797    1.925304   0.549056    2.452838  1.000000e-03
39    39.0   0.739014    1.872650   0.571943    2.394718  1.000000e-03
40    40.0   0.742676    1.847565   0.523758    2.565818  1.000000e-03
41    41.0   0.757935    1.801445   0.534036    2.526049  1.000000e-03
42    42.0   0.766602    1.761179   0.509328    2.580734  1.000000e-03
43    43.0   0.782227    1.739179   0.557310    2.457778  1.000000e-03
44    44.0   0.792969    1.703401   0.592861    2.297766  1.000000e-03
45    45.0   0.794189    1.694721   0.541739    2.490333  1.000000e-03
46    46.0   0.819336    1.621056   0.540817    2.494510  1.000000e-03
47    47.0   0.828857    1.598794   0.576159    2.392121  1.000000e-03
48    48.0   0.829956    1.598462   0.544065    2.508233  1.000000e-03
49    49.0   0.833496    1.581737   0.610370    2.276237  1.000000e-03
50    50.0   0.851929    1.536703   0.586243    2.341782  1.000000e-03
51    51.0   0.852661    1.525942   0.526680    2.581073  1.000000e-03
52    52.0   0.858276    1.501797   0.577538    2.418461  1.000000e-03
53    53.0   0.861084    1.508541   0.580802    2.380355  1.000000e-03
54    54.0   0.867065    1.491114   0.586437    2.350633  1.000000e-03
55    55.0   0.881226    1.448335   0.528773    2.549203  1.000000e-03
56    56.0   0.948853    1.252058   0.715505    1.921947  1.000000e-04
57    57.0   0.971924    1.170377   0.721085    1.902650  1.000000e-04
58    58.0   0.976440    1.155008   0.725688    1.884107  1.000000e-04
59    59.0   0.980957    1.135611   0.730416    1.886636  1.000000e-04
60    60.0   0.982056    1.125788   0.734563    1.879064  1.000000e-04
61    61.0   0.984375    1.112191   0.733005    1.878634  1.000000e-04
62    62.0   0.988770    1.101431   0.737236    1.878183  1.000000e-04
63    63.0   0.986572    1.099285   0.733818    1.873863  1.000000e-04
64    64.0   0.989380    1.091253   0.733888    1.866794  1.000000e-04
65    65.0   0.988525    1.087498   0.737167    1.872553  1.000000e-04
66    66.0   0.990601    1.083195   0.734880    1.870855  1.000000e-04
67    67.0   0.989502    1.081178   0.740778    1.871231  1.000000e-04
68    68.0   0.990112    1.073698   0.741368    1.866429  1.000000e-04
69    69.0   0.991577    1.070671   0.739096    1.870720  1.000000e-04
70    70.0   0.989868    1.069042   0.741135    1.867657  1.000000e-04
71    71.0   0.992920    1.055335   0.742623    1.868291  1.000000e-05
72    72.0   0.994751    1.052839   0.743119    1.863244  1.000000e-05
73    73.0   0.993530    1.053063   0.742127    1.870611  1.000000e-05
74    74.0   0.995728    1.051440   0.742375    1.867514  1.000000e-05
75    75.0   0.993408    1.051046   0.739027    1.878133  1.000000e-05
76    76.0   0.994019    1.050642   0.742499    1.867466  1.000000e-05
77    77.0   0.993652    1.049256   0.741755    1.870744  1.000000e-05
78    78.0   0.994263    1.050437   0.743243    1.862890  1.000000e-05
79    79.0   0.993774    1.047616   0.740887    1.864830  1.000000e-06
80    80.0   0.995605    1.047760   0.742375    1.867629  1.000000e-06
81    81.0   0.994141    1.046162   0.742251    1.864383  1.000000e-06
82    82.0   0.994019    1.045122   0.741825    1.867348  1.000000e-06
83    83.0   0.993408    1.045828   0.740639    1.868302  1.000000e-06
84    84.0   0.993652    1.046935   0.742003    1.862953  1.000000e-06
85    85.0   0.995117    1.047828   0.741879    1.867393  1.000000e-07
86    86.0   0.994507    1.045652   0.741383    1.868838  1.000000e-07
87    87.0   0.993896    1.047248   0.742003    1.863503  1.000000e-07
88    88.0   0.993652    1.048450   0.743065    1.861144  1.000000e-07
89    89.0   0.995361    1.046566   0.742941    1.864678  1.000000e-07
90    90.0   0.995117    1.046662   0.743615    1.868250  1.000000e-07
91    91.0   0.994385    1.046811   0.743615    1.868032  1.000000e-07
92    92.0   0.994385    1.045518   0.742003    1.865830  1.000000e-07
93    93.0   0.993774    1.047420   0.742995    1.862095  1.000000e-07
94    94.0   0.994019    1.047828   0.742817    1.868736  1.000000e-07
95    95.0   0.995483    1.044805   0.743367    1.862046  1.000000e-07
96    96.0   0.992920    1.045533   0.742747    1.860728  1.000000e-07
97    97.0   0.994751    1.044711   0.742003    1.864994  1.000000e-07
98    98.0   0.994385    1.048321   0.742747    1.866992  1.000000e-07
99    99.0   0.993652    1.047016   0.744111    1.860494  1.000000e-07
100  100.0   0.993896    1.049286   0.742941    1.863425  1.000000e-07
101  101.0   0.995483    1.046332   0.741259    1.863948  1.000000e-07
102  102.0   0.993164    1.048728   0.742127    1.866306  1.000000e-07
103  103.0   0.992798    1.047948   0.741259    1.861582  1.000000e-07
104  104.0   0.993896    1.047493   0.742127    1.866138  1.000000e-07
105  105.0   0.994995    1.043120   0.744111    1.865990  1.000000e-07
106  106.0   0.994629    1.046859   0.743615    1.864174  1.000000e-07
107  107.0   0.994507    1.045695   0.741701    1.858279  1.000000e-07
108  108.0   0.994263    1.050636   0.744111    1.866663  1.000000e-07
109  109.0   0.995361    1.045377   0.742127    1.864932  1.000000e-07
110  110.0   0.994263    1.048710   0.741259    1.868562  1.000000e-07
111  111.0   0.992188    1.048824   0.740267    1.866278  1.000000e-07
112  112.0   0.995728    1.043953   0.743243    1.869298  1.000000e-07
113  113.0   0.995483    1.044571   0.740956    1.870588  1.000000e-07
114  114.0   0.994263    1.046736   0.743491    1.858435  1.000000e-07
115  115.0   0.994141    1.048241   0.743491    1.866056  1.000000e-07
116  116.0   0.994141    1.048286   0.739468    1.870319  1.000000e-07
117  117.0   0.994629    1.045296   0.744359    1.862569  1.000000e-07
118  118.0   0.993652    1.045514   0.741755    1.861053  1.000000e-07
119  119.0   0.993774    1.046581   0.741879    1.869652  1.000000e-07
120  120.0   0.994385    1.047536   0.743615    1.862945  1.000000e-07
121  121.0   0.994873    1.046933   0.742127    1.866775  1.000000e-07
122  122.0   0.994141    1.046741   0.742251    1.862503  1.000000e-07

EXPERIMENT: C-38
================

NAME: ghost_20200921033732

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.4}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.4}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0     0.0   0.006836    5.353631   0.007192    5.336851  1.000000e-03
1     1.0   0.008301    5.288296   0.010045    5.209210  1.000000e-03
2     2.0   0.012695    5.194348   0.016121    5.196589  1.000000e-03
3     3.0   0.015259    5.139791   0.014633    5.128697  1.000000e-03
4     4.0   0.019653    5.080125   0.020213    5.118343  1.000000e-03
5     5.0   0.021973    5.025325   0.026414    5.045919  1.000000e-03
6     6.0   0.031128    4.948566   0.031761    4.961257  1.000000e-03
7     7.0   0.037354    4.856718   0.043115    4.832860  1.000000e-03
8     8.0   0.045166    4.755210   0.045293    4.755380  1.000000e-03
9     9.0   0.055054    4.652187   0.054098    4.645178  1.000000e-03
10   10.0   0.059814    4.548551   0.068304    4.570553  1.000000e-03
11   11.0   0.077393    4.451059   0.064375    4.529413  1.000000e-03
12   12.0   0.082520    4.353456   0.076667    4.405070  1.000000e-03
13   13.0   0.099121    4.259690   0.085665    4.339053  1.000000e-03
14   14.0   0.109741    4.157310   0.106499    4.174254  1.000000e-03
15   15.0   0.130981    4.065267   0.117674    4.169566  1.000000e-03
16   16.0   0.146729    3.966586   0.143771    3.957644  1.000000e-03
17   17.0   0.172485    3.839961   0.153567    3.963912  1.000000e-03
18   18.0   0.192017    3.739733   0.180190    3.800606  1.000000e-03
19   19.0   0.206421    3.641659   0.201852    3.703786  1.000000e-03
20   20.0   0.239258    3.530395   0.241991    3.560131  1.000000e-03
21   21.0   0.264404    3.433772   0.250974    3.501148  1.000000e-03
22   22.0   0.291016    3.316905   0.260771    3.438985  1.000000e-03
23   23.0   0.309204    3.226998   0.286138    3.341551  1.000000e-03
24   24.0   0.337524    3.141305   0.315458    3.286745  1.000000e-03
25   25.0   0.372437    3.028568   0.340290    3.187533  1.000000e-03
26   26.0   0.396484    2.943796   0.368316    3.052396  1.000000e-03
27   27.0   0.419434    2.858886   0.377299    3.046313  1.000000e-03
28   28.0   0.457642    2.739081   0.336679    3.114569  1.000000e-03
29   29.0   0.472900    2.681854   0.399387    2.980631  1.000000e-03
30   30.0   0.508057    2.586791   0.375771    3.048634  1.000000e-03
31   31.0   0.520752    2.531168   0.428033    2.833552  1.000000e-03
32   32.0   0.550903    2.448285   0.455880    2.742472  1.000000e-03
33   33.0   0.571045    2.375931   0.448401    2.781859  1.000000e-03
34   34.0   0.597168    2.279681   0.486471    2.648315  1.000000e-03
35   35.0   0.610840    2.244920   0.514085    2.568358  1.000000e-03
36   36.0   0.636230    2.175381   0.501065    2.627451  1.000000e-03
37   37.0   0.656738    2.124416   0.475737    2.658530  1.000000e-03
38   38.0   0.663574    2.070141   0.564472    2.386671  1.000000e-03
39   39.0   0.676636    2.031290   0.552528    2.420939  1.000000e-03
40   40.0   0.703857    1.969920   0.483580    2.669811  1.000000e-03
41   41.0   0.708008    1.949945   0.516357    2.528621  1.000000e-03
42   42.0   0.721436    1.912680   0.583476    2.349473  1.000000e-03
43   43.0   0.730103    1.873447   0.565588    2.381241  1.000000e-03
44   44.0   0.752686    1.819846   0.551194    2.455302  1.000000e-03
45   45.0   0.768066    1.782270   0.548971    2.440680  1.000000e-03
46   46.0   0.769165    1.766328   0.587359    2.309757  1.000000e-03
47   47.0   0.773193    1.747989   0.535097    2.492057  1.000000e-03
48   48.0   0.788452    1.724169   0.547692    2.485955  1.000000e-03
49   49.0   0.801636    1.673048   0.573416    2.387488  1.000000e-03
50   50.0   0.807495    1.658812   0.576764    2.381675  1.000000e-03
51   51.0   0.804077    1.654898   0.609974    2.260288  1.000000e-03
52   52.0   0.818237    1.624435   0.611556    2.236702  1.000000e-03
53   53.0   0.810425    1.647091   0.548297    2.479241  1.000000e-03
54   54.0   0.835815    1.572350   0.599155    2.277736  1.000000e-03
55   55.0   0.832520    1.589422   0.555489    2.442966  1.000000e-03
56   56.0   0.846802    1.544579   0.565355    2.427217  1.000000e-03
57   57.0   0.842651    1.548460   0.594721    2.332856  1.000000e-03
58   58.0   0.845215    1.555349   0.572424    2.337784  1.000000e-03
59   59.0   0.932983    1.295974   0.757365    1.771590  1.000000e-04
60   60.0   0.965454    1.193636   0.765356    1.739874  1.000000e-04
61   61.0   0.970337    1.169786   0.771199    1.728289  1.000000e-04
62   62.0   0.976685    1.146264   0.773447    1.726558  1.000000e-04
63   63.0   0.979126    1.136015   0.773501    1.721889  1.000000e-04
64   64.0   0.980591    1.127238   0.777206    1.710700  1.000000e-04
65   65.0   0.981201    1.118258   0.778446    1.709263  1.000000e-04
66   66.0   0.981445    1.111835   0.780182    1.703501  1.000000e-04
67   67.0   0.983643    1.104639   0.777896    1.718470  1.000000e-04
68   68.0   0.986084    1.097739   0.781066    1.713781  1.000000e-04
69   69.0   0.986572    1.088462   0.782166    1.705807  1.000000e-04
70   70.0   0.988159    1.083975   0.780430    1.714662  1.000000e-04
71   71.0   0.986450    1.083346   0.784468    1.704506  1.000000e-04
72   72.0   0.987427    1.074656   0.781368    1.714913  1.000000e-04
73   73.0   0.990356    1.060915   0.781174    1.704509  1.000000e-05
74   74.0   0.991577    1.062105   0.784042    1.701988  1.000000e-05
75   75.0   0.992432    1.058237   0.786095    1.696710  1.000000e-05
76   76.0   0.992065    1.054224   0.786963    1.698518  1.000000e-05
77   77.0   0.992188    1.056460   0.786343    1.696729  1.000000e-05
78   78.0   0.992798    1.052734   0.788204    1.697807  1.000000e-05
79   79.0   0.991821    1.052139   0.786591    1.698776  1.000000e-05
80   80.0   0.992065    1.052649   0.785584    1.699509  1.000000e-05
81   81.0   0.992310    1.049433   0.786328    1.697230  1.000000e-05
82   82.0   0.993286    1.050994   0.785530    1.700586  1.000000e-06
83   83.0   0.992554    1.052372   0.785832    1.695212  1.000000e-06
84   84.0   0.991211    1.052498   0.785778    1.698256  1.000000e-06
85   85.0   0.992310    1.050788   0.784964    1.700221  1.000000e-06
86   86.0   0.993652    1.048281   0.785351    1.703513  1.000000e-06
87   87.0   0.993164    1.050722   0.784731    1.701592  1.000000e-06
88   88.0   0.992310    1.048472   0.785530    1.698200  1.000000e-07
89   89.0   0.991333    1.051507   0.784786    1.701838  1.000000e-07
90   90.0   0.992310    1.049689   0.785530    1.697554  1.000000e-07
91   91.0   0.992432    1.050918   0.785847    1.702600  1.000000e-07
92   92.0   0.992554    1.050276   0.784483    1.703648  1.000000e-07
93   93.0   0.992554    1.053183   0.786522    1.699613  1.000000e-07
94   94.0   0.992065    1.051387   0.785406    1.696405  1.000000e-07
95   95.0   0.992676    1.048288   0.786343    1.695910  1.000000e-07
96   96.0   0.992432    1.050462   0.784111    1.697329  1.000000e-07
97   97.0   0.992920    1.050564   0.785158    1.703536  1.000000e-07
98   98.0   0.993652    1.051352   0.785406    1.703854  1.000000e-07

EXPERIMENT: C-39
================

NAME: ghost_20200921054059

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.005615    5.338487   0.008433    5.989491  1.000000e-03
1      1.0   0.009399    5.251981   0.011409    5.248475  1.000000e-03
2      2.0   0.010864    5.190397   0.014137    5.204443  1.000000e-03
3      3.0   0.015137    5.132105   0.013765    5.179194  1.000000e-03
4      4.0   0.017090    5.089460   0.018105    5.137512  1.000000e-03
5      5.0   0.026367    5.034539   0.019717    5.095953  1.000000e-03
6      6.0   0.028687    4.972659   0.023189    5.013976  1.000000e-03
7      7.0   0.034546    4.914831   0.024554    4.992119  1.000000e-03
8      8.0   0.034668    4.828656   0.038070    4.866616  1.000000e-03
9      9.0   0.039062    4.738656   0.038690    4.794395  1.000000e-03
10    10.0   0.052856    4.650881   0.042907    4.761174  1.000000e-03
11    11.0   0.058105    4.559516   0.051463    4.606821  1.000000e-03
12    12.0   0.069946    4.459502   0.057292    4.595941  1.000000e-03
13    13.0   0.077148    4.378394   0.074157    4.398535  1.000000e-03
14    14.0   0.093994    4.281218   0.083046    4.337664  1.000000e-03
15    15.0   0.104492    4.180193   0.060065    4.586617  1.000000e-03
16    16.0   0.114868    4.099241   0.114722    4.160487  1.000000e-03
17    17.0   0.134888    3.992173   0.133051    4.036396  1.000000e-03
18    18.0   0.157959    3.883541   0.118207    4.091022  1.000000e-03
19    19.0   0.165771    3.813089   0.163488    3.872362  1.000000e-03
20    20.0   0.196777    3.699172   0.179261    3.774077  1.000000e-03
21    21.0   0.218872    3.584902   0.162378    3.844870  1.000000e-03
22    22.0   0.247681    3.499384   0.219694    3.615475  1.000000e-03
23    23.0   0.266235    3.408297   0.261956    3.424851  1.000000e-03
24    24.0   0.290405    3.317949   0.237421    3.511193  1.000000e-03
25    25.0   0.312012    3.223743   0.231828    3.569351  1.000000e-03
26    26.0   0.343750    3.103880   0.284556    3.376909  1.000000e-03
27    27.0   0.369629    3.029145   0.327348    3.174347  1.000000e-03
28    28.0   0.393066    2.965785   0.277403    3.409381  1.000000e-03
29    29.0   0.416870    2.871296   0.293469    3.321265  1.000000e-03
30    30.0   0.434082    2.817220   0.238401    3.610982  1.000000e-03
31    31.0   0.467651    2.726677   0.401928    2.969362  1.000000e-03
32    32.0   0.489746    2.642403   0.311708    3.233731  1.000000e-03
33    33.0   0.503174    2.603528   0.370394    3.001223  1.000000e-03
34    34.0   0.519775    2.517358   0.343445    3.125094  1.000000e-03
35    35.0   0.547485    2.485168   0.411198    2.908442  1.000000e-03
36    36.0   0.553833    2.421225   0.329416    3.172190  1.000000e-03
37    37.0   0.562988    2.401837   0.426243    2.817077  1.000000e-03
38    38.0   0.586548    2.343008   0.454680    2.703604  1.000000e-03
39    39.0   0.605225    2.287341   0.356163    3.090370  1.000000e-03
40    40.0   0.606934    2.260761   0.488386    2.648269  1.000000e-03
41    41.0   0.614624    2.226396   0.418908    2.877972  1.000000e-03
42    42.0   0.633667    2.174686   0.443519    2.805526  1.000000e-03
43    43.0   0.647461    2.155909   0.461709    2.752511  1.000000e-03
44    44.0   0.656738    2.128464   0.348840    3.208227  1.000000e-03
45    45.0   0.657593    2.100556   0.446075    2.789622  1.000000e-03
46    46.0   0.669189    2.076504   0.497780    2.626002  1.000000e-03
47    47.0   0.667725    2.061772   0.462940    2.744853  1.000000e-03
48    48.0   0.695557    2.013103   0.491764    2.630331  1.000000e-03
49    49.0   0.690430    2.012818   0.448162    2.755617  1.000000e-03
50    50.0   0.705078    1.980006   0.534508    2.565499  1.000000e-03
51    51.0   0.707520    1.965441   0.437340    2.845253  1.000000e-03
52    52.0   0.720459    1.950175   0.462468    2.763153  1.000000e-03
53    53.0   0.724365    1.907478   0.530315    2.532064  1.000000e-03
54    54.0   0.722900    1.906158   0.455593    2.770082  1.000000e-03
55    55.0   0.734253    1.888922   0.547870    2.431410  1.000000e-03
56    56.0   0.735718    1.870044   0.539646    2.506812  1.000000e-03
57    57.0   0.753296    1.848873   0.479369    2.691881  1.000000e-03
58    58.0   0.754883    1.811939   0.584567    2.332663  1.000000e-03
59    59.0   0.751343    1.831319   0.508233    2.635474  1.000000e-03
60    60.0   0.756104    1.806303   0.422746    2.957471  1.000000e-03
61    61.0   0.757202    1.796418   0.531244    2.497113  1.000000e-03
62    62.0   0.757080    1.810117   0.528564    2.556185  1.000000e-03
63    63.0   0.756714    1.802240   0.523882    2.536389  1.000000e-03
64    64.0   0.781128    1.745119   0.556705    2.472932  1.000000e-03
65    65.0   0.893677    1.426009   0.790133    1.679799  1.000000e-04
66    66.0   0.932739    1.306037   0.797395    1.647118  1.000000e-04
67    67.0   0.940552    1.276672   0.805456    1.633832  1.000000e-04
68    68.0   0.948364    1.249608   0.807192    1.617397  1.000000e-04
69    69.0   0.952271    1.240140   0.810416    1.609940  1.000000e-04
70    70.0   0.956665    1.217481   0.814082    1.607378  1.000000e-04
71    71.0   0.960083    1.207914   0.814578    1.598821  1.000000e-04
72    72.0   0.961304    1.198171   0.806944    1.616840  1.000000e-04
73    73.0   0.965332    1.188684   0.812896    1.594459  1.000000e-04
74    74.0   0.967285    1.178234   0.811036    1.607278  1.000000e-04
75    75.0   0.965698    1.172291   0.816492    1.596832  1.000000e-04
76    76.0   0.969360    1.160395   0.812648    1.599409  1.000000e-04
77    77.0   0.971680    1.160613   0.814027    1.602315  1.000000e-04
78    78.0   0.975586    1.144388   0.808447    1.613640  1.000000e-04
79    79.0   0.977661    1.136676   0.817593    1.597510  1.000000e-04
80    80.0   0.979004    1.119449   0.821453    1.575700  1.000000e-05
81    81.0   0.981689    1.114189   0.820089    1.576325  1.000000e-05
82    82.0   0.983398    1.107462   0.821577    1.578135  1.000000e-05
83    83.0   0.982910    1.109541   0.822941    1.576571  1.000000e-05
84    84.0   0.983398    1.104925   0.823189    1.568842  1.000000e-05
85    85.0   0.983154    1.105823   0.822693    1.573768  1.000000e-05
86    86.0   0.981567    1.102534   0.825297    1.566877  1.000000e-05
87    87.0   0.984253    1.100432   0.822321    1.572536  1.000000e-05
88    88.0   0.982178    1.100152   0.822197    1.574061  1.000000e-05
89    89.0   0.984497    1.096573   0.823065    1.570917  1.000000e-05
90    90.0   0.984619    1.099604   0.823630    1.573224  1.000000e-05
91    91.0   0.983154    1.100048   0.823065    1.576135  1.000000e-05
92    92.0   0.984009    1.098098   0.824002    1.571694  1.000000e-05
93    93.0   0.984863    1.095182   0.822886    1.573759  1.000000e-06
94    94.0   0.985718    1.094212   0.823382    1.568387  1.000000e-06
95    95.0   0.984985    1.095441   0.823754    1.565521  1.000000e-06
96    96.0   0.985352    1.090766   0.824374    1.568387  1.000000e-06
97    97.0   0.984741    1.090657   0.825367    1.566835  1.000000e-06
98    98.0   0.983398    1.097200   0.823878    1.564782  1.000000e-06
99    99.0   0.986450    1.093222   0.824250    1.570218  1.000000e-06
100  100.0   0.983398    1.095383   0.824623    1.567247  1.000000e-06
101  101.0   0.986572    1.091940   0.824623    1.569399  1.000000e-06
102  102.0   0.982422    1.094451   0.824498    1.567676  1.000000e-06
103  103.0   0.984131    1.094071   0.822390    1.566334  1.000000e-06
104  104.0   0.986694    1.090604   0.823878    1.566289  1.000000e-06
105  105.0   0.985474    1.089752   0.825491    1.567006  1.000000e-07
106  106.0   0.984985    1.092032   0.824498    1.563492  1.000000e-07
107  107.0   0.984741    1.091362   0.824623    1.568029  1.000000e-07
108  108.0   0.983643    1.095378   0.824374    1.566528  1.000000e-07
109  109.0   0.986694    1.092182   0.824746    1.568030  1.000000e-07
110  110.0   0.984619    1.091607   0.824250    1.567022  1.000000e-07
111  111.0   0.986572    1.089970   0.822762    1.565811  1.000000e-07
112  112.0   0.983276    1.095956   0.823010    1.568801  1.000000e-07
113  113.0   0.985962    1.090820   0.824995    1.567903  1.000000e-07
114  114.0   0.985229    1.094880   0.824498    1.571317  1.000000e-07
115  115.0   0.983887    1.092608   0.824002    1.563907  1.000000e-07
116  116.0   0.984131    1.096343   0.824250    1.569550  1.000000e-07
117  117.0   0.984131    1.090972   0.823010    1.568401  1.000000e-07
118  118.0   0.984985    1.094077   0.824746    1.569618  1.000000e-07
119  119.0   0.983643    1.093016   0.822638    1.568725  1.000000e-07
120  120.0   0.984985    1.089843   0.824002    1.573504  1.000000e-07
121  121.0   0.985474    1.093647   0.823382    1.566151  1.000000e-07

EXPERIMENT: C-40
================

NAME: ghost_20200921092137

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.7}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.7}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.006104    5.362166   0.012153    5.262550  1.000000e-03
1      1.0   0.008301    5.276319   0.010541    5.227253  1.000000e-03
2      2.0   0.013794    5.197681   0.017237    5.183189  1.000000e-03
3      3.0   0.018066    5.130241   0.016741    5.132008  1.000000e-03
4      4.0   0.019043    5.088284   0.021081    5.083713  1.000000e-03
5      5.0   0.023560    5.040853   0.022073    5.055157  1.000000e-03
6      6.0   0.026978    4.990419   0.023438    5.026408  1.000000e-03
7      7.0   0.030151    4.947067   0.031498    4.948882  1.000000e-03
8      8.0   0.036133    4.897782   0.024430    5.052288  1.000000e-03
9      9.0   0.041870    4.801074   0.031692    4.945389  1.000000e-03
10    10.0   0.048828    4.689623   0.052579    4.688573  1.000000e-03
11    11.0   0.059814    4.578208   0.043224    4.703252  1.000000e-03
12    12.0   0.069214    4.475371   0.075993    4.408225  1.000000e-03
13    13.0   0.084961    4.348604   0.068716    4.456801  1.000000e-03
14    14.0   0.092896    4.249075   0.101206    4.234774  1.000000e-03
15    15.0   0.109741    4.137049   0.104351    4.224946  1.000000e-03
16    16.0   0.128662    4.044196   0.099391    4.226241  1.000000e-03
17    17.0   0.142944    3.961645   0.126270    4.023027  1.000000e-03
18    18.0   0.166016    3.856273   0.141039    3.991144  1.000000e-03
19    19.0   0.183228    3.741656   0.121791    4.077691  1.000000e-03
20    20.0   0.201904    3.668496   0.191762    3.700293  1.000000e-03
21    21.0   0.224854    3.572504   0.163494    3.864136  1.000000e-03
22    22.0   0.252319    3.474703   0.187412    3.736773  1.000000e-03
23    23.0   0.277832    3.385836   0.227149    3.585158  1.000000e-03
24    24.0   0.306274    3.269763   0.305737    3.294680  1.000000e-03
25    25.0   0.328491    3.203515   0.245998    3.506248  1.000000e-03
26    26.0   0.350464    3.107436   0.283334    3.323094  1.000000e-03
27    27.0   0.379150    3.013537   0.178756    3.867449  1.000000e-03
28    28.0   0.403931    2.933599   0.315204    3.294489  1.000000e-03
29    29.0   0.425293    2.856598   0.233071    3.549078  1.000000e-03
30    30.0   0.445801    2.798180   0.258777    3.443314  1.000000e-03
31    31.0   0.588623    2.400942   0.580306    2.407857  1.000000e-04
32    32.0   0.651245    2.254997   0.598783    2.359745  1.000000e-04
33    33.0   0.673706    2.201521   0.598163    2.348339  1.000000e-04
34    34.0   0.683960    2.153956   0.624220    2.277161  1.000000e-04
35    35.0   0.693604    2.109622   0.621491    2.286205  1.000000e-04
36    36.0   0.713623    2.069703   0.627940    2.249072  1.000000e-04
37    37.0   0.715942    2.041638   0.631055    2.245203  1.000000e-04
38    38.0   0.726440    2.014386   0.628327    2.228069  1.000000e-04
39    39.0   0.731201    2.001102   0.638922    2.216641  1.000000e-04
40    40.0   0.744629    1.957757   0.653555    2.173514  1.000000e-04
41    41.0   0.749512    1.933608   0.650152    2.175113  1.000000e-04
42    42.0   0.752808    1.915326   0.660197    2.145278  1.000000e-04
43    43.0   0.764160    1.887629   0.666452    2.126915  1.000000e-04
44    44.0   0.776245    1.857059   0.662414    2.140026  1.000000e-04
45    45.0   0.783813    1.824460   0.666328    2.116725  1.000000e-04
46    46.0   0.791138    1.806304   0.666149    2.116902  1.000000e-04
47    47.0   0.798584    1.785597   0.674582    2.090957  1.000000e-04
48    48.0   0.806030    1.758654   0.673908    2.074593  1.000000e-04
49    49.0   0.809814    1.746801   0.675326    2.080127  1.000000e-04
50    50.0   0.820435    1.716235   0.684503    2.036955  1.000000e-04
51    51.0   0.824219    1.694818   0.683813    2.038145  1.000000e-04
52    52.0   0.830811    1.678208   0.686928    2.037010  1.000000e-04
53    53.0   0.833496    1.661330   0.685867    2.042658  1.000000e-04
54    54.0   0.840698    1.644379   0.690470    2.008803  1.000000e-04
55    55.0   0.850220    1.619855   0.680247    2.058581  1.000000e-04
56    56.0   0.859253    1.606185   0.692137    2.010186  1.000000e-04
57    57.0   0.859741    1.595602   0.681829    2.034248  1.000000e-04
58    58.0   0.866821    1.567911   0.698972    2.007029  1.000000e-04
59    59.0   0.869629    1.567225   0.703282    1.978142  1.000000e-04
60    60.0   0.873657    1.542871   0.704568    1.984174  1.000000e-04
61    61.0   0.881348    1.518525   0.707598    1.959571  1.000000e-04
62    62.0   0.882446    1.515840   0.703669    1.960393  1.000000e-04
63    63.0   0.890869    1.482921   0.698391    1.988526  1.000000e-04
64    64.0   0.896851    1.472969   0.711412    1.955866  1.000000e-04
65    65.0   0.894531    1.463825   0.704289    1.956219  1.000000e-04
66    66.0   0.907593    1.439552   0.701661    1.970070  1.000000e-04
67    67.0   0.907104    1.431260   0.709056    1.952683  1.000000e-04
68    68.0   0.909546    1.422591   0.708258    1.946632  1.000000e-04
69    69.0   0.916748    1.410679   0.708560    1.961960  1.000000e-04
70    70.0   0.918091    1.393639   0.721309    1.915481  1.000000e-04
71    71.0   0.925537    1.378901   0.713272    1.932386  1.000000e-04
72    72.0   0.924072    1.370507   0.715287    1.927752  1.000000e-04
73    73.0   0.929688    1.362148   0.712365    1.936146  1.000000e-04
74    74.0   0.932617    1.349671   0.712583    1.933677  1.000000e-04
75    75.0   0.930664    1.344503   0.717147    1.911240  1.000000e-04
76    76.0   0.939819    1.326495   0.704855    1.952074  1.000000e-04
77    77.0   0.942261    1.316132   0.725673    1.898818  1.000000e-04
78    78.0   0.938232    1.312038   0.730510    1.889951  1.000000e-04
79    79.0   0.940063    1.305318   0.723952    1.912869  1.000000e-04
80    80.0   0.944214    1.296717   0.712628    1.938413  1.000000e-04
81    81.0   0.942749    1.298897   0.718596    1.903274  1.000000e-04
82    82.0   0.946045    1.284433   0.711854    1.947024  1.000000e-04
83    83.0   0.951172    1.270918   0.713768    1.926183  1.000000e-04
84    84.0   0.951782    1.267407   0.727673    1.883167  1.000000e-04
85    85.0   0.954590    1.257054   0.720232    1.912683  1.000000e-04
86    86.0   0.957153    1.248335   0.721115    1.903796  1.000000e-04
87    87.0   0.964111    1.231225   0.722658    1.900529  1.000000e-04
88    88.0   0.955566    1.245443   0.712241    1.937615  1.000000e-04
89    89.0   0.962891    1.225371   0.726200    1.902524  1.000000e-04
90    90.0   0.963501    1.225556   0.727618    1.872089  1.000000e-04
91    91.0   0.960938    1.219567   0.710257    1.932811  1.000000e-04
92    92.0   0.966553    1.209620   0.724944    1.877823  1.000000e-04
93    93.0   0.966431    1.198815   0.728308    1.879569  1.000000e-04
94    94.0   0.968140    1.196888   0.721775    1.902115  1.000000e-04
95    95.0   0.969116    1.197663   0.725292    1.884539  1.000000e-04
96    96.0   0.968140    1.193656   0.717480    1.905378  1.000000e-04
97    97.0   0.981689    1.142847   0.746125    1.814366  1.000000e-05
98    98.0   0.984741    1.126899   0.748025    1.813349  1.000000e-05
99    99.0   0.985596    1.116354   0.750163    1.816059  1.000000e-05
100  100.0   0.988281    1.110221   0.746676    1.814595  1.000000e-05
101  101.0   0.985596    1.114948   0.747723    1.819526  1.000000e-05
102  102.0   0.988037    1.110407   0.749459    1.808351  1.000000e-05
103  103.0   0.987793    1.106296   0.750436    1.811417  1.000000e-05
104  104.0   0.988525    1.105078   0.746165    1.811786  1.000000e-05
105  105.0   0.987305    1.105091   0.745421    1.809403  1.000000e-05
106  106.0   0.988037    1.100748   0.748288    1.808876  1.000000e-05
107  107.0   0.989258    1.100075   0.749513    1.810119  1.000000e-05
108  108.0   0.987793    1.098247   0.750079    1.813538  1.000000e-05
109  109.0   0.988159    1.095681   0.751249    1.801893  1.000000e-06
110  110.0   0.989136    1.098036   0.751071    1.809021  1.000000e-06
111  111.0   0.989380    1.092418   0.750575    1.805289  1.000000e-06
112  112.0   0.990845    1.094428   0.749141    1.807902  1.000000e-06
113  113.0   0.989746    1.093737   0.747723    1.809032  1.000000e-06
114  114.0   0.988159    1.096473   0.750133    1.804572  1.000000e-06
115  115.0   0.990234    1.093534   0.750009    1.808070  1.000000e-06
116  116.0   0.992065    1.093420   0.750327    1.804816  1.000000e-07
117  117.0   0.989990    1.094528   0.751195    1.801844  1.000000e-07
118  118.0   0.987549    1.096756   0.747350    1.809832  1.000000e-07
119  119.0   0.987671    1.095448   0.750079    1.805239  1.000000e-07
120  120.0   0.989014    1.092902   0.747846    1.811240  1.000000e-07
121  121.0   0.991577    1.091760   0.750575    1.808503  1.000000e-07
122  122.0   0.988770    1.091755   0.750575    1.807152  1.000000e-07
123  123.0   0.989868    1.095071   0.748343    1.812369  1.000000e-07
124  124.0   0.988525    1.095006   0.750947    1.804179  1.000000e-07
125  125.0   0.988647    1.095788   0.750009    1.808028  1.000000e-07
126  126.0   0.992432    1.089066   0.750024    1.810611  1.000000e-07
127  127.0   0.991455    1.092981   0.749513    1.810986  1.000000e-07
128  128.0   0.989868    1.094414   0.750629    1.801875  1.000000e-07
129  129.0   0.988281    1.095856   0.750947    1.803694  1.000000e-07
130  130.0   0.991455    1.090647   0.750947    1.804118  1.000000e-07
131  131.0   0.989502    1.092308   0.749211    1.805426  1.000000e-07
132  132.0   0.990479    1.088987   0.750327    1.806299  1.000000e-07

EXPERIMENT: C-41
================

NAME: ghost_20200921144157

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.3,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.004639    5.330980   0.008557    5.335160  1.000000e-03
1      1.0   0.010010    5.242358   0.015129    5.180655  1.000000e-03
2      2.0   0.012329    5.202265   0.013393    5.217023  1.000000e-03
3      3.0   0.014282    5.150604   0.017485    5.149447  1.000000e-03
4      4.0   0.016968    5.110716   0.013517    5.124792  1.000000e-03
5      5.0   0.021729    5.052408   0.026414    5.002678  1.000000e-03
6      6.0   0.025757    4.952190   0.027530    4.975543  1.000000e-03
7      7.0   0.031982    4.881746   0.027793    4.940761  1.000000e-03
8      8.0   0.040161    4.790327   0.046657    4.741618  1.000000e-03
9      9.0   0.046265    4.689636   0.054067    4.649926  1.000000e-03
10    10.0   0.053101    4.593657   0.054083    4.644407  1.000000e-03
11    11.0   0.064575    4.509776   0.062188    4.539790  1.000000e-03
12    12.0   0.069336    4.431578   0.060392    4.540545  1.000000e-03
13    13.0   0.083984    4.337466   0.081836    4.376973  1.000000e-03
14    14.0   0.095703    4.239355   0.101801    4.230463  1.000000e-03
15    15.0   0.109863    4.160496   0.114341    4.130420  1.000000e-03
16    16.0   0.119995    4.060236   0.110294    4.190554  1.000000e-03
17    17.0   0.140991    3.982177   0.138514    3.997972  1.000000e-03
18    18.0   0.163208    3.870780   0.180120    3.801027  1.000000e-03
19    19.0   0.179077    3.778751   0.128805    4.030798  1.000000e-03
20    20.0   0.202393    3.665034   0.189629    3.775513  1.000000e-03
21    21.0   0.230835    3.576907   0.193271    3.711599  1.000000e-03
22    22.0   0.247070    3.464422   0.212220    3.648112  1.000000e-03
23    23.0   0.278198    3.371503   0.230007    3.578316  1.000000e-03
24    24.0   0.296143    3.297027   0.273386    3.378658  1.000000e-03
25    25.0   0.320190    3.227556   0.289477    3.351110  1.000000e-03
26    26.0   0.349731    3.119147   0.282185    3.341732  1.000000e-03
27    27.0   0.367798    3.056089   0.287919    3.341873  1.000000e-03
28    28.0   0.390137    2.987994   0.292235    3.331359  1.000000e-03
29    29.0   0.412476    2.912396   0.254531    3.496297  1.000000e-03
30    30.0   0.423706    2.844391   0.331168    3.193181  1.000000e-03
31    31.0   0.449097    2.763195   0.366431    3.090412  1.000000e-03
32    32.0   0.475464    2.686297   0.386977    2.956177  1.000000e-03
33    33.0   0.484009    2.662509   0.404263    2.935427  1.000000e-03
34    34.0   0.508057    2.586691   0.356474    3.066440  1.000000e-03
35    35.0   0.538330    2.511044   0.419927    2.886802  1.000000e-03
36    36.0   0.524414    2.503239   0.471793    2.736700  1.000000e-03
37    37.0   0.558716    2.422127   0.425305    2.858505  1.000000e-03
38    38.0   0.564575    2.411865   0.309512    3.346262  1.000000e-03
39    39.0   0.580078    2.358056   0.435395    2.848159  1.000000e-03
40    40.0   0.595215    2.320376   0.437642    2.794484  1.000000e-03
41    41.0   0.611572    2.267051   0.481680    2.704808  1.000000e-03
42    42.0   0.609497    2.249787   0.471805    2.732517  1.000000e-03
43    43.0   0.614624    2.233691   0.399780    3.015671  1.000000e-03
44    44.0   0.629761    2.194978   0.509080    2.581526  1.000000e-03
45    45.0   0.640625    2.183394   0.390583    3.029906  1.000000e-03
46    46.0   0.652832    2.139411   0.479179    2.670935  1.000000e-03
47    47.0   0.656738    2.128584   0.474978    2.726139  1.000000e-03
48    48.0   0.668945    2.086609   0.512053    2.564636  1.000000e-03
49    49.0   0.678833    2.043749   0.424800    2.857687  1.000000e-03
50    50.0   0.676392    2.060533   0.469031    2.716857  1.000000e-03
51    51.0   0.683838    2.024936   0.373530    3.122778  1.000000e-03
52    52.0   0.690918    2.002474   0.542018    2.501863  1.000000e-03
53    53.0   0.704102    1.978339   0.385949    3.042799  1.000000e-03
54    54.0   0.699951    1.989626   0.485694    2.687577  1.000000e-03
55    55.0   0.705200    1.956354   0.547613    2.463000  1.000000e-03
56    56.0   0.726685    1.923656   0.524596    2.580487  1.000000e-03
57    57.0   0.719971    1.925339   0.523253    2.537361  1.000000e-03
58    58.0   0.721558    1.920840   0.516765    2.587967  1.000000e-03
59    59.0   0.739014    1.879702   0.539894    2.474673  1.000000e-03
60    60.0   0.730835    1.888194   0.465892    2.756159  1.000000e-03
61    61.0   0.732788    1.885535   0.513148    2.563497  1.000000e-03
62    62.0   0.854736    1.549110   0.777608    1.717580  1.000000e-04
63    63.0   0.906494    1.409263   0.788521    1.682784  1.000000e-04
64    64.0   0.913574    1.369609   0.792683    1.665700  1.000000e-04
65    65.0   0.927368    1.327964   0.798085    1.645706  1.000000e-04
66    66.0   0.934326    1.312467   0.801433    1.639880  1.000000e-04
67    67.0   0.938965    1.290051   0.801929    1.638540  1.000000e-04
68    68.0   0.941772    1.280014   0.799325    1.628819  1.000000e-04
69    69.0   0.943848    1.267375   0.806230    1.617462  1.000000e-04
70    70.0   0.949463    1.252485   0.801681    1.635241  1.000000e-04
71    71.0   0.952759    1.244622   0.802247    1.636535  1.000000e-04
72    72.0   0.960571    1.226509   0.808393    1.607536  1.000000e-04
73    73.0   0.958740    1.224089   0.809246    1.601381  1.000000e-04
74    74.0   0.957397    1.216871   0.805580    1.620917  1.000000e-04
75    75.0   0.962769    1.205993   0.808556    1.616481  1.000000e-04
76    76.0   0.963745    1.193891   0.804975    1.617146  1.000000e-04
77    77.0   0.966431    1.189415   0.811230    1.602705  1.000000e-04
78    78.0   0.969360    1.174039   0.805843    1.617264  1.000000e-04
79    79.0   0.968506    1.177177   0.806587    1.615788  1.000000e-04
80    80.0   0.977539    1.151188   0.815888    1.585411  1.000000e-05
81    81.0   0.976074    1.145345   0.818120    1.585091  1.000000e-05
82    82.0   0.977539    1.139148   0.818065    1.582820  1.000000e-05
83    83.0   0.978394    1.135123   0.818244    1.578621  1.000000e-05
84    84.0   0.980347    1.132077   0.818616    1.578576  1.000000e-05
85    85.0   0.978149    1.133872   0.815516    1.579982  1.000000e-05
86    86.0   0.979858    1.131276   0.818437    1.579667  1.000000e-05
87    87.0   0.978638    1.131621   0.818189    1.579473  1.000000e-05
88    88.0   0.979370    1.127314   0.819236    1.574272  1.000000e-05
89    89.0   0.978271    1.125082   0.816260    1.579686  1.000000e-05
90    90.0   0.980835    1.124917   0.819305    1.577119  1.000000e-05
91    91.0   0.981201    1.122187   0.819429    1.574415  1.000000e-05
92    92.0   0.980713    1.120560   0.818933    1.577619  1.000000e-05
93    93.0   0.982422    1.118678   0.819305    1.576706  1.000000e-05
94    94.0   0.982422    1.121043   0.818809    1.581042  1.000000e-05
95    95.0   0.982422    1.116784   0.818685    1.578716  1.000000e-06
96    96.0   0.983032    1.117121   0.819057    1.577299  1.000000e-06
97    97.0   0.981934    1.117193   0.819305    1.574455  1.000000e-06
98    98.0   0.981812    1.116303   0.820545    1.575148  1.000000e-06
99    99.0   0.981445    1.118249   0.816632    1.581219  1.000000e-06
100  100.0   0.981323    1.116303   0.819856    1.571001  1.000000e-06
101  101.0   0.981934    1.118927   0.820173    1.575890  1.000000e-06
102  102.0   0.982300    1.117086   0.820848    1.571616  1.000000e-06
103  103.0   0.982178    1.116202   0.819181    1.578665  1.000000e-06
104  104.0   0.980469    1.114954   0.819429    1.574352  1.000000e-06
105  105.0   0.981201    1.117009   0.817197    1.574617  1.000000e-06
106  106.0   0.982056    1.114314   0.819677    1.573888  1.000000e-06
107  107.0   0.983154    1.110720   0.819925    1.574220  1.000000e-07
108  108.0   0.981323    1.118563   0.819801    1.577153  1.000000e-07
109  109.0   0.979980    1.118791   0.819057    1.577456  1.000000e-07
110  110.0   0.983765    1.115115   0.819677    1.578814  1.000000e-07
111  111.0   0.982666    1.116482   0.818933    1.575286  1.000000e-07
112  112.0   0.983032    1.112459   0.819305    1.572491  1.000000e-07
113  113.0   0.982422    1.118474   0.820104    1.572610  1.000000e-07
114  114.0   0.979980    1.118722   0.820297    1.575381  1.000000e-07
115  115.0   0.984863    1.114448   0.820297    1.579872  1.000000e-07

EXPERIMENT: C-42
================

NAME: ghost_20200921124543

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.4,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.008423    5.357541   0.010680    5.338696  1.000000e-03
1      1.0   0.009521    5.261122   0.013765    5.182360  1.000000e-03
2      2.0   0.011353    5.214787   0.013090    5.211539  1.000000e-03
3      3.0   0.013794    5.171942   0.012718    5.152537  1.000000e-03
4      4.0   0.015259    5.120431   0.021895    5.094154  1.000000e-03
5      5.0   0.020874    5.073891   0.024747    5.041940  1.000000e-03
6      6.0   0.024658    5.021620   0.019484    5.074224  1.000000e-03
7      7.0   0.027832    4.960526   0.031637    4.958077  1.000000e-03
8      8.0   0.033447    4.898855   0.029955    5.003111  1.000000e-03
9      9.0   0.039551    4.816022   0.025243    5.054385  1.000000e-03
10    10.0   0.047974    4.731419   0.040372    4.790482  1.000000e-03
11    11.0   0.060181    4.626214   0.056593    4.630351  1.000000e-03
12    12.0   0.061646    4.525647   0.062322    4.541554  1.000000e-03
13    13.0   0.071777    4.440273   0.086394    4.359485  1.000000e-03
14    14.0   0.083618    4.346385   0.086890    4.339327  1.000000e-03
15    15.0   0.094604    4.263014   0.079767    4.377524  1.000000e-03
16    16.0   0.103027    4.183775   0.098562    4.192700  1.000000e-03
17    17.0   0.118164    4.088443   0.082015    4.350801  1.000000e-03
18    18.0   0.131714    4.012600   0.128185    4.058037  1.000000e-03
19    19.0   0.140747    3.942809   0.129386    4.023083  1.000000e-03
20    20.0   0.155640    3.870786   0.141097    3.936121  1.000000e-03
21    21.0   0.169312    3.820808   0.146801    3.888000  1.000000e-03
22    22.0   0.190674    3.736044   0.114574    4.094105  1.000000e-03
23    23.0   0.200562    3.660455   0.182298    3.771142  1.000000e-03
24    24.0   0.222290    3.606685   0.186320    3.741107  1.000000e-03
25    25.0   0.234863    3.536839   0.182615    3.809524  1.000000e-03
26    26.0   0.256592    3.462319   0.195914    3.733249  1.000000e-03
27    27.0   0.282959    3.369453   0.164867    3.846500  1.000000e-03
28    28.0   0.295166    3.302738   0.221445    3.630065  1.000000e-03
29    29.0   0.307983    3.261504   0.193325    3.788664  1.000000e-03
30    30.0   0.338745    3.171934   0.240388    3.469247  1.000000e-03
31    31.0   0.342773    3.112410   0.269778    3.420113  1.000000e-03
32    32.0   0.383179    3.015629   0.269273    3.413010  1.000000e-03
33    33.0   0.392578    2.963624   0.323479    3.163909  1.000000e-03
34    34.0   0.407471    2.921089   0.286510    3.365613  1.000000e-03
35    35.0   0.429321    2.854529   0.240938    3.594436  1.000000e-03
36    36.0   0.439575    2.809470   0.330230    3.158894  1.000000e-03
37    37.0   0.452271    2.780577   0.429064    2.851998  1.000000e-03
38    38.0   0.479004    2.701297   0.239039    3.641584  1.000000e-03
39    39.0   0.490723    2.648636   0.343236    3.138537  1.000000e-03
40    40.0   0.506104    2.598774   0.338950    3.141761  1.000000e-03
41    41.0   0.522583    2.560071   0.428816    2.828017  1.000000e-03
42    42.0   0.529663    2.523442   0.370037    3.096261  1.000000e-03
43    43.0   0.541016    2.484128   0.353931    3.094743  1.000000e-03
44    44.0   0.547485    2.479899   0.415236    2.866495  1.000000e-03
45    45.0   0.563843    2.420605   0.380592    3.049501  1.000000e-03
46    46.0   0.571045    2.386539   0.444774    2.793363  1.000000e-03
47    47.0   0.581055    2.360185   0.329625    3.235363  1.000000e-03
48    48.0   0.581177    2.347676   0.433211    2.833511  1.000000e-03
49    49.0   0.592773    2.330311   0.440519    2.804961  1.000000e-03
50    50.0   0.599976    2.311407   0.347080    3.126060  1.000000e-03
51    51.0   0.619507    2.259245   0.442279    2.806214  1.000000e-03
52    52.0   0.619873    2.229177   0.382344    2.994571  1.000000e-03
53    53.0   0.752441    1.873643   0.726091    1.909805  1.000000e-04
54    54.0   0.812256    1.727331   0.731105    1.893523  1.000000e-04
55    55.0   0.826782    1.670696   0.739925    1.849420  1.000000e-04
56    56.0   0.837158    1.636777   0.744761    1.830483  1.000000e-04
57    57.0   0.848389    1.607462   0.748606    1.820515  1.000000e-04
58    58.0   0.849243    1.593479   0.754489    1.801184  1.000000e-04
59    59.0   0.862183    1.566277   0.753992    1.794259  1.000000e-04
60    60.0   0.864990    1.545948   0.753496    1.801876  1.000000e-04
61    61.0   0.873291    1.523772   0.758620    1.769779  1.000000e-04
62    62.0   0.879517    1.505814   0.761914    1.788041  1.000000e-04
63    63.0   0.884888    1.489496   0.769245    1.747588  1.000000e-04
64    64.0   0.893433    1.475652   0.768005    1.747470  1.000000e-04
65    65.0   0.890991    1.458137   0.771904    1.738906  1.000000e-04
66    66.0   0.896240    1.452137   0.769920    1.739610  1.000000e-04
67    67.0   0.901978    1.431847   0.777112    1.712494  1.000000e-04
68    68.0   0.906006    1.415179   0.778422    1.715525  1.000000e-04
69    69.0   0.911865    1.406018   0.770362    1.721838  1.000000e-04
70    70.0   0.915894    1.394894   0.773090    1.715891  1.000000e-04
71    71.0   0.914917    1.383512   0.772206    1.720606  1.000000e-04
72    72.0   0.927002    1.361333   0.776740    1.717271  1.000000e-04
73    73.0   0.921631    1.366520   0.758566    1.755379  1.000000e-04
74    74.0   0.938721    1.317742   0.788893    1.662405  1.000000e-05
75    75.0   0.946289    1.307666   0.790451    1.664282  1.000000e-05
76    76.0   0.944092    1.300392   0.790451    1.661287  1.000000e-05
77    77.0   0.946289    1.293362   0.791373    1.652626  1.000000e-05
78    78.0   0.947754    1.291115   0.792683    1.649578  1.000000e-05
79    79.0   0.944580    1.286494   0.794473    1.656171  1.000000e-05
80    80.0   0.948608    1.285542   0.793729    1.650538  1.000000e-05
81    81.0   0.943359    1.288084   0.793481    1.652792  1.000000e-05
82    82.0   0.950317    1.278966   0.791939    1.652601  1.000000e-05
83    83.0   0.949219    1.282667   0.794171    1.654168  1.000000e-05
84    84.0   0.949585    1.282570   0.793729    1.651790  1.000000e-05
85    85.0   0.950439    1.273198   0.794295    1.649632  1.000000e-06
86    86.0   0.948730    1.274076   0.793303    1.649065  1.000000e-06
87    87.0   0.951416    1.279301   0.793799    1.648333  1.000000e-06
88    88.0   0.951294    1.281016   0.793675    1.649298  1.000000e-06
89    89.0   0.950439    1.273739   0.793303    1.650017  1.000000e-06
90    90.0   0.948486    1.277039   0.794047    1.649156  1.000000e-06
91    91.0   0.951294    1.276170   0.792931    1.652789  1.000000e-07
92    92.0   0.948853    1.278845   0.794101    1.648056  1.000000e-07
93    93.0   0.948853    1.277655   0.792807    1.654513  1.000000e-07
94    94.0   0.951538    1.272649   0.791443    1.649929  1.000000e-07
95    95.0   0.950684    1.275659   0.795535    1.646429  1.000000e-07
96    96.0   0.947510    1.275293   0.793179    1.651268  1.000000e-07
97    97.0   0.949585    1.274678   0.793357    1.648104  1.000000e-07
98    98.0   0.949463    1.277669   0.794721    1.650124  1.000000e-07
99    99.0   0.951416    1.276114   0.792985    1.653943  1.000000e-07
100  100.0   0.953369    1.270105   0.792063    1.659965  1.000000e-07
101  101.0   0.948730    1.278210   0.793233    1.651488  1.000000e-07
102  102.0   0.949097    1.275418   0.793109    1.652387  1.000000e-07
103  103.0   0.951904    1.274891   0.794543    1.651510  1.000000e-07
104  104.0   0.952148    1.274549   0.792559    1.653669  1.000000e-07
105  105.0   0.952637    1.272569   0.792559    1.649418  1.000000e-07
106  106.0   0.948120    1.276036   0.795039    1.647026  1.000000e-07
107  107.0   0.951660    1.273297   0.795659    1.648720  1.000000e-07
108  108.0   0.949951    1.275498   0.793799    1.646830  1.000000e-07
109  109.0   0.952148    1.274091   0.793675    1.651550  1.000000e-07
110  110.0   0.951416    1.275004   0.794171    1.654019  1.000000e-07

EXPERIMENT: C-43
================

NAME: ghost_20200921124813

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.5,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.009521    5.349322   0.012222    5.268482  1.000000e-03
1      1.0   0.007446    5.253885   0.012277    5.259650  1.000000e-03
2      2.0   0.012329    5.213511   0.015997    5.195663  1.000000e-03
3      3.0   0.014282    5.180039   0.014261    5.152087  1.000000e-03
4      4.0   0.015625    5.145379   0.016989    5.131313  1.000000e-03
5      5.0   0.016357    5.104744   0.016865    5.116474  1.000000e-03
6      6.0   0.018921    5.062795   0.016245    5.113028  1.000000e-03
7      7.0   0.021240    5.025137   0.021717    5.048523  1.000000e-03
8      8.0   0.024658    4.965018   0.024058    5.038682  1.000000e-03
9      9.0   0.028687    4.910078   0.031746    4.907816  1.000000e-03
10    10.0   0.034546    4.840075   0.028715    4.974395  1.000000e-03
11    11.0   0.038574    4.768497   0.028219    4.985082  1.000000e-03
12    12.0   0.041992    4.708322   0.040814    4.714226  1.000000e-03
13    13.0   0.047607    4.623939   0.054083    4.566273  1.000000e-03
14    14.0   0.060059    4.537252   0.052222    4.562748  1.000000e-03
15    15.0   0.063477    4.482305   0.054703    4.555192  1.000000e-03
16    16.0   0.071655    4.418325   0.065670    4.477911  1.000000e-03
17    17.0   0.072388    4.352144   0.074381    4.366978  1.000000e-03
18    18.0   0.083984    4.276812   0.090199    4.280895  1.000000e-03
19    19.0   0.086914    4.237206   0.103111    4.203117  1.000000e-03
20    20.0   0.109131    4.153062   0.114713    4.148901  1.000000e-03
21    21.0   0.112671    4.079197   0.092803    4.240470  1.000000e-03
22    22.0   0.127075    4.019706   0.121410    4.055933  1.000000e-03
23    23.0   0.140137    3.935096   0.136608    3.982577  1.000000e-03
24    24.0   0.154175    3.850219   0.131851    4.041339  1.000000e-03
25    25.0   0.170166    3.797841   0.177749    3.807135  1.000000e-03
26    26.0   0.188232    3.711202   0.165254    3.815120  1.000000e-03
27    27.0   0.200806    3.640396   0.139554    3.977556  1.000000e-03
28    28.0   0.219238    3.571758   0.167774    3.892110  1.000000e-03
29    29.0   0.240845    3.500711   0.229149    3.576995  1.000000e-03
30    30.0   0.258179    3.435104   0.187010    3.840816  1.000000e-03
31    31.0   0.283447    3.362056   0.238845    3.453317  1.000000e-03
32    32.0   0.297241    3.292362   0.223607    3.572421  1.000000e-03
33    33.0   0.325073    3.218918   0.236147    3.589543  1.000000e-03
34    34.0   0.346558    3.137047   0.219043    3.649264  1.000000e-03
35    35.0   0.352783    3.083650   0.284510    3.384747  1.000000e-03
36    36.0   0.363037    3.049824   0.321619    3.198259  1.000000e-03
37    37.0   0.378296    2.985962   0.370835    3.050759  1.000000e-03
38    38.0   0.403320    2.929250   0.286960    3.267446  1.000000e-03
39    39.0   0.420898    2.855427   0.343127    3.152829  1.000000e-03
40    40.0   0.436401    2.807343   0.388335    2.991586  1.000000e-03
41    41.0   0.450562    2.784342   0.345522    3.136083  1.000000e-03
42    42.0   0.463745    2.730899   0.371222    3.021399  1.000000e-03
43    43.0   0.475586    2.701712   0.291276    3.318822  1.000000e-03
44    44.0   0.491821    2.653185   0.348910    3.115898  1.000000e-03
45    45.0   0.500854    2.598226   0.331183    3.203277  1.000000e-03
46    46.0   0.509033    2.601811   0.348018    3.149274  1.000000e-03
47    47.0   0.638184    2.254385   0.658035    2.158843  1.000000e-04
48    48.0   0.694336    2.090044   0.667762    2.110736  1.000000e-04
49    49.0   0.703125    2.047936   0.685068    2.065215  1.000000e-04
50    50.0   0.730347    1.991509   0.689781    2.052742  1.000000e-04
51    51.0   0.733887    1.968813   0.691656    2.060292  1.000000e-04
52    52.0   0.744507    1.939433   0.703685    2.019556  1.000000e-04
53    53.0   0.754883    1.911730   0.704925    2.005514  1.000000e-04
54    54.0   0.768066    1.880351   0.705545    1.998815  1.000000e-04
55    55.0   0.771851    1.857242   0.707390    1.985384  1.000000e-04
56    56.0   0.778687    1.837762   0.710327    1.968612  1.000000e-04
57    57.0   0.783691    1.817731   0.708699    1.959069  1.000000e-04
58    58.0   0.789795    1.799739   0.720247    1.934237  1.000000e-04
59    59.0   0.795532    1.788802   0.717147    1.937327  1.000000e-04
60    60.0   0.800781    1.759065   0.724835    1.916563  1.000000e-04
61    61.0   0.812134    1.733383   0.718566    1.920840  1.000000e-04
62    62.0   0.813721    1.724146   0.721929    1.910184  1.000000e-04
63    63.0   0.821167    1.708489   0.718759    1.918007  1.000000e-04
64    64.0   0.826050    1.687047   0.731586    1.886092  1.000000e-04
65    65.0   0.837769    1.667411   0.734369    1.874588  1.000000e-04
66    66.0   0.839844    1.652527   0.733005    1.862760  1.000000e-04
67    67.0   0.839966    1.642148   0.732454    1.858061  1.000000e-04
68    68.0   0.858521    1.620039   0.739329    1.841300  1.000000e-04
69    69.0   0.847168    1.610841   0.739369    1.846796  1.000000e-04
70    70.0   0.857300    1.593195   0.730386    1.866698  1.000000e-04
71    71.0   0.860352    1.579778   0.741437    1.846331  1.000000e-04
72    72.0   0.860962    1.570437   0.742499    1.830493  1.000000e-04
73    73.0   0.870483    1.552313   0.742553    1.828728  1.000000e-04
74    74.0   0.870850    1.546226   0.739493    1.819090  1.000000e-04
75    75.0   0.879272    1.525010   0.745654    1.822351  1.000000e-04
76    76.0   0.881836    1.515993   0.742499    1.806005  1.000000e-04
77    77.0   0.879272    1.510270   0.731114    1.845000  1.000000e-04
78    78.0   0.881958    1.501465   0.741949    1.819566  1.000000e-04
79    79.0   0.893921    1.476037   0.740430    1.818272  1.000000e-04
80    80.0   0.893188    1.471339   0.747196    1.796925  1.000000e-04
81    81.0   0.900146    1.448574   0.736958    1.829455  1.000000e-04
82    82.0   0.900391    1.447127   0.743724    1.800440  1.000000e-04
83    83.0   0.901978    1.439823   0.752102    1.785141  1.000000e-04
84    84.0   0.906006    1.421592   0.748382    1.797423  1.000000e-04
85    85.0   0.911621    1.413151   0.754295    1.769297  1.000000e-04
86    86.0   0.912842    1.410531   0.747003    1.802379  1.000000e-04
87    87.0   0.915161    1.403629   0.752265    1.773162  1.000000e-04
88    88.0   0.914062    1.389740   0.731199    1.828605  1.000000e-04
89    89.0   0.917114    1.388936   0.748739    1.789836  1.000000e-04
90    90.0   0.922363    1.371464   0.747181    1.787966  1.000000e-04
91    91.0   0.924438    1.367007   0.753838    1.771201  1.000000e-04
92    92.0   0.942139    1.318675   0.770703    1.716962  1.000000e-05
93    93.0   0.943970    1.308573   0.772563    1.710354  1.000000e-05
94    94.0   0.944336    1.304369   0.773059    1.712157  1.000000e-05
95    95.0   0.949585    1.295115   0.776532    1.700857  1.000000e-05
96    96.0   0.948608    1.289598   0.774300    1.705995  1.000000e-05
97    97.0   0.948120    1.285201   0.773927    1.704070  1.000000e-05
98    98.0   0.953735    1.287595   0.774106    1.706800  1.000000e-05
99    99.0   0.953369    1.284461   0.775346    1.705046  1.000000e-05
100  100.0   0.958130    1.278297   0.774671    1.702862  1.000000e-05
101  101.0   0.955200    1.279292   0.775485    1.699184  1.000000e-05
102  102.0   0.952026    1.281015   0.776532    1.700897  1.000000e-06
103  103.0   0.954956    1.276487   0.774850    1.706672  1.000000e-06
104  104.0   0.951904    1.279462   0.777276    1.700300  1.000000e-06
105  105.0   0.956787    1.270009   0.777028    1.700224  1.000000e-06
106  106.0   0.953857    1.274570   0.777261    1.697951  1.000000e-06
107  107.0   0.955322    1.269526   0.774051    1.700375  1.000000e-06
108  108.0   0.954346    1.272791   0.778709    1.691787  1.000000e-06
109  109.0   0.953125    1.272198   0.777702    1.695547  1.000000e-06
110  110.0   0.953003    1.279547   0.777717    1.693978  1.000000e-06
111  111.0   0.955078    1.270845   0.777082    1.698509  1.000000e-06
112  112.0   0.957031    1.275275   0.775842    1.705368  1.000000e-06
113  113.0   0.955322    1.269688   0.776958    1.696417  1.000000e-06
114  114.0   0.957031    1.273839   0.777276    1.692847  1.000000e-06
115  115.0   0.958740    1.273319   0.776641    1.702372  1.000000e-07
116  116.0   0.955200    1.273596   0.776834    1.698344  1.000000e-07
117  117.0   0.953857    1.276261   0.778198    1.701291  1.000000e-07
118  118.0   0.960815    1.269803   0.778198    1.704097  1.000000e-07
119  119.0   0.957397    1.272064   0.776214    1.698446  1.000000e-07
120  120.0   0.957153    1.270045   0.776710    1.696287  1.000000e-07
121  121.0   0.954468    1.273500   0.776834    1.699179  1.000000e-07
122  122.0   0.953979    1.272574   0.777950    1.696103  1.000000e-07
123  123.0   0.955444    1.268284   0.776710    1.699206  1.000000e-07

EXPERIMENT: C-44
================

NAME: ghost_20200921174351

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.25,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.005127    5.356949   0.007937    5.351676  1.000000e-03
1      1.0   0.011108    5.244282   0.012649    5.182944  1.000000e-03
2      2.0   0.014404    5.188705   0.017733    5.134123  1.000000e-03
3      3.0   0.016479    5.128654   0.013889    5.219747  1.000000e-03
4      4.0   0.021606    5.082376   0.027654    5.045203  1.000000e-03
5      5.0   0.024048    5.042986   0.019841    5.064372  1.000000e-03
6      6.0   0.027588    4.981053   0.032862    4.987364  1.000000e-03
7      7.0   0.032593    4.926059   0.036652    4.926084  1.000000e-03
8      8.0   0.038452    4.855798   0.035908    4.894122  1.000000e-03
9      9.0   0.046143    4.760468   0.044108    4.915559  1.000000e-03
10    10.0   0.054321    4.657249   0.049851    4.723231  1.000000e-03
11    11.0   0.061768    4.549340   0.047332    4.668139  1.000000e-03
12    12.0   0.070190    4.448745   0.081488    4.394295  1.000000e-03
13    13.0   0.083252    4.337141   0.074744    4.422879  1.000000e-03
14    14.0   0.096436    4.237048   0.092322    4.320605  1.000000e-03
15    15.0   0.113403    4.134489   0.115343    4.117797  1.000000e-03
16    16.0   0.127686    4.047126   0.134570    4.000481  1.000000e-03
17    17.0   0.153198    3.910048   0.124356    4.051316  1.000000e-03
18    18.0   0.165039    3.829080   0.124386    4.162343  1.000000e-03
19    19.0   0.191284    3.727819   0.189853    3.762832  1.000000e-03
20    20.0   0.201538    3.651711   0.218956    3.648693  1.000000e-03
21    21.0   0.238892    3.528022   0.228831    3.598612  1.000000e-03
22    22.0   0.254028    3.440252   0.239093    3.488632  1.000000e-03
23    23.0   0.280029    3.362597   0.265329    3.420515  1.000000e-03
24    24.0   0.309570    3.256063   0.272188    3.368097  1.000000e-03
25    25.0   0.335083    3.131190   0.305888    3.226453  1.000000e-03
26    26.0   0.363403    3.049835   0.330502    3.200684  1.000000e-03
27    27.0   0.390259    2.980943   0.321138    3.201823  1.000000e-03
28    28.0   0.421631    2.869715   0.375901    3.010100  1.000000e-03
29    29.0   0.442871    2.794433   0.341512    3.129377  1.000000e-03
30    30.0   0.460938    2.728886   0.332480    3.142877  1.000000e-03
31    31.0   0.473267    2.680165   0.351605    3.108737  1.000000e-03
32    32.0   0.494507    2.606172   0.363479    3.043499  1.000000e-03
33    33.0   0.516235    2.552967   0.344576    3.122294  1.000000e-03
34    34.0   0.526489    2.490159   0.404369    2.942538  1.000000e-03
35    35.0   0.558105    2.430901   0.387546    2.989611  1.000000e-03
36    36.0   0.566040    2.394926   0.455508    2.776775  1.000000e-03
37    37.0   0.569092    2.369260   0.310930    3.265404  1.000000e-03
38    38.0   0.584595    2.315523   0.393489    2.962903  1.000000e-03
39    39.0   0.598022    2.295995   0.450572    2.781704  1.000000e-03
40    40.0   0.613647    2.248851   0.466164    2.709507  1.000000e-03
41    41.0   0.629028    2.206955   0.362560    3.076660  1.000000e-03
42    42.0   0.640137    2.166209   0.420901    2.893369  1.000000e-03
43    43.0   0.635376    2.173214   0.467065    2.736408  1.000000e-03
44    44.0   0.651367    2.126947   0.446078    2.835544  1.000000e-03
45    45.0   0.659546    2.097990   0.498128    2.638428  1.000000e-03
46    46.0   0.676514    2.061845   0.468578    2.711864  1.000000e-03
47    47.0   0.672974    2.062516   0.504213    2.597540  1.000000e-03
48    48.0   0.697876    2.009747   0.448446    2.834975  1.000000e-03
49    49.0   0.685059    2.015266   0.409595    2.968457  1.000000e-03
50    50.0   0.699829    1.987437   0.499253    2.663626  1.000000e-03
51    51.0   0.699341    1.983413   0.472479    2.738464  1.000000e-03
52    52.0   0.721069    1.914917   0.578524    2.349387  1.000000e-03
53    53.0   0.717163    1.924417   0.492136    2.688642  1.000000e-03
54    54.0   0.727173    1.889772   0.452650    2.783161  1.000000e-03
55    55.0   0.720703    1.901152   0.486329    2.641722  1.000000e-03
56    56.0   0.732056    1.882125   0.477225    2.705926  1.000000e-03
57    57.0   0.747192    1.841604   0.437397    2.794566  1.000000e-03
58    58.0   0.741333    1.858058   0.338339    3.329503  1.000000e-03
59    59.0   0.859619    1.514458   0.776849    1.708992  1.000000e-04
60    60.0   0.914307    1.379045   0.788839    1.675302  1.000000e-04
61    61.0   0.921143    1.341107   0.793621    1.666629  1.000000e-04
62    62.0   0.932251    1.312383   0.794791    1.643454  1.000000e-04
63    63.0   0.936401    1.292817   0.798581    1.643624  1.000000e-04
64    64.0   0.945557    1.273831   0.801185    1.631818  1.000000e-04
65    65.0   0.946167    1.263740   0.805952    1.618087  1.000000e-04
66    66.0   0.953491    1.240156   0.804146    1.615895  1.000000e-04
67    67.0   0.953735    1.236453   0.809424    1.612038  1.000000e-04
68    68.0   0.957397    1.224980   0.804821    1.623067  1.000000e-04
69    69.0   0.959961    1.212045   0.804603    1.630916  1.000000e-04
70    70.0   0.962402    1.203846   0.803913    1.623171  1.000000e-04
71    71.0   0.963135    1.204399   0.808641    1.607918  1.000000e-04
72    72.0   0.967285    1.182649   0.816686    1.592244  1.000000e-04
73    73.0   0.966553    1.183769   0.806061    1.607898  1.000000e-04
74    74.0   0.971069    1.172021   0.802728    1.619722  1.000000e-04
75    75.0   0.970215    1.166237   0.804340    1.625525  1.000000e-04
76    76.0   0.973267    1.151627   0.809191    1.610221  1.000000e-04
77    77.0   0.971558    1.151364   0.805689    1.607859  1.000000e-04
78    78.0   0.973877    1.147059   0.804588    1.611884  1.000000e-04
79    79.0   0.980225    1.122824   0.820778    1.575989  1.000000e-05
80    80.0   0.980713    1.114221   0.821894    1.568070  1.000000e-05
81    81.0   0.982910    1.108978   0.821344    1.568347  1.000000e-05
82    82.0   0.982178    1.108908   0.823824    1.565718  1.000000e-05
83    83.0   0.983154    1.105895   0.821894    1.569277  1.000000e-05
84    84.0   0.983154    1.102764   0.824142    1.565035  1.000000e-05
85    85.0   0.983154    1.102471   0.823080    1.571522  1.000000e-05
86    86.0   0.983887    1.103315   0.824390    1.561501  1.000000e-05
87    87.0   0.986694    1.097868   0.822832    1.564588  1.000000e-05
88    88.0   0.985107    1.096940   0.824196    1.568856  1.000000e-05
89    89.0   0.984741    1.100413   0.822460    1.570433  1.000000e-05
90    90.0   0.984131    1.097996   0.823010    1.571119  1.000000e-05
91    91.0   0.985474    1.094558   0.822212    1.570425  1.000000e-05
92    92.0   0.984741    1.093980   0.823824    1.573426  1.000000e-05
93    93.0   0.984497    1.092247   0.821909    1.572351  1.000000e-06
94    94.0   0.985840    1.091768   0.824514    1.570686  1.000000e-06
95    95.0   0.985718    1.091731   0.823328    1.572309  1.000000e-06
96    96.0   0.986572    1.092062   0.824390    1.573502  1.000000e-06
97    97.0   0.986328    1.093141   0.823522    1.568703  1.000000e-06
98    98.0   0.987915    1.087131   0.823576    1.568167  1.000000e-06
99    99.0   0.984619    1.089715   0.822569    1.569879  1.000000e-07
100  100.0   0.986694    1.090543   0.823576    1.564529  1.000000e-07
101  101.0   0.987427    1.089436   0.823026    1.571142  1.000000e-07

EXPERIMENT: C-45
================

NAME: ghost_20200921220941

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}, 'RandomResizedCrop': {'size': [227, "
                        "227], 'scale': [0.6, 1.0], 'ratio': [0.8, 1.2]}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.004883    5.351474   0.008433    5.452629  1.000000e-03
1      1.0   0.007690    5.302966   0.010541    5.307467  1.000000e-03
2      2.0   0.010254    5.243384   0.012966    5.322049  1.000000e-03
3      3.0   0.014160    5.149616   0.018423    5.134787  1.000000e-03
4      4.0   0.020874    5.081862   0.021577    5.106394  1.000000e-03
5      5.0   0.023438    5.029363   0.026855    5.043498  1.000000e-03
6      6.0   0.031738    4.942827   0.032118    4.989408  1.000000e-03
7      7.0   0.034302    4.857234   0.038760    4.848297  1.000000e-03
8      8.0   0.042236    4.745293   0.045154    4.762002  1.000000e-03
9      9.0   0.052124    4.652424   0.023189    4.982187  1.000000e-03
10    10.0   0.055298    4.572597   0.052098    4.670187  1.000000e-03
11    11.0   0.065063    4.494003   0.065422    4.522479  1.000000e-03
12    12.0   0.071167    4.407763   0.064871    4.513372  1.000000e-03
13    13.0   0.082275    4.374393   0.071429    4.447402  1.000000e-03
14    14.0   0.087646    4.300220   0.057912    4.566107  1.000000e-03
15    15.0   0.097168    4.210295   0.097624    4.253712  1.000000e-03
16    16.0   0.109009    4.140861   0.121310    4.089175  1.000000e-03
17    17.0   0.123291    4.056600   0.124410    4.086017  1.000000e-03
18    18.0   0.140503    3.972633   0.095105    4.311167  1.000000e-03
19    19.0   0.159058    3.880664   0.149668    3.941444  1.000000e-03
20    20.0   0.178223    3.776171   0.155784    3.934724  1.000000e-03
21    21.0   0.198608    3.703315   0.170913    3.805720  1.000000e-03
22    22.0   0.226807    3.610826   0.165309    3.877724  1.000000e-03
23    23.0   0.249512    3.496072   0.203225    3.668015  1.000000e-03
24    24.0   0.273071    3.392673   0.261197    3.437290  1.000000e-03
25    25.0   0.293457    3.320491   0.259639    3.486192  1.000000e-03
26    26.0   0.320801    3.204982   0.228964    3.628564  1.000000e-03
27    27.0   0.353516    3.110968   0.253106    3.517839  1.000000e-03
28    28.0   0.376831    3.033193   0.252749    3.525075  1.000000e-03
29    29.0   0.398315    2.952755   0.311420    3.273841  1.000000e-03
30    30.0   0.424316    2.876017   0.329967    3.170971  1.000000e-03
31    31.0   0.444092    2.796448   0.356877    3.081688  1.000000e-03
32    32.0   0.459595    2.737660   0.417765    2.923254  1.000000e-03
33    33.0   0.493896    2.676663   0.322457    3.251906  1.000000e-03
34    34.0   0.496582    2.633359   0.421787    2.884343  1.000000e-03
35    35.0   0.531006    2.537212   0.449874    2.765606  1.000000e-03
36    36.0   0.542358    2.502046   0.375602    3.108729  1.000000e-03
37    37.0   0.557129    2.455956   0.466615    2.702033  1.000000e-03
38    38.0   0.564819    2.405510   0.359690    3.075155  1.000000e-03
39    39.0   0.577026    2.364310   0.304630    3.358034  1.000000e-03
40    40.0   0.595947    2.323752   0.424948    2.890284  1.000000e-03
41    41.0   0.607422    2.288283   0.402472    2.972337  1.000000e-03
42    42.0   0.617432    2.254805   0.399442    2.977199  1.000000e-03
43    43.0   0.628784    2.217172   0.412447    2.936968  1.000000e-03
44    44.0   0.756348    1.889671   0.692339    2.020488  1.000000e-04
45    45.0   0.809814    1.726010   0.708088    1.978433  1.000000e-04
46    46.0   0.826172    1.671792   0.716358    1.951577  1.000000e-04
47    47.0   0.834473    1.642522   0.725534    1.922606  1.000000e-04
48    48.0   0.846802    1.602634   0.728208    1.912944  1.000000e-04
49    49.0   0.852661    1.585031   0.733238    1.900018  1.000000e-04
50    50.0   0.861084    1.566184   0.740500    1.868266  1.000000e-04
51    51.0   0.866089    1.544469   0.735400    1.874207  1.000000e-04
52    52.0   0.864624    1.533634   0.742608    1.856577  1.000000e-04
53    53.0   0.880859    1.513350   0.731874    1.881557  1.000000e-04
54    54.0   0.884155    1.497948   0.730758    1.874882  1.000000e-04
55    55.0   0.887207    1.478287   0.738253    1.860994  1.000000e-04
56    56.0   0.894409    1.467516   0.744964    1.842965  1.000000e-04
57    57.0   0.902588    1.444305   0.750916    1.827244  1.000000e-04
58    58.0   0.901001    1.436270   0.749304    1.831441  1.000000e-04
59    59.0   0.904053    1.417642   0.749483    1.824611  1.000000e-04
60    60.0   0.910522    1.408408   0.751025    1.839311  1.000000e-04
61    61.0   0.915771    1.395683   0.758163    1.804951  1.000000e-04
62    62.0   0.915039    1.391345   0.739562    1.847822  1.000000e-04
63    63.0   0.921387    1.375982   0.760976    1.801893  1.000000e-04
64    64.0   0.922485    1.365977   0.754334    1.797797  1.000000e-04
65    65.0   0.921387    1.360473   0.755559    1.795764  1.000000e-04
66    66.0   0.922485    1.345970   0.756442    1.790153  1.000000e-04
67    67.0   0.927246    1.334165   0.759597    1.782745  1.000000e-04
68    68.0   0.930786    1.322438   0.765425    1.768492  1.000000e-04
69    69.0   0.937744    1.316755   0.754404    1.798312  1.000000e-04
70    70.0   0.934937    1.310255   0.755257    1.781630  1.000000e-04
71    71.0   0.939331    1.287995   0.764488    1.773204  1.000000e-04
72    72.0   0.941650    1.290850   0.769766    1.747671  1.000000e-04
73    73.0   0.943237    1.279523   0.760976    1.779231  1.000000e-04
74    74.0   0.945435    1.273074   0.764860    1.770582  1.000000e-04
75    75.0   0.945679    1.271171   0.767797    1.756249  1.000000e-04
76    76.0   0.946045    1.261529   0.757117    1.796355  1.000000e-04
77    77.0   0.947144    1.258912   0.767975    1.765194  1.000000e-04
78    78.0   0.950317    1.252481   0.760961    1.784368  1.000000e-04
79    79.0   0.961792    1.212425   0.782414    1.719560  1.000000e-05
80    80.0   0.967163    1.199955   0.781422    1.712899  1.000000e-05
81    81.0   0.965698    1.201270   0.780237    1.715824  1.000000e-05
82    82.0   0.968384    1.190510   0.779245    1.714466  1.000000e-05
83    83.0   0.967651    1.189825   0.781050    1.711569  1.000000e-05
84    84.0   0.964478    1.192210   0.779617    1.717615  1.000000e-05
85    85.0   0.967529    1.188869   0.780361    1.707447  1.000000e-05
86    86.0   0.968628    1.185704   0.781105    1.716454  1.000000e-05
87    87.0   0.969116    1.187292   0.781849    1.710766  1.000000e-05
88    88.0   0.967651    1.185595   0.780926    1.711045  1.000000e-05
89    89.0   0.973022    1.177595   0.781670    1.708865  1.000000e-05
90    90.0   0.968384    1.181653   0.782221    1.704523  1.000000e-05
91    91.0   0.968506    1.179298   0.781298    1.713563  1.000000e-05
92    92.0   0.967896    1.181426   0.784453    1.706974  1.000000e-05
93    93.0   0.966797    1.180909   0.782965    1.703702  1.000000e-05
94    94.0   0.969482    1.175795   0.781368    1.710532  1.000000e-05
95    95.0   0.973022    1.169727   0.783158    1.703997  1.000000e-05
96    96.0   0.973755    1.166674   0.780733    1.707731  1.000000e-05
97    97.0   0.970825    1.169521   0.781477    1.707788  1.000000e-06
98    98.0   0.972900    1.168229   0.780485    1.708956  1.000000e-06
99    99.0   0.969849    1.172059   0.785019    1.699130  1.000000e-06
100  100.0   0.972534    1.168150   0.787320    1.701803  1.000000e-06
101  101.0   0.969849    1.171381   0.783903    1.703157  1.000000e-06
102  102.0   0.970459    1.168945   0.783600    1.704592  1.000000e-06
103  103.0   0.975586    1.162503   0.783406    1.700382  1.000000e-06
104  104.0   0.972656    1.170159   0.783903    1.708355  1.000000e-06
105  105.0   0.972168    1.169851   0.783406    1.703770  1.000000e-06
106  106.0   0.970459    1.171886   0.784027    1.702834  1.000000e-07
107  107.0   0.973511    1.161909   0.784027    1.703233  1.000000e-07
108  108.0   0.970581    1.170292   0.780926    1.714891  1.000000e-07
109  109.0   0.971802    1.167006   0.784081    1.705579  1.000000e-07
110  110.0   0.973877    1.166141   0.784647    1.700760  1.000000e-07
111  111.0   0.972656    1.166142   0.784825    1.704665  1.000000e-07
112  112.0   0.970703    1.175682   0.781794    1.709823  1.000000e-07
113  113.0   0.970947    1.171399   0.783779    1.705286  1.000000e-07
114  114.0   0.970337    1.166100   0.784647    1.703596  1.000000e-07

EXPERIMENT: C-46
================

NAME: ghost_20200921221221

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}, 'RandomRotation': {'degrees': 15}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.006226    5.343657   0.010417    5.277884  1.000000e-03
1      1.0   0.009644    5.255726   0.011974    5.201680  1.000000e-03
2      2.0   0.012939    5.194641   0.012525    5.186673  1.000000e-03
3      3.0   0.016602    5.140917   0.015129    5.164396  1.000000e-03
4      4.0   0.018188    5.097948   0.020848    5.097416  1.000000e-03
5      5.0   0.023438    5.040550   0.019469    5.097516  1.000000e-03
6      6.0   0.027588    5.011893   0.025422    5.028774  1.000000e-03
7      7.0   0.032104    4.946479   0.026290    4.988914  1.000000e-03
8      8.0   0.032593    4.880489   0.037946    4.884948  1.000000e-03
9      9.0   0.039673    4.814547   0.020213    5.093940  1.000000e-03
10    10.0   0.046631    4.733789   0.034931    4.850098  1.000000e-03
11    11.0   0.050659    4.660756   0.052098    4.667944  1.000000e-03
12    12.0   0.057983    4.574207   0.048061    4.662421  1.000000e-03
13    13.0   0.062500    4.506965   0.057168    4.602120  1.000000e-03
14    14.0   0.075562    4.431111   0.045898    4.691411  1.000000e-03
15    15.0   0.083130    4.353148   0.071211    4.569995  1.000000e-03
16    16.0   0.092651    4.266698   0.094331    4.235179  1.000000e-03
17    17.0   0.110962    4.179847   0.077257    4.394381  1.000000e-03
18    18.0   0.123291    4.095085   0.078760    4.405948  1.000000e-03
19    19.0   0.130005    4.036530   0.133284    4.078061  1.000000e-03
20    20.0   0.154663    3.927306   0.150041    3.981837  1.000000e-03
21    21.0   0.172485    3.841982   0.165805    3.872514  1.000000e-03
22    22.0   0.182007    3.771108   0.190730    3.755308  1.000000e-03
23    23.0   0.206177    3.687929   0.188453    3.762379  1.000000e-03
24    24.0   0.228882    3.598600   0.217834    3.658319  1.000000e-03
25    25.0   0.255371    3.507442   0.203573    3.721940  1.000000e-03
26    26.0   0.272583    3.424432   0.218469    3.646179  1.000000e-03
27    27.0   0.295532    3.319309   0.230150    3.597423  1.000000e-03
28    28.0   0.331177    3.214713   0.284456    3.360992  1.000000e-03
29    29.0   0.355103    3.137169   0.340027    3.213781  1.000000e-03
30    30.0   0.369873    3.060201   0.282318    3.357677  1.000000e-03
31    31.0   0.417603    2.920785   0.247363    3.557286  1.000000e-03
32    32.0   0.428955    2.874264   0.359768    3.098451  1.000000e-03
33    33.0   0.445801    2.809263   0.359466    3.107598  1.000000e-03
34    34.0   0.475220    2.715985   0.389808    2.982340  1.000000e-03
35    35.0   0.486206    2.674526   0.376555    3.017681  1.000000e-03
36    36.0   0.514893    2.603684   0.443713    2.811463  1.000000e-03
37    37.0   0.536987    2.531398   0.405200    2.928050  1.000000e-03
38    38.0   0.540039    2.501892   0.388305    2.986890  1.000000e-03
39    39.0   0.568726    2.424240   0.394118    2.993182  1.000000e-03
40    40.0   0.572021    2.408797   0.406425    2.975263  1.000000e-03
41    41.0   0.588867    2.360720   0.387398    3.004438  1.000000e-03
42    42.0   0.596191    2.335082   0.482597    2.673834  1.000000e-03
43    43.0   0.614258    2.276820   0.413137    2.942611  1.000000e-03
44    44.0   0.627197    2.230578   0.429025    2.850581  1.000000e-03
45    45.0   0.630737    2.215201   0.370061    3.054606  1.000000e-03
46    46.0   0.633301    2.203613   0.469095    2.729021  1.000000e-03
47    47.0   0.651367    2.154963   0.454664    2.785781  1.000000e-03
48    48.0   0.677368    2.085302   0.441852    2.832546  1.000000e-03
49    49.0   0.795044    1.769988   0.715574    1.940145  1.000000e-04
50    50.0   0.844604    1.610925   0.735609    1.892244  1.000000e-04
51    51.0   0.863525    1.571345   0.738089    1.869430  1.000000e-04
52    52.0   0.872437    1.536857   0.744166    1.850595  1.000000e-04
53    53.0   0.881836    1.507692   0.748119    1.835108  1.000000e-04
54    54.0   0.891479    1.476673   0.745708    1.844079  1.000000e-04
55    55.0   0.897339    1.460796   0.751288    1.826982  1.000000e-04
56    56.0   0.901733    1.444077   0.760891    1.794176  1.000000e-04
57    57.0   0.907227    1.427664   0.758302    1.804945  1.000000e-04
58    58.0   0.910767    1.411050   0.763511    1.781081  1.000000e-04
59    59.0   0.916260    1.393256   0.759969    1.793888  1.000000e-04
60    60.0   0.915771    1.388965   0.767921    1.773189  1.000000e-04
61    61.0   0.928711    1.360815   0.759651    1.786086  1.000000e-04
62    62.0   0.929443    1.355502   0.759349    1.796047  1.000000e-04
63    63.0   0.932983    1.337715   0.765123    1.783934  1.000000e-04
64    64.0   0.935669    1.327217   0.761264    1.780552  1.000000e-04
65    65.0   0.934937    1.320486   0.758783    1.797894  1.000000e-04
66    66.0   0.943604    1.307727   0.758481    1.801515  1.000000e-04
67    67.0   0.953979    1.273097   0.773362    1.743211  1.000000e-05
68    68.0   0.956055    1.262727   0.775098    1.740175  1.000000e-05
69    69.0   0.959839    1.256994   0.777097    1.740200  1.000000e-05
70    70.0   0.959106    1.253244   0.777028    1.738586  1.000000e-05
71    71.0   0.958984    1.250868   0.776214    1.737415  1.000000e-05
72    72.0   0.961304    1.243013   0.777082    1.735324  1.000000e-05
73    73.0   0.957397    1.247882   0.777950    1.737177  1.000000e-05
74    74.0   0.959839    1.243319   0.777702    1.740830  1.000000e-05
75    75.0   0.959351    1.242382   0.776036    1.733990  1.000000e-05
76    76.0   0.958618    1.242613   0.776834    1.738671  1.000000e-05
77    77.0   0.958618    1.238047   0.778570    1.732936  1.000000e-05
78    78.0   0.961914    1.237349   0.779438    1.732214  1.000000e-05
79    79.0   0.964233    1.234129   0.778198    1.732143  1.000000e-05
80    80.0   0.962036    1.234621   0.777578    1.734063  1.000000e-05
81    81.0   0.965454    1.226977   0.779562    1.730658  1.000000e-05
82    82.0   0.961670    1.232796   0.779190    1.730370  1.000000e-05
83    83.0   0.961914    1.229426   0.779438    1.736703  1.000000e-05
84    84.0   0.962524    1.238890   0.777082    1.733110  1.000000e-05
85    85.0   0.966797    1.226995   0.776710    1.733093  1.000000e-05
86    86.0   0.963501    1.229047   0.777578    1.732462  1.000000e-05
87    87.0   0.967285    1.222055   0.778818    1.725738  1.000000e-05
88    88.0   0.965454    1.222747   0.778764    1.728747  1.000000e-05
89    89.0   0.964600    1.219375   0.780554    1.725674  1.000000e-05
90    90.0   0.965942    1.218606   0.782538    1.724261  1.000000e-05
91    91.0   0.966675    1.220478   0.778818    1.732893  1.000000e-05
92    92.0   0.965942    1.218724   0.777950    1.734967  1.000000e-05
93    93.0   0.966309    1.215980   0.778144    1.727177  1.000000e-05
94    94.0   0.966553    1.216732   0.777524    1.727893  1.000000e-06
95    95.0   0.967529    1.211289   0.777097    1.725807  1.000000e-06
96    96.0   0.964111    1.212635   0.780693    1.723457  1.000000e-06
97    97.0   0.963989    1.218600   0.779384    1.726487  1.000000e-06
98    98.0   0.967651    1.207165   0.778640    1.732470  1.000000e-06
99    99.0   0.965698    1.207268   0.781740    1.720707  1.000000e-06
100  100.0   0.967773    1.213168   0.779190    1.731032  1.000000e-06
101  101.0   0.967529    1.209283   0.779329    1.728002  1.000000e-06
102  102.0   0.967651    1.209154   0.778640    1.730531  1.000000e-06
103  103.0   0.968262    1.212881   0.779686    1.724490  1.000000e-06
104  104.0   0.969238    1.210752   0.779384    1.726388  1.000000e-06
105  105.0   0.969360    1.202938   0.777400    1.728759  1.000000e-06
106  106.0   0.967651    1.213488   0.777578    1.726390  1.000000e-07
107  107.0   0.965576    1.215433   0.777841    1.721659  1.000000e-07
108  108.0   0.966675    1.212425   0.778833    1.730993  1.000000e-07
109  109.0   0.969971    1.208692   0.779686    1.726627  1.000000e-07
110  110.0   0.968506    1.210037   0.779438    1.728462  1.000000e-07
111  111.0   0.969604    1.206950   0.779438    1.721144  1.000000e-07
112  112.0   0.970337    1.208895   0.779756    1.724342  1.000000e-07
113  113.0   0.968262    1.208788   0.780802    1.728131  1.000000e-07
114  114.0   0.967896    1.211907   0.778020    1.731451  1.000000e-07

EXPERIMENT: C-47
================

NAME: ghost_20200922001403

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}, 'RandomPerspective': {'p': 0.5, "
                        "'distortion_scale': 0.2}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.005737    5.362200   0.010789    5.280180  1.000000e-03
1      1.0   0.008545    5.325890   0.008929    5.362748  1.000000e-03
2      2.0   0.011719    5.258878   0.008185    5.367203  1.000000e-03
3      3.0   0.012573    5.189949   0.013889    5.146123  1.000000e-03
4      4.0   0.014038    5.132665   0.013765    5.135124  1.000000e-03
5      5.0   0.016602    5.093195   0.017733    5.118009  1.000000e-03
6      6.0   0.023438    5.059266   0.017361    5.086966  1.000000e-03
7      7.0   0.024658    5.021218   0.020585    5.077801  1.000000e-03
8      8.0   0.027832    4.975088   0.024802    4.995037  1.000000e-03
9      9.0   0.031372    4.929129   0.024802    5.040186  1.000000e-03
10    10.0   0.034546    4.868562   0.025050    4.964438  1.000000e-03
11    11.0   0.037842    4.801193   0.035412    4.918257  1.000000e-03
12    12.0   0.042480    4.729501   0.035218    4.829087  1.000000e-03
13    13.0   0.047363    4.637529   0.037342    4.813537  1.000000e-03
14    14.0   0.059692    4.546982   0.047991    4.696231  1.000000e-03
15    15.0   0.071289    4.459669   0.046945    4.656086  1.000000e-03
16    16.0   0.075317    4.375569   0.056796    4.532839  1.000000e-03
17    17.0   0.090088    4.284770   0.101066    4.243923  1.000000e-03
18    18.0   0.094849    4.202513   0.111843    4.194308  1.000000e-03
19    19.0   0.113770    4.112183   0.125883    4.116628  1.000000e-03
20    20.0   0.133179    4.006293   0.141420    3.968853  1.000000e-03
21    21.0   0.151978    3.907652   0.104663    4.249607  1.000000e-03
22    22.0   0.163940    3.832145   0.154087    3.893992  1.000000e-03
23    23.0   0.184326    3.737894   0.188389    3.730116  1.000000e-03
24    24.0   0.205322    3.651383   0.132090    4.076508  1.000000e-03
25    25.0   0.234985    3.576451   0.190885    3.704658  1.000000e-03
26    26.0   0.240967    3.477924   0.222981    3.620605  1.000000e-03
27    27.0   0.272949    3.404131   0.203137    3.675569  1.000000e-03
28    28.0   0.298462    3.307446   0.190794    3.693803  1.000000e-03
29    29.0   0.306519    3.244576   0.234520    3.621804  1.000000e-03
30    30.0   0.336792    3.151383   0.303538    3.292009  1.000000e-03
31    31.0   0.369629    3.041475   0.222322    3.637285  1.000000e-03
32    32.0   0.384644    2.994487   0.248085    3.512448  1.000000e-03
33    33.0   0.397949    2.940413   0.246752    3.540086  1.000000e-03
34    34.0   0.425415    2.856231   0.332160    3.141040  1.000000e-03
35    35.0   0.457764    2.772147   0.293484    3.312499  1.000000e-03
36    36.0   0.466309    2.725199   0.349333    3.072255  1.000000e-03
37    37.0   0.487061    2.674016   0.359517    3.039158  1.000000e-03
38    38.0   0.501465    2.627774   0.347885    3.108814  1.000000e-03
39    39.0   0.509766    2.569677   0.407381    2.949394  1.000000e-03
40    40.0   0.524170    2.531430   0.299615    3.376958  1.000000e-03
41    41.0   0.548096    2.470128   0.417036    2.902146  1.000000e-03
42    42.0   0.546997    2.453052   0.410666    2.940579  1.000000e-03
43    43.0   0.564331    2.402506   0.417571    2.872517  1.000000e-03
44    44.0   0.578369    2.353882   0.387017    2.997200  1.000000e-03
45    45.0   0.596924    2.311413   0.464609    2.699667  1.000000e-03
46    46.0   0.597534    2.296045   0.434497    2.835204  1.000000e-03
47    47.0   0.612061    2.257632   0.327097    3.302963  1.000000e-03
48    48.0   0.620117    2.244190   0.379476    3.022738  1.000000e-03
49    49.0   0.627930    2.199733   0.392860    3.025800  1.000000e-03
50    50.0   0.641724    2.173590   0.469098    2.727993  1.000000e-03
51    51.0   0.632935    2.180632   0.228404    3.967733  1.000000e-03
52    52.0   0.778564    1.800247   0.731532    1.882552  1.000000e-04
53    53.0   0.829834    1.660576   0.744553    1.851128  1.000000e-04
54    54.0   0.844116    1.613719   0.746195    1.834781  1.000000e-04
55    55.0   0.855957    1.577722   0.754489    1.814370  1.000000e-04
56    56.0   0.863770    1.549165   0.754613    1.809052  1.000000e-04
57    57.0   0.870483    1.532924   0.759325    1.781726  1.000000e-04
58    58.0   0.876953    1.505558   0.762316    1.767779  1.000000e-04
59    59.0   0.882812    1.487978   0.763735    1.777340  1.000000e-04
60    60.0   0.887085    1.471127   0.766904    1.763171  1.000000e-04
61    61.0   0.895874    1.453405   0.775461    1.747497  1.000000e-04
62    62.0   0.896484    1.439732   0.771299    1.742067  1.000000e-04
63    63.0   0.899170    1.435041   0.777623    1.732208  1.000000e-04
64    64.0   0.904175    1.417062   0.772787    1.743259  1.000000e-04
65    65.0   0.909424    1.399645   0.774826    1.743835  1.000000e-04
66    66.0   0.916504    1.381736   0.775996    1.726799  1.000000e-04
67    67.0   0.916138    1.375404   0.777623    1.725839  1.000000e-04
68    68.0   0.922363    1.365015   0.778794    1.713214  1.000000e-04
69    69.0   0.921265    1.357568   0.778670    1.710753  1.000000e-04
70    70.0   0.926025    1.340405   0.779677    1.711527  1.000000e-04
71    71.0   0.931885    1.329402   0.783769    1.697521  1.000000e-04
72    72.0   0.932617    1.320471   0.774756    1.719475  1.000000e-04
73    73.0   0.937134    1.308839   0.779042    1.702221  1.000000e-04
74    74.0   0.937988    1.301980   0.783963    1.705356  1.000000e-04
75    75.0   0.941284    1.287301   0.785699    1.678601  1.000000e-04
76    76.0   0.941406    1.279762   0.779593    1.698506  1.000000e-04
77    77.0   0.943359    1.276298   0.782142    1.697784  1.000000e-04
78    78.0   0.944946    1.271132   0.782599    1.693813  1.000000e-04
79    79.0   0.947632    1.257229   0.776934    1.702834  1.000000e-04
80    80.0   0.949951    1.250076   0.778561    1.701446  1.000000e-04
81    81.0   0.951050    1.247772   0.779444    1.702646  1.000000e-04
82    82.0   0.961548    1.208752   0.796666    1.643727  1.000000e-05
83    83.0   0.969360    1.191039   0.797465    1.638057  1.000000e-05
84    84.0   0.968140    1.187586   0.796557    1.645450  1.000000e-05
85    85.0   0.970337    1.185767   0.795937    1.637041  1.000000e-05
86    86.0   0.970459    1.179122   0.798650    1.637801  1.000000e-05
87    87.0   0.968628    1.185417   0.798581    1.636659  1.000000e-05
88    88.0   0.969238    1.183964   0.799340    1.639570  1.000000e-05
89    89.0   0.968018    1.182881   0.798774    1.638045  1.000000e-05
90    90.0   0.971436    1.179597   0.798844    1.639421  1.000000e-06
91    91.0   0.974243    1.172637   0.800139    1.636733  1.000000e-06
92    92.0   0.970581    1.175868   0.799216    1.633476  1.000000e-06
93    93.0   0.971191    1.169133   0.800635    1.629524  1.000000e-06
94    94.0   0.972778    1.175209   0.799092    1.635171  1.000000e-06
95    95.0   0.970215    1.175956   0.797534    1.640648  1.000000e-06
96    96.0   0.971802    1.176703   0.800402    1.636345  1.000000e-06
97    97.0   0.973389    1.173434   0.800263    1.635244  1.000000e-06
98    98.0   0.971558    1.172271   0.800580    1.636423  1.000000e-06
99    99.0   0.972168    1.172300   0.800263    1.631055  1.000000e-06
100  100.0   0.972290    1.170139   0.798030    1.636594  1.000000e-07
101  101.0   0.972046    1.173332   0.800828    1.634215  1.000000e-07
102  102.0   0.972046    1.175133   0.801642    1.632020  1.000000e-07
103  103.0   0.970703    1.173069   0.801448    1.638094  1.000000e-07
104  104.0   0.972900    1.172345   0.801448    1.638179  1.000000e-07
105  105.0   0.973145    1.169565   0.799727    1.637511  1.000000e-07
106  106.0   0.973389    1.170714   0.799022    1.636721  1.000000e-07
107  107.0   0.972412    1.174598   0.802207    1.634488  1.000000e-07
108  108.0   0.973389    1.169501   0.801022    1.631692  1.000000e-07

EXPERIMENT: C-48
================

NAME: ghost_20200922071738

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'True',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': "{'RandomErasing': {'p': 0.5, 'scale': [0.02, 0.25]}}",
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.007568    5.330752   0.011161    5.247981  1.000000e-03
1      1.0   0.011597    5.248102   0.009425    5.241703  1.000000e-03
2      2.0   0.011597    5.207046   0.014137    5.332410  1.000000e-03
3      3.0   0.014893    5.172600   0.013889    5.160540  1.000000e-03
4      4.0   0.013794    5.128403   0.017237    5.098832  1.000000e-03
5      5.0   0.016968    5.096977   0.023065    5.059735  1.000000e-03
6      6.0   0.023682    5.059804   0.026290    5.028971  1.000000e-03
7      7.0   0.025146    5.006720   0.020585    5.136107  1.000000e-03
8      8.0   0.027222    4.973548   0.028274    5.002733  1.000000e-03
9      9.0   0.032959    4.916602   0.026910    4.958527  1.000000e-03
10    10.0   0.035767    4.866978   0.025615    4.940426  1.000000e-03
11    11.0   0.036621    4.793551   0.039683    4.804915  1.000000e-03
12    12.0   0.042603    4.730274   0.040635    4.794694  1.000000e-03
13    13.0   0.052002    4.662973   0.054276    4.666901  1.000000e-03
14    14.0   0.057495    4.595795   0.050362    4.633644  1.000000e-03
15    15.0   0.059326    4.535046   0.062267    4.540769  1.000000e-03
16    16.0   0.073120    4.468621   0.076458    4.434809  1.000000e-03
17    17.0   0.078979    4.394153   0.056796    4.744755  1.000000e-03
18    18.0   0.087402    4.325759   0.079897    4.316320  1.000000e-03
19    19.0   0.102295    4.253152   0.070497    4.539895  1.000000e-03
20    20.0   0.110229    4.179976   0.127232    4.079291  1.000000e-03
21    21.0   0.116333    4.109727   0.128805    4.063443  1.000000e-03
22    22.0   0.133545    4.049564   0.135943    4.015100  1.000000e-03
23    23.0   0.150635    3.955476   0.169346    3.873979  1.000000e-03
24    24.0   0.164551    3.875413   0.126573    3.995082  1.000000e-03
25    25.0   0.179565    3.800306   0.162937    3.926970  1.000000e-03
26    26.0   0.203369    3.736285   0.176300    3.800391  1.000000e-03
27    27.0   0.212646    3.664582   0.197947    3.676969  1.000000e-03
28    28.0   0.226685    3.574340   0.186614    3.807223  1.000000e-03
29    29.0   0.247314    3.521837   0.232745    3.552667  1.000000e-03
30    30.0   0.262085    3.458027   0.165303    3.984353  1.000000e-03
31    31.0   0.283936    3.377479   0.284635    3.343937  1.000000e-03
32    32.0   0.300903    3.324059   0.279139    3.416195  1.000000e-03
33    33.0   0.315186    3.261276   0.258910    3.483129  1.000000e-03
34    34.0   0.333252    3.193948   0.290799    3.309664  1.000000e-03
35    35.0   0.354614    3.126772   0.300855    3.343478  1.000000e-03
36    36.0   0.368530    3.080806   0.322611    3.207383  1.000000e-03
37    37.0   0.387573    3.016037   0.303024    3.257178  1.000000e-03
38    38.0   0.398804    2.984829   0.315413    3.210597  1.000000e-03
39    39.0   0.407104    2.948099   0.215048    3.701751  1.000000e-03
40    40.0   0.426758    2.877409   0.354149    3.165600  1.000000e-03
41    41.0   0.442017    2.843907   0.308136    3.271491  1.000000e-03
42    42.0   0.453491    2.799660   0.344742    3.137805  1.000000e-03
43    43.0   0.466431    2.745622   0.296361    3.352728  1.000000e-03
44    44.0   0.471680    2.744211   0.283751    3.429034  1.000000e-03
45    45.0   0.487549    2.691737   0.357116    3.168409  1.000000e-03
46    46.0   0.497192    2.660329   0.386167    2.993102  1.000000e-03
47    47.0   0.502808    2.631873   0.375097    3.024540  1.000000e-03
48    48.0   0.511719    2.601815   0.457175    2.784464  1.000000e-03
49    49.0   0.526001    2.565607   0.363038    3.098813  1.000000e-03
50    50.0   0.538696    2.524372   0.415771    2.905319  1.000000e-03
51    51.0   0.536499    2.524275   0.441556    2.791843  1.000000e-03
52    52.0   0.548218    2.486549   0.364608    3.120477  1.000000e-03
53    53.0   0.549072    2.480404   0.211733    3.808348  1.000000e-03
54    54.0   0.568604    2.429698   0.408143    2.938982  1.000000e-03
55    55.0   0.683472    2.080480   0.707544    1.972480  1.000000e-04
56    56.0   0.745972    1.925159   0.725292    1.916515  1.000000e-04
57    57.0   0.764893    1.878831   0.731121    1.896357  1.000000e-04
58    58.0   0.776001    1.832922   0.742529    1.873711  1.000000e-04
59    59.0   0.789917    1.804757   0.746249    1.862465  1.000000e-04
60    60.0   0.793945    1.779201   0.742227    1.850649  1.000000e-04
61    61.0   0.802368    1.766235   0.748784    1.840277  1.000000e-04
62    62.0   0.806519    1.738936   0.744622    1.846953  1.000000e-04
63    63.0   0.813721    1.719381   0.750520    1.845844  1.000000e-04
64    64.0   0.816406    1.709749   0.753248    1.824239  1.000000e-04
65    65.0   0.824463    1.687928   0.757162    1.807214  1.000000e-04
66    66.0   0.827881    1.674293   0.760634    1.796952  1.000000e-04
67    67.0   0.833130    1.658183   0.757217    1.811518  1.000000e-04
68    68.0   0.830200    1.649787   0.764851    1.775615  1.000000e-04
69    69.0   0.838623    1.634833   0.765595    1.776413  1.000000e-04
70    70.0   0.840820    1.627393   0.765897    1.779239  1.000000e-04
71    71.0   0.850586    1.605414   0.767579    1.766987  1.000000e-04
72    72.0   0.843994    1.605266   0.763913    1.768577  1.000000e-04
73    73.0   0.853149    1.580597   0.761046    1.767920  1.000000e-04
74    74.0   0.858887    1.565459   0.767990    1.758413  1.000000e-04
75    75.0   0.857178    1.561419   0.769866    1.772065  1.000000e-04
76    76.0   0.861572    1.544729   0.774756    1.747733  1.000000e-04
77    77.0   0.867798    1.537608   0.770610    1.748948  1.000000e-04
78    78.0   0.860229    1.539074   0.766820    1.776419  1.000000e-04
79    79.0   0.871948    1.520525   0.771036    1.752658  1.000000e-04
80    80.0   0.873169    1.510755   0.766835    1.749402  1.000000e-04
81    81.0   0.879028    1.497186   0.774895    1.740144  1.000000e-04
82    82.0   0.877197    1.503303   0.773090    1.740135  1.000000e-04
83    83.0   0.875000    1.498374   0.776616    1.735167  1.000000e-04
84    84.0   0.888794    1.469949   0.768199    1.770774  1.000000e-04
85    85.0   0.887817    1.466697   0.763208    1.761522  1.000000e-04
86    86.0   0.893799    1.451967   0.761557    1.765522  1.000000e-04
87    87.0   0.893677    1.448378   0.781150    1.715811  1.000000e-04
88    88.0   0.892700    1.434803   0.784677    1.687939  1.000000e-04
89    89.0   0.889648    1.442272   0.771299    1.732068  1.000000e-04
90    90.0   0.896118    1.429915   0.780243    1.712954  1.000000e-04
91    91.0   0.895752    1.425591   0.771284    1.740587  1.000000e-04
92    92.0   0.904297    1.406742   0.776081    1.718154  1.000000e-04
93    93.0   0.903564    1.403189   0.770927    1.734262  1.000000e-04
94    94.0   0.895508    1.414995   0.770129    1.748456  1.000000e-04
95    95.0   0.917847    1.360964   0.797326    1.654036  1.000000e-05
96    96.0   0.914673    1.352142   0.798442    1.645486  1.000000e-05
97    97.0   0.921265    1.342936   0.800937    1.645871  1.000000e-05
98    98.0   0.927490    1.329705   0.800247    1.649186  1.000000e-05
99    99.0   0.921631    1.339509   0.801239    1.650458  1.000000e-05
100  100.0   0.926514    1.327896   0.800565    1.646728  1.000000e-05
101  101.0   0.923340    1.337018   0.800813    1.637157  1.000000e-05
102  102.0   0.929077    1.320058   0.802356    1.644325  1.000000e-05
103  103.0   0.929199    1.313841   0.802673    1.644778  1.000000e-05
104  104.0   0.922852    1.323182   0.804231    1.640151  1.000000e-05
105  105.0   0.925049    1.319353   0.802177    1.648484  1.000000e-05
106  106.0   0.928467    1.320310   0.801875    1.643711  1.000000e-05
107  107.0   0.929443    1.319623   0.803169    1.637126  1.000000e-05
108  108.0   0.933960    1.305215   0.804037    1.641162  1.000000e-06
109  109.0   0.929565    1.313072   0.802673    1.646218  1.000000e-06
110  110.0   0.935303    1.307620   0.803789    1.637893  1.000000e-06
111  111.0   0.935059    1.306649   0.803541    1.638622  1.000000e-06
112  112.0   0.929321    1.309359   0.803665    1.637762  1.000000e-06
113  113.0   0.930420    1.305300   0.803789    1.640701  1.000000e-06
114  114.0   0.928955    1.312955   0.804231    1.640038  1.000000e-07
115  115.0   0.933228    1.309091   0.805277    1.636816  1.000000e-07
116  116.0   0.932739    1.312986   0.802921    1.642846  1.000000e-07
117  117.0   0.928345    1.307930   0.803789    1.642446  1.000000e-07
118  118.0   0.933105    1.309355   0.804037    1.632954  1.000000e-07
119  119.0   0.933838    1.306412   0.804355    1.636284  1.000000e-07
120  120.0   0.930054    1.307064   0.802425    1.640805  1.000000e-07
121  121.0   0.936768    1.303463   0.805649    1.636771  1.000000e-07
122  122.0   0.932007    1.310633   0.803611    1.637974  1.000000e-07
123  123.0   0.932129    1.308865   0.802301    1.644346  1.000000e-07
124  124.0   0.931396    1.307972   0.804355    1.637644  1.000000e-07
125  125.0   0.931152    1.311183   0.805401    1.634656  1.000000e-07
126  126.0   0.932495    1.303207   0.804781    1.638258  1.000000e-07
127  127.0   0.930908    1.312128   0.804161    1.636594  1.000000e-07
128  128.0   0.927490    1.311087   0.802425    1.641997  1.000000e-07
129  129.0   0.931274    1.307847   0.803789    1.646113  1.000000e-07
130  130.0   0.929077    1.316746   0.801681    1.636500  1.000000e-07
131  131.0   0.928345    1.312771   0.803417    1.644031  1.000000e-07
132  132.0   0.929077    1.310908   0.802743    1.642748  1.000000e-07
133  133.0   0.931274    1.306457   0.804905    1.635568  1.000000e-07

EXPERIMENT: C-50
================

NAME: ghost_20200922060810

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [67, 82, 95, 107]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.008789    5.310587   0.009797    5.300360  1.000000e-03
1      1.0   0.011597    5.230509   0.015129    5.202743  1.000000e-03
2      2.0   0.012207    5.178302   0.012773    5.145216  1.000000e-03
3      3.0   0.013794    5.116442   0.014578    5.198411  1.000000e-03
4      4.0   0.017700    5.071394   0.021453    5.037583  1.000000e-03
5      5.0   0.024170    5.010454   0.027158    5.050159  1.000000e-03
6      6.0   0.027710    4.959751   0.024747    5.048742  1.000000e-03
7      7.0   0.030273    4.886800   0.027406    4.964282  1.000000e-03
8      8.0   0.036499    4.811533   0.034117    4.826703  1.000000e-03
9      9.0   0.044678    4.701542   0.048006    4.679767  1.000000e-03
10    10.0   0.049316    4.603120   0.037148    4.805694  1.000000e-03
11    11.0   0.059692    4.507995   0.055516    4.571047  1.000000e-03
12    12.0   0.075562    4.418528   0.076861    4.401350  1.000000e-03
13    13.0   0.081177    4.320765   0.055764    4.602170  1.000000e-03
14    14.0   0.093140    4.236495   0.083736    4.328046  1.000000e-03
15    15.0   0.103027    4.138438   0.086573    4.264387  1.000000e-03
16    16.0   0.118652    4.055239   0.084519    4.350755  1.000000e-03
17    17.0   0.147827    3.948509   0.111459    4.177568  1.000000e-03
18    18.0   0.154663    3.853916   0.128914    4.073573  1.000000e-03
19    19.0   0.174438    3.754614   0.171742    3.793428  1.000000e-03
20    20.0   0.203491    3.663835   0.201132    3.681006  1.000000e-03
21    21.0   0.216309    3.584552   0.185561    3.775836  1.000000e-03
22    22.0   0.250366    3.467542   0.214703    3.598763  1.000000e-03
23    23.0   0.270142    3.399386   0.242124    3.501507  1.000000e-03
24    24.0   0.286011    3.292139   0.251921    3.442787  1.000000e-03
25    25.0   0.322754    3.188291   0.290408    3.346585  1.000000e-03
26    26.0   0.346924    3.108135   0.270071    3.403274  1.000000e-03
27    27.0   0.358643    3.058702   0.298082    3.327135  1.000000e-03
28    28.0   0.399048    2.940554   0.275845    3.410160  1.000000e-03
29    29.0   0.424072    2.866996   0.289262    3.294363  1.000000e-03
30    30.0   0.449341    2.782690   0.307010    3.322966  1.000000e-03
31    31.0   0.459717    2.727884   0.369277    3.058892  1.000000e-03
32    32.0   0.500488    2.598192   0.349848    3.124814  1.000000e-03
33    33.0   0.494385    2.602245   0.359962    3.112910  1.000000e-03
34    34.0   0.524658    2.527390   0.340493    3.139416  1.000000e-03
35    35.0   0.554321    2.451705   0.406232    2.897883  1.000000e-03
36    36.0   0.565063    2.401803   0.388003    2.948155  1.000000e-03
37    37.0   0.569702    2.368539   0.489789    2.632042  1.000000e-03
38    38.0   0.591797    2.322929   0.363008    3.114025  1.000000e-03
39    39.0   0.609863    2.263856   0.457308    2.727444  1.000000e-03
40    40.0   0.619141    2.237592   0.422755    2.901211  1.000000e-03
41    41.0   0.638550    2.182381   0.465716    2.706614  1.000000e-03
42    42.0   0.643799    2.156828   0.508251    2.580447  1.000000e-03
43    43.0   0.654907    2.112678   0.391768    2.983351  1.000000e-03
44    44.0   0.670776    2.078780   0.470220    2.717685  1.000000e-03
45    45.0   0.672241    2.049038   0.506530    2.631685  1.000000e-03
46    46.0   0.683838    2.036462   0.431351    2.862915  1.000000e-03
47    47.0   0.686646    2.007961   0.468723    2.743157  1.000000e-03
48    48.0   0.689697    2.009718   0.505360    2.621759  1.000000e-03
49    49.0   0.701660    1.964821   0.516768    2.537663  1.000000e-03
50    50.0   0.720337    1.933836   0.523728    2.520987  1.000000e-03
51    51.0   0.722534    1.914301   0.518698    2.582866  1.000000e-03
52    52.0   0.731201    1.895510   0.578415    2.350436  1.000000e-03
53    53.0   0.735962    1.865786   0.288712    3.492089  1.000000e-03
54    54.0   0.730835    1.873800   0.375547    3.102982  1.000000e-03
55    55.0   0.737061    1.858105   0.430072    2.881489  1.000000e-03
56    56.0   0.758057    1.803660   0.467661    2.740348  1.000000e-03
57    57.0   0.755005    1.806478   0.482721    2.715419  1.000000e-03
58    58.0   0.762085    1.797378   0.560395    2.419241  1.000000e-03
59    59.0   0.770508    1.772544   0.538058    2.526452  1.000000e-03
60    60.0   0.769653    1.760164   0.476838    2.753071  1.000000e-03
61    61.0   0.768066    1.762297   0.490409    2.644851  1.000000e-03
62    62.0   0.784790    1.733026   0.499383    2.675218  1.000000e-03
63    63.0   0.784058    1.725108   0.392234    3.010589  1.000000e-03
64    64.0   0.778320    1.727881   0.484883    2.763640  1.000000e-03
65    65.0   0.794922    1.702776   0.497175    2.626384  1.000000e-03
66    66.0   0.776733    1.741504   0.494804    2.690088  1.000000e-03
67    67.0   0.901001    1.394050   0.793823    1.643305  1.000000e-04
68    68.0   0.942505    1.270965   0.803650    1.610175  1.000000e-04
69    69.0   0.954224    1.232503   0.812440    1.588740  1.000000e-04
70    70.0   0.951294    1.219144   0.814687    1.577473  1.000000e-04
71    71.0   0.956665    1.204225   0.814865    1.579863  1.000000e-04
72    72.0   0.964844    1.183470   0.822251    1.572321  1.000000e-04
73    73.0   0.962769    1.174722   0.822003    1.561112  1.000000e-04
74    74.0   0.967529    1.162216   0.817330    1.576108  1.000000e-04
75    75.0   0.969971    1.153077   0.820887    1.574146  1.000000e-04
76    76.0   0.970581    1.145460   0.825917    1.554673  1.000000e-04
77    77.0   0.974976    1.132621   0.821755    1.553477  1.000000e-04
78    78.0   0.976685    1.132338   0.824181    1.555836  1.000000e-04
79    79.0   0.976074    1.126525   0.827514    1.553442  1.000000e-04
80    80.0   0.976074    1.118997   0.825034    1.551377  1.000000e-04
81    81.0   0.979858    1.114361   0.821081    1.557434  1.000000e-04
82    82.0   0.983398    1.091352   0.830327    1.532979  1.000000e-05
83    83.0   0.983521    1.086289   0.833055    1.528112  1.000000e-05
84    84.0   0.983887    1.082556   0.832807    1.530411  1.000000e-05
85    85.0   0.985107    1.080663   0.833551    1.525305  1.000000e-05
86    86.0   0.986572    1.082351   0.833427    1.523808  1.000000e-05
87    87.0   0.984375    1.080110   0.834791    1.525447  1.000000e-05
88    88.0   0.987061    1.075764   0.833799    1.523246  1.000000e-05
89    89.0   0.986450    1.075930   0.833799    1.528416  1.000000e-05
90    90.0   0.984375    1.076554   0.834419    1.522309  1.000000e-05
91    91.0   0.985474    1.073947   0.833358    1.528092  1.000000e-05
92    92.0   0.987549    1.074974   0.833110    1.528143  1.000000e-05
93    93.0   0.987427    1.069859   0.833923    1.527349  1.000000e-05
94    94.0   0.985840    1.073260   0.834350    1.523188  1.000000e-05
95    95.0   0.986816    1.073083   0.835287    1.529263  1.000000e-06
96    96.0   0.988770    1.069594   0.835024    1.525289  1.000000e-06
97    97.0   0.988159    1.068712   0.834226    1.524734  1.000000e-06
98    98.0   0.987305    1.070236   0.835094    1.523884  1.000000e-06
99    99.0   0.988647    1.064955   0.835287    1.526866  1.000000e-06
100  100.0   0.988403    1.069431   0.835163    1.522464  1.000000e-06
101  101.0   0.986694    1.067078   0.834543    1.526109  1.000000e-06
102  102.0   0.987183    1.068644   0.833055    1.529064  1.000000e-06
103  103.0   0.987793    1.066039   0.836775    1.522009  1.000000e-06
104  104.0   0.989136    1.067660   0.837946    1.524683  1.000000e-06
105  105.0   0.989136    1.064069   0.836086    1.525389  1.000000e-06
106  106.0   0.986206    1.068906   0.834032    1.533074  1.000000e-06
107  107.0   0.989258    1.066504   0.834543    1.526306  1.000000e-07
108  108.0   0.985962    1.068923   0.835218    1.524773  1.000000e-07
109  109.0   0.989014    1.066814   0.836031    1.524887  1.000000e-07
110  110.0   0.986206    1.068281   0.835218    1.525331  1.000000e-07
111  111.0   0.988525    1.067250   0.833978    1.524698  1.000000e-07
112  112.0   0.986450    1.065767   0.835024    1.523938  1.000000e-07
113  113.0   0.987671    1.064973   0.834474    1.523003  1.000000e-07
114  114.0   0.987183    1.064862   0.836086    1.520796  1.000000e-07
115  115.0   0.989136    1.065335   0.836086    1.520849  1.000000e-07
116  116.0   0.989258    1.067301   0.834047    1.523033  1.000000e-07
117  117.0   0.989380    1.064483   0.833923    1.525035  1.000000e-07
118  118.0   0.987061    1.068654   0.835411    1.525461  1.000000e-07
119  119.0   0.987549    1.066993   0.835907    1.524961  1.000000e-07
120  120.0   0.987915    1.066535   0.835272    1.523122  1.000000e-07
121  121.0   0.986938    1.067851   0.834543    1.527215  1.000000e-07
122  122.0   0.987549    1.068838   0.834295    1.525521  1.000000e-07
123  123.0   0.988525    1.068183   0.835411    1.528295  1.000000e-07
124  124.0   0.988770    1.066186   0.834171    1.525592  1.000000e-07
125  125.0   0.989014    1.064380   0.834722    1.523450  1.000000e-07
126  126.0   0.987793    1.066460   0.831815    1.528665  1.000000e-07
127  127.0   0.986816    1.068668   0.837643    1.522678  1.000000e-07
128  128.0   0.988525    1.066159   0.834295    1.525631  1.000000e-07
129  129.0   0.986938    1.066419   0.836954    1.525104  1.000000e-07

EXPERIMENT: C-51
================

NAME: ghost_20200922061137

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [63, 78, 91, 103]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.007446    5.312986   0.010169    5.234593  1.000000e-03
1      1.0   0.011108    5.220177   0.013889    5.288152  1.000000e-03
2      2.0   0.013916    5.177586   0.013765    5.199263  1.000000e-03
3      3.0   0.017212    5.119590   0.021949    5.106779  1.000000e-03
4      4.0   0.020630    5.049519   0.019911    5.055981  1.000000e-03
5      5.0   0.030029    4.974448   0.024058    5.019629  1.000000e-03
6      6.0   0.034058    4.909381   0.035357    4.954296  1.000000e-03
7      7.0   0.042236    4.834528   0.044588    4.802517  1.000000e-03
8      8.0   0.042847    4.740969   0.035838    4.875245  1.000000e-03
9      9.0   0.054077    4.669999   0.051672    4.671941  1.000000e-03
10    10.0   0.063477    4.569884   0.046821    4.689374  1.000000e-03
11    11.0   0.069580    4.486165   0.068716    4.483545  1.000000e-03
12    12.0   0.079468    4.396204   0.081077    4.385661  1.000000e-03
13    13.0   0.091797    4.308317   0.092416    4.293641  1.000000e-03
14    14.0   0.105957    4.227476   0.108661    4.211478  1.000000e-03
15    15.0   0.117188    4.120737   0.109847    4.147902  1.000000e-03
16    16.0   0.122803    4.053339   0.093160    4.304477  1.000000e-03
17    17.0   0.144531    3.955210   0.146692    3.956281  1.000000e-03
18    18.0   0.161133    3.851641   0.161271    3.919178  1.000000e-03
19    19.0   0.188843    3.735297   0.161117    3.885476  1.000000e-03
20    20.0   0.221802    3.613307   0.191653    3.810924  1.000000e-03
21    21.0   0.239624    3.523946   0.224778    3.614515  1.000000e-03
22    22.0   0.277832    3.409597   0.290091    3.377027  1.000000e-03
23    23.0   0.300903    3.316275   0.264128    3.434168  1.000000e-03
24    24.0   0.337036    3.181223   0.305135    3.297933  1.000000e-03
25    25.0   0.358032    3.097104   0.281861    3.373829  1.000000e-03
26    26.0   0.387939    2.998358   0.349769    3.122033  1.000000e-03
27    27.0   0.413330    2.901406   0.319550    3.220029  1.000000e-03
28    28.0   0.437134    2.823618   0.384933    3.012618  1.000000e-03
29    29.0   0.463623    2.736730   0.372066    3.041813  1.000000e-03
30    30.0   0.485596    2.677553   0.319550    3.210604  1.000000e-03
31    31.0   0.512085    2.599249   0.351792    3.129065  1.000000e-03
32    32.0   0.524048    2.547976   0.398519    2.933982  1.000000e-03
33    33.0   0.540649    2.482679   0.399774    2.951916  1.000000e-03
34    34.0   0.560181    2.424029   0.342268    3.133185  1.000000e-03
35    35.0   0.574951    2.386382   0.410409    2.904824  1.000000e-03
36    36.0   0.593506    2.320709   0.337462    3.218687  1.000000e-03
37    37.0   0.599365    2.293064   0.447130    2.797014  1.000000e-03
38    38.0   0.615723    2.240957   0.443480    2.825732  1.000000e-03
39    39.0   0.630615    2.207027   0.385994    3.077623  1.000000e-03
40    40.0   0.648193    2.155873   0.454531    2.739327  1.000000e-03
41    41.0   0.654663    2.129918   0.427289    2.860923  1.000000e-03
42    42.0   0.660034    2.107655   0.426708    2.819399  1.000000e-03
43    43.0   0.692383    2.046310   0.521193    2.529137  1.000000e-03
44    44.0   0.682007    2.041636   0.492904    2.650940  1.000000e-03
45    45.0   0.690308    2.030493   0.500236    2.605733  1.000000e-03
46    46.0   0.706909    1.963761   0.503693    2.585548  1.000000e-03
47    47.0   0.711304    1.967090   0.492617    2.659123  1.000000e-03
48    48.0   0.713745    1.945300   0.489293    2.721830  1.000000e-03
49    49.0   0.723877    1.918970   0.507220    2.608251  1.000000e-03
50    50.0   0.732788    1.893128   0.538579    2.494826  1.000000e-03
51    51.0   0.732056    1.887517   0.469576    2.746133  1.000000e-03
52    52.0   0.739502    1.875268   0.474978    2.719066  1.000000e-03
53    53.0   0.750366    1.838239   0.434233    2.852943  1.000000e-03
54    54.0   0.755615    1.823767   0.420221    2.892514  1.000000e-03
55    55.0   0.754028    1.828100   0.367929    3.181535  1.000000e-03
56    56.0   0.759766    1.796375   0.290176    3.581576  1.000000e-03
57    57.0   0.767090    1.786052   0.536625    2.525100  1.000000e-03
58    58.0   0.772339    1.775947   0.385220    3.075362  1.000000e-03
59    59.0   0.774170    1.776776   0.469963    2.718996  1.000000e-03
60    60.0   0.781128    1.750820   0.570881    2.426491  1.000000e-03
61    61.0   0.779053    1.745792   0.544755    2.488981  1.000000e-03
62    62.0   0.780640    1.726233   0.478162    2.716691  1.000000e-03
63    63.0   0.897095    1.416131   0.794195    1.671564  1.000000e-04
64    64.0   0.936279    1.290688   0.799101    1.654706  1.000000e-04
65    65.0   0.948242    1.251237   0.804433    1.634644  1.000000e-04
66    66.0   0.953979    1.236367   0.806898    1.632197  1.000000e-04
67    67.0   0.957153    1.220714   0.806348    1.632030  1.000000e-04
68    68.0   0.959473    1.204121   0.808332    1.619564  1.000000e-04
69    69.0   0.966064    1.190380   0.806790    1.624270  1.000000e-04
70    70.0   0.967896    1.178156   0.814052    1.610916  1.000000e-04
71    71.0   0.969238    1.171484   0.810634    1.615253  1.000000e-04
72    72.0   0.972656    1.163188   0.812107    1.609923  1.000000e-04
73    73.0   0.973511    1.154625   0.814726    1.616140  1.000000e-04
74    74.0   0.976074    1.146590   0.810247    1.620288  1.000000e-04
75    75.0   0.973389    1.140216   0.808774    1.624664  1.000000e-04
76    76.0   0.975830    1.139276   0.817191    1.601121  1.000000e-04
77    77.0   0.978149    1.127434   0.816958    1.601416  1.000000e-04
78    78.0   0.981201    1.105727   0.821353    1.586080  1.000000e-05
79    79.0   0.981323    1.101113   0.821919    1.584819  1.000000e-05
80    80.0   0.984253    1.098431   0.822732    1.577579  1.000000e-05
81    81.0   0.985229    1.092894   0.824027    1.577651  1.000000e-05
82    82.0   0.983887    1.096138   0.820058    1.579791  1.000000e-05
83    83.0   0.983643    1.095023   0.823779    1.576501  1.000000e-05
84    84.0   0.985718    1.091903   0.823972    1.585043  1.000000e-05
85    85.0   0.985229    1.088322   0.823833    1.578284  1.000000e-05
86    86.0   0.985596    1.088595   0.823888    1.577696  1.000000e-05
87    87.0   0.985962    1.087388   0.820733    1.580137  1.000000e-05
88    88.0   0.986694    1.083640   0.823516    1.578007  1.000000e-05
89    89.0   0.988037    1.082659   0.822415    1.574712  1.000000e-05
90    90.0   0.985352    1.089277   0.823903    1.574248  1.000000e-05
91    91.0   0.987793    1.083906   0.822717    1.576346  1.000000e-06
92    92.0   0.987061    1.083931   0.822593    1.580418  1.000000e-06
93    93.0   0.988647    1.080123   0.825445    1.577101  1.000000e-06
94    94.0   0.987427    1.082092   0.825391    1.577096  1.000000e-06
95    95.0   0.987549    1.079715   0.824399    1.574869  1.000000e-06
96    96.0   0.984985    1.079703   0.824027    1.571921  1.000000e-06
97    97.0   0.986938    1.077135   0.821973    1.579868  1.000000e-06
98    98.0   0.987793    1.079146   0.822593    1.580656  1.000000e-06
99    99.0   0.987183    1.080250   0.821159    1.582616  1.000000e-06
100  100.0   0.988403    1.079608   0.821477    1.581674  1.000000e-06
101  101.0   0.987671    1.080292   0.823407    1.577667  1.000000e-06
102  102.0   0.988525    1.076591   0.824027    1.581460  1.000000e-06
103  103.0   0.986572    1.081444   0.823833    1.577197  1.000000e-07
104  104.0   0.986450    1.078956   0.822345    1.584824  1.000000e-07
105  105.0   0.986328    1.082276   0.822151    1.582594  1.000000e-07
106  106.0   0.987915    1.080179   0.823407    1.583761  1.000000e-07
107  107.0   0.986206    1.081956   0.824577    1.575044  1.000000e-07
108  108.0   0.986694    1.081642   0.820415    1.586159  1.000000e-07
109  109.0   0.986450    1.080765   0.822717    1.583233  1.000000e-07
110  110.0   0.986084    1.079954   0.821725    1.584084  1.000000e-07
111  111.0   0.986572    1.080863   0.823337    1.576267  1.000000e-07

EXPERIMENT: C-53
================

NAME: ghost_20200922184526

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [66, 81, 94, 106]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.008057    5.329851   0.009494    5.321342  1.000000e-03
1      1.0   0.011353    5.231747   0.011285    5.198741  1.000000e-03
2      2.0   0.012085    5.186630   0.011920    5.173471  1.000000e-03
3      3.0   0.015259    5.134311   0.012594    5.135393  1.000000e-03
4      4.0   0.018311    5.093567   0.020709    5.089024  1.000000e-03
5      5.0   0.023804    5.038107   0.020104    5.051152  1.000000e-03
6      6.0   0.028687    4.984319   0.020035    5.106627  1.000000e-03
7      7.0   0.032715    4.920887   0.025506    5.102067  1.000000e-03
8      8.0   0.037964    4.859153   0.039574    4.891702  1.000000e-03
9      9.0   0.045044    4.773023   0.040427    4.814027  1.000000e-03
10    10.0   0.047607    4.691701   0.051850    4.712874  1.000000e-03
11    11.0   0.055542    4.608514   0.037287    4.930102  1.000000e-03
12    12.0   0.065552    4.530293   0.053408    4.733828  1.000000e-03
13    13.0   0.074585    4.443353   0.067064    4.570037  1.000000e-03
14    14.0   0.088379    4.333665   0.069103    4.433451  1.000000e-03
15    15.0   0.096802    4.253120   0.111807    4.193871  1.000000e-03
16    16.0   0.109619    4.164475   0.118488    4.158092  1.000000e-03
17    17.0   0.129272    4.058589   0.114505    4.165246  1.000000e-03
18    18.0   0.140015    3.974353   0.142685    4.002982  1.000000e-03
19    19.0   0.164795    3.860750   0.154907    3.928847  1.000000e-03
20    20.0   0.184937    3.764974   0.191792    3.782030  1.000000e-03
21    21.0   0.209961    3.658162   0.214957    3.672406  1.000000e-03
22    22.0   0.232788    3.545066   0.238062    3.574469  1.000000e-03
23    23.0   0.259888    3.448011   0.212873    3.654388  1.000000e-03
24    24.0   0.286865    3.347690   0.270304    3.445116  1.000000e-03
25    25.0   0.319458    3.233462   0.270785    3.375188  1.000000e-03
26    26.0   0.344482    3.131817   0.335042    3.208151  1.000000e-03
27    27.0   0.360840    3.048905   0.242169    3.552831  1.000000e-03
28    28.0   0.386230    2.979064   0.351892    3.140387  1.000000e-03
29    29.0   0.401001    2.919709   0.371709    3.023460  1.000000e-03
30    30.0   0.426392    2.850070   0.324208    3.199758  1.000000e-03
31    31.0   0.453613    2.751042   0.312288    3.237794  1.000000e-03
32    32.0   0.471924    2.687626   0.377383    3.010410  1.000000e-03
33    33.0   0.486328    2.637991   0.410439    2.900235  1.000000e-03
34    34.0   0.517456    2.562036   0.345553    3.146176  1.000000e-03
35    35.0   0.532471    2.515281   0.317040    3.197228  1.000000e-03
36    36.0   0.550781    2.461599   0.372874    3.141857  1.000000e-03
37    37.0   0.563110    2.415150   0.332432    3.219356  1.000000e-03
38    38.0   0.571655    2.366186   0.422740    2.862666  1.000000e-03
39    39.0   0.586060    2.326538   0.398806    2.963682  1.000000e-03
40    40.0   0.605103    2.276170   0.436559    2.841969  1.000000e-03
41    41.0   0.609497    2.260937   0.357938    3.124775  1.000000e-03
42    42.0   0.628174    2.210346   0.436242    2.827352  1.000000e-03
43    43.0   0.633057    2.175314   0.492160    2.578146  1.000000e-03
44    44.0   0.646729    2.137839   0.361495    3.155195  1.000000e-03
45    45.0   0.642212    2.139272   0.491029    2.668219  1.000000e-03
46    46.0   0.670166    2.092948   0.462879    2.825109  1.000000e-03
47    47.0   0.666504    2.084753   0.413439    2.934115  1.000000e-03
48    48.0   0.686523    2.022961   0.481922    2.694342  1.000000e-03
49    49.0   0.698242    2.002142   0.346049    3.136725  1.000000e-03
50    50.0   0.690918    1.992755   0.436272    2.832962  1.000000e-03
51    51.0   0.697021    1.986306   0.449992    2.789824  1.000000e-03
52    52.0   0.719971    1.916987   0.486704    2.656755  1.000000e-03
53    53.0   0.720459    1.928464   0.443489    2.817405  1.000000e-03
54    54.0   0.725830    1.902028   0.502438    2.632061  1.000000e-03
55    55.0   0.734619    1.878733   0.420245    2.922384  1.000000e-03
56    56.0   0.745483    1.856405   0.496237    2.618493  1.000000e-03
57    57.0   0.740234    1.851269   0.548033    2.469967  1.000000e-03
58    58.0   0.754028    1.815192   0.411649    2.927960  1.000000e-03
59    59.0   0.761963    1.792492   0.550638    2.465592  1.000000e-03
60    60.0   0.754761    1.805704   0.457477    2.779431  1.000000e-03
61    61.0   0.767334    1.777536   0.541476    2.423231  1.000000e-03
62    62.0   0.781738    1.733633   0.511297    2.613198  1.000000e-03
63    63.0   0.775513    1.745686   0.425925    2.899033  1.000000e-03
64    64.0   0.780884    1.734748   0.525984    2.523438  1.000000e-03
65    65.0   0.777588    1.737176   0.514249    2.589949  1.000000e-03
66    66.0   0.898438    1.405856   0.798605    1.643656  1.000000e-04
67    67.0   0.936401    1.286026   0.806914    1.621018  1.000000e-04
68    68.0   0.945801    1.260180   0.807534    1.607054  1.000000e-04
69    69.0   0.954468    1.226057   0.807836    1.608229  1.000000e-04
70    70.0   0.959106    1.210002   0.813362    1.587523  1.000000e-04
71    71.0   0.955933    1.207960   0.814548    1.578668  1.000000e-04
72    72.0   0.965332    1.188706   0.813168    1.580402  1.000000e-04
73    73.0   0.966675    1.176398   0.816338    1.579812  1.000000e-04
74    74.0   0.968750    1.162893   0.817896    1.570824  1.000000e-04
75    75.0   0.971313    1.155400   0.818764    1.571430  1.000000e-04
76    76.0   0.971558    1.148436   0.816656    1.588468  1.000000e-04
77    77.0   0.972290    1.143805   0.825282    1.550756  1.000000e-04
78    78.0   0.976562    1.133655   0.819950    1.563478  1.000000e-04
79    79.0   0.975220    1.131870   0.818997    1.560043  1.000000e-04
80    80.0   0.977417    1.123025   0.822484    1.556260  1.000000e-04
81    81.0   0.981567    1.102431   0.823903    1.547617  1.000000e-05
82    82.0   0.982300    1.096106   0.826452    1.540320  1.000000e-05
83    83.0   0.984253    1.093094   0.826080    1.545622  1.000000e-05
84    84.0   0.982910    1.090759   0.826452    1.539726  1.000000e-05
85    85.0   0.983887    1.091697   0.826452    1.541598  1.000000e-05
86    86.0   0.985229    1.087943   0.827196    1.536748  1.000000e-05
87    87.0   0.985107    1.081984   0.829925    1.537424  1.000000e-05
88    88.0   0.985229    1.084988   0.827941    1.535895  1.000000e-05
89    89.0   0.985596    1.084214   0.829374    1.535527  1.000000e-05
90    90.0   0.985107    1.084764   0.829057    1.536692  1.000000e-05
91    91.0   0.989624    1.078838   0.827762    1.536931  1.000000e-05
92    92.0   0.986206    1.081876   0.829553    1.529839  1.000000e-05
93    93.0   0.985718    1.078145   0.827018    1.539934  1.000000e-05
94    94.0   0.984497    1.077712   0.828878    1.534681  1.000000e-06
95    95.0   0.986816    1.075492   0.828437    1.535584  1.000000e-06
96    96.0   0.986572    1.079418   0.828065    1.536245  1.000000e-06
97    97.0   0.986206    1.079184   0.829126    1.534938  1.000000e-06
98    98.0   0.986450    1.076541   0.829305    1.535920  1.000000e-06
99    99.0   0.987305    1.075004   0.827142    1.538171  1.000000e-06
100  100.0   0.986694    1.075005   0.828685    1.533261  1.000000e-06
101  101.0   0.986694    1.076498   0.828630    1.541318  1.000000e-06
102  102.0   0.988525    1.073696   0.829002    1.533791  1.000000e-06
103  103.0   0.985107    1.075706   0.828437    1.537204  1.000000e-06
104  104.0   0.986572    1.073460   0.830173    1.537037  1.000000e-06
105  105.0   0.987549    1.074675   0.828754    1.538194  1.000000e-06
106  106.0   0.986572    1.076432   0.829622    1.534974  1.000000e-07
107  107.0   0.985352    1.075738   0.829994    1.534452  1.000000e-07

EXPERIMENT: C-55
================

NAME: ghost_20200922221841

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [68, 83, 96, 108]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.007202    5.349634   0.010541    5.333549  1.000000e-03
1      1.0   0.011597    5.259652   0.012649    5.363226  1.000000e-03
2      2.0   0.011230    5.172728   0.013765    5.191160  1.000000e-03
3      3.0   0.015137    5.128074   0.015129    5.125889  1.000000e-03
4      4.0   0.017700    5.076920   0.016245    5.161699  1.000000e-03
5      5.0   0.022095    5.044067   0.019717    5.078834  1.000000e-03
6      6.0   0.025635    4.992588   0.028150    5.024432  1.000000e-03
7      7.0   0.031250    4.923566   0.032366    4.953863  1.000000e-03
8      8.0   0.036011    4.851324   0.021949    5.088210  1.000000e-03
9      9.0   0.043579    4.749228   0.031622    4.870713  1.000000e-03
10    10.0   0.049561    4.631597   0.038061    4.870106  1.000000e-03
11    11.0   0.058472    4.524341   0.051726    4.590393  1.000000e-03
12    12.0   0.071899    4.419063   0.080496    4.387380  1.000000e-03
13    13.0   0.082642    4.328421   0.080209    4.382659  1.000000e-03
14    14.0   0.092651    4.222919   0.094358    4.246029  1.000000e-03
15    15.0   0.107788    4.140296   0.110863    4.141819  1.000000e-03
16    16.0   0.120239    4.029707   0.086119    4.263928  1.000000e-03
17    17.0   0.141357    3.954557   0.146599    3.991091  1.000000e-03
18    18.0   0.166748    3.833069   0.154886    3.893175  1.000000e-03
19    19.0   0.189941    3.740212   0.175214    3.815732  1.000000e-03
20    20.0   0.201294    3.664930   0.146762    3.978354  1.000000e-03
21    21.0   0.227661    3.543577   0.166137    3.831272  1.000000e-03
22    22.0   0.255127    3.461266   0.203349    3.655693  1.000000e-03
23    23.0   0.276367    3.385055   0.222116    3.571398  1.000000e-03
24    24.0   0.297852    3.300015   0.272134    3.400552  1.000000e-03
25    25.0   0.334595    3.175130   0.291231    3.342963  1.000000e-03
26    26.0   0.346680    3.103515   0.262939    3.417308  1.000000e-03
27    27.0   0.374512    3.020326   0.310954    3.226249  1.000000e-03
28    28.0   0.393799    2.931927   0.322962    3.187276  1.000000e-03
29    29.0   0.412842    2.869187   0.313807    3.226735  1.000000e-03
30    30.0   0.437012    2.796983   0.364263    3.053987  1.000000e-03
31    31.0   0.472168    2.694008   0.353265    3.100397  1.000000e-03
32    32.0   0.492188    2.614180   0.381739    2.991489  1.000000e-03
33    33.0   0.520874    2.559171   0.335925    3.121975  1.000000e-03
34    34.0   0.520264    2.542541   0.382304    3.039973  1.000000e-03
35    35.0   0.548584    2.457196   0.451933    2.766056  1.000000e-03
36    36.0   0.567749    2.401862   0.429969    2.863364  1.000000e-03
37    37.0   0.573853    2.366910   0.384981    2.988234  1.000000e-03
38    38.0   0.591309    2.328200   0.390440    3.005921  1.000000e-03
39    39.0   0.607422    2.276415   0.410717    2.903634  1.000000e-03
40    40.0   0.624390    2.216837   0.429388    2.876730  1.000000e-03
41    41.0   0.630005    2.188055   0.448649    2.782370  1.000000e-03
42    42.0   0.653687    2.134045   0.478069    2.671075  1.000000e-03
43    43.0   0.658813    2.132900   0.438598    2.813513  1.000000e-03
44    44.0   0.655762    2.111441   0.484820    2.668309  1.000000e-03
45    45.0   0.675903    2.060162   0.521804    2.517746  1.000000e-03
46    46.0   0.677490    2.045409   0.528897    2.521303  1.000000e-03
47    47.0   0.689697    2.011282   0.387135    2.959102  1.000000e-03
48    48.0   0.699219    1.971367   0.438565    2.835531  1.000000e-03
49    49.0   0.713623    1.955128   0.549367    2.445739  1.000000e-03
50    50.0   0.722168    1.934617   0.448534    2.830261  1.000000e-03
51    51.0   0.721680    1.923096   0.489081    2.641316  1.000000e-03
52    52.0   0.724976    1.904024   0.447817    2.799080  1.000000e-03
53    53.0   0.732666    1.875824   0.546445    2.444708  1.000000e-03
54    54.0   0.745361    1.841354   0.530657    2.500806  1.000000e-03
55    55.0   0.731323    1.876446   0.525966    2.552089  1.000000e-03
56    56.0   0.753662    1.822513   0.516402    2.593314  1.000000e-03
57    57.0   0.751099    1.820476   0.500817    2.653857  1.000000e-03
58    58.0   0.753052    1.824985   0.524814    2.580966  1.000000e-03
59    59.0   0.752563    1.811398   0.404115    3.055534  1.000000e-03
60    60.0   0.769409    1.766857   0.507979    2.564572  1.000000e-03
61    61.0   0.770264    1.754050   0.528540    2.516354  1.000000e-03
62    62.0   0.775146    1.748726   0.521565    2.579617  1.000000e-03
63    63.0   0.775513    1.741754   0.431644    2.941133  1.000000e-03
64    64.0   0.778931    1.730901   0.456080    2.782081  1.000000e-03
65    65.0   0.788940    1.716965   0.460699    2.818039  1.000000e-03
66    66.0   0.784180    1.727685   0.490339    2.737304  1.000000e-03
67    67.0   0.792847    1.696597   0.443286    2.847866  1.000000e-03
68    68.0   0.899658    1.393699   0.801844    1.629530  1.000000e-04
69    69.0   0.939819    1.273328   0.811602    1.591287  1.000000e-04
70    70.0   0.948730    1.242402   0.813819    1.589666  1.000000e-04
71    71.0   0.954224    1.223039   0.815292    1.579843  1.000000e-04
72    72.0   0.956787    1.203360   0.815431    1.579979  1.000000e-04
73    73.0   0.961304    1.189829   0.820158    1.571761  1.000000e-04
74    74.0   0.962524    1.179843   0.820074    1.564024  1.000000e-04
75    75.0   0.964844    1.172784   0.821096    1.565796  1.000000e-04
76    76.0   0.970093    1.157441   0.825754    1.550314  1.000000e-04
77    77.0   0.970093    1.153742   0.819593    1.560179  1.000000e-04
78    78.0   0.971313    1.142922   0.825010    1.561316  1.000000e-04
79    79.0   0.972412    1.135427   0.822778    1.566754  1.000000e-04
80    80.0   0.976074    1.134212   0.829900    1.547674  1.000000e-04
81    81.0   0.978638    1.122450   0.829072    1.546486  1.000000e-04
82    82.0   0.978149    1.119246   0.825491    1.557785  1.000000e-04
83    83.0   0.981567    1.096488   0.831691    1.539903  1.000000e-05
84    84.0   0.982666    1.095064   0.833675    1.534641  1.000000e-05
85    85.0   0.984009    1.089015   0.833303    1.535995  1.000000e-05
86    86.0   0.982788    1.091470   0.835426    1.526166  1.000000e-05
87    87.0   0.983887    1.087773   0.833690    1.535172  1.000000e-05
88    88.0   0.983765    1.087319   0.834682    1.526949  1.000000e-05
89    89.0   0.983398    1.083177   0.837038    1.528464  1.000000e-05
90    90.0   0.986450    1.079803   0.834434    1.530154  1.000000e-05
91    91.0   0.984863    1.080016   0.835426    1.527457  1.000000e-05
92    92.0   0.984497    1.080798   0.834791    1.529459  1.000000e-05
93    93.0   0.985962    1.079099   0.835729    1.527624  1.000000e-05
94    94.0   0.985840    1.077431   0.836403    1.529681  1.000000e-05
95    95.0   0.987427    1.074640   0.836101    1.530750  1.000000e-05
96    96.0   0.986816    1.077091   0.835411    1.528938  1.000000e-06
97    97.0   0.986572    1.076735   0.836969    1.529022  1.000000e-06
98    98.0   0.985596    1.074664   0.834667    1.533580  1.000000e-06
99    99.0   0.987061    1.074750   0.836279    1.529308  1.000000e-06
100  100.0   0.987549    1.070501   0.835411    1.530621  1.000000e-06
101  101.0   0.985718    1.074980   0.836527    1.525822  1.000000e-06
102  102.0   0.986450    1.075325   0.834861    1.526969  1.000000e-06
103  103.0   0.984863    1.075624   0.835907    1.529462  1.000000e-06
104  104.0   0.986084    1.076035   0.837217    1.526208  1.000000e-06
105  105.0   0.987793    1.070233   0.835977    1.527703  1.000000e-06
106  106.0   0.985718    1.076580   0.835605    1.533084  1.000000e-06
107  107.0   0.987427    1.071705   0.836155    1.527171  1.000000e-06
108  108.0   0.986450    1.071478   0.836349    1.531559  1.000000e-07
109  109.0   0.987305    1.072289   0.837202    1.528591  1.000000e-07
110  110.0   0.986572    1.072612   0.836721    1.532670  1.000000e-07
111  111.0   0.985840    1.071154   0.836969    1.525991  1.000000e-07
112  112.0   0.985229    1.073546   0.836031    1.527177  1.000000e-07
113  113.0   0.986084    1.072795   0.835411    1.527768  1.000000e-07
114  114.0   0.986206    1.072864   0.835411    1.529854  1.000000e-07
115  115.0   0.986328    1.070063   0.836899    1.528976  1.000000e-07
116  116.0   0.986938    1.071272   0.836473    1.532536  1.000000e-07

EXPERIMENT: C-56
================

NAME: ghost_20200922230745

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'MultiStepLR',
 'lr_scheduler_params': "{'gamma': 0.1, 'milestones': [64, 79, 92, 104]}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0      0.0   0.006592    5.321215   0.009618    5.219828  1.000000e-03
1      1.0   0.011719    5.222775   0.013214    5.193241  1.000000e-03
2      2.0   0.015503    5.159358   0.018601    5.141021  1.000000e-03
3      3.0   0.021973    5.088685   0.019097    5.081828  1.000000e-03
4      4.0   0.025024    5.029270   0.018477    5.139143  1.000000e-03
5      5.0   0.028198    4.967606   0.027351    4.972241  1.000000e-03
6      6.0   0.030273    4.885849   0.032560    4.903331  1.000000e-03
7      7.0   0.041748    4.801877   0.041062    4.792350  1.000000e-03
8      8.0   0.044800    4.691426   0.026057    5.007965  1.000000e-03
9      9.0   0.054199    4.604290   0.042813    4.740698  1.000000e-03
10    10.0   0.067139    4.494956   0.059911    4.545737  1.000000e-03
11    11.0   0.071533    4.410440   0.075993    4.368093  1.000000e-03
12    12.0   0.080444    4.314396   0.049331    4.611796  1.000000e-03
13    13.0   0.100098    4.207280   0.111389    4.205520  1.000000e-03
14    14.0   0.110352    4.112040   0.108979    4.202116  1.000000e-03
15    15.0   0.131592    4.004366   0.112094    4.179939  1.000000e-03
16    16.0   0.144409    3.918158   0.105065    4.194618  1.000000e-03
17    17.0   0.161865    3.822242   0.144251    3.964957  1.000000e-03
18    18.0   0.195068    3.722196   0.175795    3.827883  1.000000e-03
19    19.0   0.205444    3.634206   0.174168    3.773177  1.000000e-03
20    20.0   0.230957    3.540847   0.184654    3.734991  1.000000e-03
21    21.0   0.256592    3.446304   0.165045    3.900643  1.000000e-03
22    22.0   0.293091    3.323029   0.261282    3.463739  1.000000e-03
23    23.0   0.311646    3.235632   0.279937    3.361318  1.000000e-03
24    24.0   0.345337    3.149684   0.312660    3.217661  1.000000e-03
25    25.0   0.370483    3.033266   0.269188    3.451586  1.000000e-03
26    26.0   0.392090    2.954138   0.309878    3.258725  1.000000e-03
27    27.0   0.416016    2.890205   0.317442    3.226036  1.000000e-03
28    28.0   0.431641    2.821530   0.372750    3.032807  1.000000e-03
29    29.0   0.461060    2.737015   0.403852    2.918624  1.000000e-03
30    30.0   0.478394    2.680148   0.389382    2.959000  1.000000e-03
31    31.0   0.505981    2.604652   0.406425    2.920636  1.000000e-03
32    32.0   0.523682    2.523773   0.398186    2.937743  1.000000e-03
33    33.0   0.538574    2.485940   0.352273    3.124412  1.000000e-03
34    34.0   0.563477    2.411283   0.393622    2.953110  1.000000e-03
35    35.0   0.569824    2.385710   0.446193    2.819750  1.000000e-03
36    36.0   0.589355    2.338439   0.436396    2.811598  1.000000e-03
37    37.0   0.598145    2.284335   0.468856    2.733531  1.000000e-03
38    38.0   0.626831    2.229198   0.362799    3.032886  1.000000e-03
39    39.0   0.625977    2.213172   0.440186    2.775369  1.000000e-03
40    40.0   0.641235    2.167259   0.350498    3.169749  1.000000e-03
41    41.0   0.652588    2.135086   0.475722    2.661236  1.000000e-03
42    42.0   0.660889    2.106709   0.416842    2.855009  1.000000e-03
43    43.0   0.680054    2.060207   0.484774    2.662680  1.000000e-03
44    44.0   0.691162    2.009165   0.452944    2.754641  1.000000e-03
45    45.0   0.696899    1.991321   0.457780    2.781925  1.000000e-03
46    46.0   0.697632    1.974364   0.440488    2.862659  1.000000e-03
47    47.0   0.703857    1.980083   0.380741    3.089472  1.000000e-03
48    48.0   0.708618    1.944646   0.493990    2.614970  1.000000e-03
49    49.0   0.721436    1.914055   0.342849    3.157643  1.000000e-03
50    50.0   0.731079    1.893323   0.379640    3.058634  1.000000e-03
51    51.0   0.744019    1.852022   0.477854    2.682913  1.000000e-03
52    52.0   0.739868    1.862240   0.501034    2.615418  1.000000e-03
53    53.0   0.757446    1.815813   0.493013    2.656102  1.000000e-03
54    54.0   0.743896    1.843333   0.491664    2.643850  1.000000e-03
55    55.0   0.753418    1.816437   0.489269    2.697269  1.000000e-03
56    56.0   0.764282    1.793936   0.479660    2.699329  1.000000e-03
57    57.0   0.770264    1.763608   0.555483    2.465593  1.000000e-03
58    58.0   0.782837    1.742076   0.414997    2.967396  1.000000e-03
59    59.0   0.775757    1.757164   0.539422    2.513232  1.000000e-03
60    60.0   0.779541    1.734536   0.463306    2.779065  1.000000e-03
61    61.0   0.785645    1.732935   0.471723    2.763834  1.000000e-03
62    62.0   0.791138    1.697584   0.156132    4.451094  1.000000e-03
63    63.0   0.791992    1.716573   0.535478    2.508826  1.000000e-03
64    64.0   0.899414    1.400414   0.784949    1.678097  1.000000e-04
65    65.0   0.942261    1.277643   0.798357    1.635941  1.000000e-04
66    66.0   0.948975    1.242423   0.804116    1.619801  1.000000e-04
67    67.0   0.954346    1.218148   0.805852    1.614072  1.000000e-04
68    68.0   0.961670    1.199626   0.807534    1.616072  1.000000e-04
69    69.0   0.963989    1.188803   0.808347    1.598195  1.000000e-04
70    70.0   0.967407    1.179339   0.807658    1.598546  1.000000e-04
71    71.0   0.966675    1.169316   0.810440    1.605844  1.000000e-04
72    72.0   0.972046    1.156047   0.817082    1.595790  1.000000e-04
73    73.0   0.974243    1.147268   0.813664    1.601191  1.000000e-04
74    74.0   0.974731    1.141281   0.813362    1.590192  1.000000e-04
75    75.0   0.976196    1.135521   0.816586    1.592631  1.000000e-04
76    76.0   0.978638    1.128950   0.819741    1.583312  1.000000e-04
77    77.0   0.978516    1.119012   0.817191    1.586722  1.000000e-04
78    78.0   0.981567    1.111665   0.815773    1.580213  1.000000e-04
79    79.0   0.985474    1.092031   0.821477    1.569280  1.000000e-05
80    80.0   0.983398    1.090542   0.823213    1.567194  1.000000e-05
81    81.0   0.985107    1.085224   0.824701    1.565760  1.000000e-05
82    82.0   0.986572    1.083101   0.824825    1.566445  1.000000e-05
83    83.0   0.985718    1.080270   0.824508    1.566471  1.000000e-05
84    84.0   0.987061    1.075697   0.824756    1.565358  1.000000e-05
85    85.0   0.987915    1.076558   0.823764    1.564828  1.000000e-05
86    86.0   0.988037    1.077623   0.824384    1.564488  1.000000e-05
87    87.0   0.985352    1.077665   0.825128    1.561267  1.000000e-05
88    88.0   0.986694    1.076091   0.826685    1.567095  1.000000e-05
89    89.0   0.987183    1.076045   0.826616    1.559659  1.000000e-05
90    90.0   0.988159    1.074385   0.826988    1.562455  1.000000e-05
91    91.0   0.985962    1.075055   0.824880    1.560002  1.000000e-05
92    92.0   0.989502    1.069041   0.825500    1.562408  1.000000e-06
93    93.0   0.988159    1.068965   0.824632    1.568480  1.000000e-06
94    94.0   0.988647    1.072281   0.826189    1.559668  1.000000e-06
95    95.0   0.988037    1.069624   0.825445    1.563913  1.000000e-06
96    96.0   0.988403    1.067355   0.825500    1.564405  1.000000e-06
97    97.0   0.988159    1.071250   0.825073    1.566167  1.000000e-06
98    98.0   0.988525    1.068556   0.825624    1.563772  1.000000e-06
99    99.0   0.988892    1.069577   0.827856    1.559754  1.000000e-06
100  100.0   0.987305    1.071004   0.826492    1.560490  1.000000e-06
101  101.0   0.989502    1.069857   0.824632    1.565396  1.000000e-06
102  102.0   0.986816    1.071534   0.824508    1.560661  1.000000e-06
103  103.0   0.989868    1.069010   0.826933    1.563220  1.000000e-06
104  104.0   0.989258    1.066073   0.826616    1.563336  1.000000e-07

EXPERIMENT: C-58
================

NAME: ghost_20200923083648

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'ReduceLROnPlateau',
 'lr_scheduler_params': "{'factor': 0.1, 'patience': 5, 'threshold': 0.001, "
                        "'min_lr': 1e-07}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 4153092.0,
 'optimizer': 'AdamW',
 'out_channels': 1280.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss            lr
0     0.0   0.006836    5.449575   0.007456    8.617170  1.000000e-03
1     1.0   0.009277    5.380211   0.011409    5.264784  1.000000e-03
2     2.0   0.008911    5.327161   0.010169    5.195281  1.000000e-03
3     3.0   0.010376    5.249767   0.018477    5.255176  1.000000e-03
4     4.0   0.016968    5.172492   0.016865    5.129055  1.000000e-03
5     5.0   0.019531    5.114929   0.016617    5.111562  1.000000e-03
6     6.0   0.023804    5.052705   0.028646    5.818511  1.000000e-03
7     7.0   0.026978    4.996769   0.018353    5.344427  1.000000e-03
8     8.0   0.031128    4.908484   0.031622    4.947857  1.000000e-03
9     9.0   0.044189    4.798406   0.036086    4.921706  1.000000e-03
10   10.0   0.046875    4.684481   0.046007    4.700737  1.000000e-03
11   11.0   0.058594    4.572890   0.027654    5.641273  1.000000e-03
12   12.0   0.075317    4.462334   0.058795    4.566359  1.000000e-03
13   13.0   0.088013    4.349849   0.080635    4.492888  1.000000e-03
14   14.0   0.106445    4.221910   0.057044    4.758955  1.000000e-03
15   15.0   0.119995    4.118992   0.094787    4.352783  1.000000e-03
16   16.0   0.140625    4.017354   0.091469    4.395788  1.000000e-03
17   17.0   0.161133    3.890088   0.126821    4.145440  1.000000e-03
18   18.0   0.193970    3.734773   0.142228    4.052400  1.000000e-03
19   19.0   0.223633    3.600327   0.203666    3.737566  1.000000e-03
20   20.0   0.258423    3.466200   0.236634    3.559433  1.000000e-03
21   21.0   0.288452    3.346695   0.264957    3.454686  1.000000e-03
22   22.0   0.331177    3.208347   0.232473    3.628030  1.000000e-03
23   23.0   0.367798    3.064828   0.296098    3.356893  1.000000e-03
24   24.0   0.405640    2.949854   0.320818    3.251004  1.000000e-03
25   25.0   0.437866    2.820266   0.381630    3.036245  1.000000e-03
26   26.0   0.459717    2.733499   0.411135    2.916267  1.000000e-03
27   27.0   0.498413    2.619484   0.386675    3.014525  1.000000e-03
28   28.0   0.523438    2.516216   0.369244    3.121232  1.000000e-03
29   29.0   0.550903    2.441667   0.381491    3.017671  1.000000e-03
30   30.0   0.577026    2.339103   0.393293    3.076511  1.000000e-03
31   31.0   0.611816    2.270619   0.414165    2.922305  1.000000e-03
32   32.0   0.622925    2.219384   0.458606    2.781989  1.000000e-03
33   33.0   0.641357    2.155102   0.399711    3.058712  1.000000e-03
34   34.0   0.672607    2.074758   0.409577    3.011260  1.000000e-03
35   35.0   0.673340    2.060211   0.406921    2.999139  1.000000e-03
36   36.0   0.694702    1.996227   0.536531    2.510634  1.000000e-03
37   37.0   0.709229    1.952712   0.497883    2.661874  1.000000e-03
38   38.0   0.721558    1.910601   0.480513    2.689303  1.000000e-03
39   39.0   0.724121    1.902270   0.498754    2.644525  1.000000e-03
40   40.0   0.753052    1.823056   0.463442    2.783239  1.000000e-03
41   41.0   0.748413    1.827687   0.443268    2.838745  1.000000e-03
42   42.0   0.756104    1.802576   0.475495    2.717775  1.000000e-03
43   43.0   0.885986    1.455347   0.739964    1.854007  1.000000e-04
44   44.0   0.936279    1.325195   0.743630    1.840333  1.000000e-04
45   45.0   0.944946    1.292552   0.754721    1.808132  1.000000e-04
46   46.0   0.952271    1.269766   0.755233    1.799273  1.000000e-04
47   47.0   0.960571    1.243815   0.759766    1.791796  1.000000e-04
48   48.0   0.960449    1.227071   0.759766    1.788727  1.000000e-04
49   49.0   0.965088    1.215480   0.765897    1.785590  1.000000e-04
50   50.0   0.967651    1.198001   0.768695    1.775583  1.000000e-04
51   51.0   0.972656    1.189470   0.767842    1.772409  1.000000e-04
52   52.0   0.970337    1.181762   0.772703    1.758301  1.000000e-04
53   53.0   0.974487    1.173787   0.764216    1.776409  1.000000e-04
54   54.0   0.975342    1.161563   0.771369    1.757123  1.000000e-04
55   55.0   0.978882    1.152368   0.768556    1.753299  1.000000e-04
56   56.0   0.979614    1.144655   0.771989    1.762982  1.000000e-04
57   57.0   0.980469    1.136369   0.771036    1.758306  1.000000e-04
58   58.0   0.982666    1.129250   0.776825    1.737454  1.000000e-04
59   59.0   0.982910    1.120906   0.779429    1.750228  1.000000e-04
60   60.0   0.984619    1.115327   0.772703    1.759729  1.000000e-04
61   61.0   0.986206    1.105946   0.773625    1.759414  1.000000e-04
62   62.0   0.988647    1.104759   0.774880    1.736300  1.000000e-04
63   63.0   0.986694    1.096567   0.774578    1.751766  1.000000e-04
64   64.0   0.987793    1.092320   0.773035    1.750162  1.000000e-04
65   65.0   0.990479    1.069861   0.777871    1.731339  1.000000e-05
66   66.0   0.992432    1.064815   0.781096    1.734992  1.000000e-05
67   67.0   0.992798    1.063155   0.781468    1.730763  1.000000e-05
68   68.0   0.992065    1.062065   0.781964    1.733474  1.000000e-05
69   69.0   0.990234    1.064147   0.784072    1.722535  1.000000e-05
70   70.0   0.991333    1.061233   0.784142    1.731983  1.000000e-05
71   71.0   0.992920    1.058886   0.783824    1.720105  1.000000e-05
72   72.0   0.992920    1.058291   0.785010    1.727553  1.000000e-05
73   73.0   0.991943    1.058881   0.786126    1.721899  1.000000e-05
74   74.0   0.992676    1.057658   0.784002    1.730967  1.000000e-05
75   75.0   0.993164    1.054379   0.788288    1.721836  1.000000e-05
76   76.0   0.993652    1.053535   0.784994    1.722760  1.000000e-05
77   77.0   0.992676    1.053968   0.785064    1.727839  1.000000e-05
78   78.0   0.992554    1.049530   0.787242    1.724518  1.000000e-06
79   79.0   0.993652    1.051243   0.783878    1.725886  1.000000e-06
80   80.0   0.992676    1.051067   0.785560    1.729849  1.000000e-06
81   81.0   0.993408    1.049997   0.784886    1.726492  1.000000e-06
82   82.0   0.994141    1.051512   0.787792    1.727761  1.000000e-06
83   83.0   0.992188    1.049837   0.785560    1.723538  1.000000e-06
84   84.0   0.992310    1.051215   0.785560    1.726107  1.000000e-07
85   85.0   0.994385    1.052691   0.786180    1.726919  1.000000e-07
86   86.0   0.993042    1.051018   0.786800    1.724650  1.000000e-07

EXPERIMENT: C-63
================

NAME: ghost_20200923172308

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'LambdaLR',
 'lr_scheduler_params': "{'lr_lambda': 'lambda epoch: pow(0.0001/0.001, 1/67) "
                        "** epoch'}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0      0.0   0.006836    5.341644   0.010541    5.336372  0.001000
1      1.0   0.011230    5.249367   0.012153    5.199307  0.000966
2      2.0   0.015503    5.173629   0.016865    5.196189  0.000934
3      3.0   0.017090    5.114971   0.017485    5.121389  0.000902
4      4.0   0.021362    5.051901   0.024802    5.058632  0.000872
5      5.0   0.026123    4.996997   0.029142    5.018140  0.000842
6      6.0   0.029541    4.933092   0.022073    5.058482  0.000814
7      7.0   0.038940    4.838161   0.041047    4.845107  0.000786
8      8.0   0.047852    4.727159   0.047619    4.758071  0.000760
9      9.0   0.062256    4.595387   0.051657    4.691314  0.000734
10    10.0   0.071655    4.486357   0.069692    4.484673  0.000709
11    11.0   0.082031    4.376709   0.088657    4.370891  0.000685
12    12.0   0.101685    4.272865   0.076474    4.408388  0.000662
13    13.0   0.115723    4.170137   0.090967    4.372284  0.000640
14    14.0   0.124268    4.092800   0.112611    4.175647  0.000618
15    15.0   0.148071    3.984743   0.139624    4.018172  0.000597
16    16.0   0.171021    3.877925   0.162130    3.918303  0.000577
17    17.0   0.182129    3.807077   0.176173    3.870466  0.000558
18    18.0   0.210205    3.697607   0.196241    3.770311  0.000539
19    19.0   0.234985    3.599561   0.215647    3.670128  0.000520
20    20.0   0.250244    3.503942   0.209431    3.728805  0.000503
21    21.0   0.282715    3.401197   0.263227    3.463223  0.000486
22    22.0   0.301025    3.323341   0.283059    3.364872  0.000470
23    23.0   0.335449    3.209156   0.321695    3.248433  0.000454
24    24.0   0.350464    3.125206   0.292556    3.307166  0.000438
25    25.0   0.391846    3.005416   0.357829    3.148230  0.000424
26    26.0   0.422974    2.899310   0.347026    3.135012  0.000409
27    27.0   0.441895    2.833269   0.373049    3.050469  0.000395
28    28.0   0.468262    2.747778   0.413485    2.906139  0.000382
29    29.0   0.509644    2.644366   0.427365    2.878367  0.000369
30    30.0   0.527588    2.566282   0.416095    2.877348  0.000357
31    31.0   0.544312    2.490388   0.444726    2.807157  0.000345
32    32.0   0.580200    2.404204   0.459089    2.751979  0.000333
33    33.0   0.604614    2.338175   0.468535    2.717107  0.000322
34    34.0   0.620728    2.281429   0.488570    2.662132  0.000311
35    35.0   0.644043    2.209103   0.505072    2.572826  0.000300
36    36.0   0.667847    2.122171   0.537057    2.504913  0.000290
37    37.0   0.690918    2.070828   0.521224    2.580181  0.000280
38    38.0   0.710449    2.018782   0.538926    2.493932  0.000271
39    39.0   0.729614    1.962238   0.523301    2.519996  0.000262
40    40.0   0.742065    1.906724   0.551769    2.426316  0.000253
41    41.0   0.775513    1.838545   0.561326    2.404424  0.000244
42    42.0   0.792114    1.786620   0.579507    2.342509  0.000236
43    43.0   0.797852    1.759243   0.580902    2.350386  0.000228
44    44.0   0.806519    1.719774   0.572224    2.372815  0.000220
45    45.0   0.835815    1.657050   0.590877    2.324575  0.000213
46    46.0   0.843262    1.624593   0.581894    2.355929  0.000206
47    47.0   0.862793    1.584463   0.589210    2.323009  0.000199
48    48.0   0.873291    1.548986   0.606114    2.265816  0.000192
49    49.0   0.884399    1.514258   0.603386    2.287017  0.000186
50    50.0   0.884888    1.492678   0.616879    2.239316  0.000179
51    51.0   0.901611    1.457555   0.616313    2.240117  0.000173
52    52.0   0.904419    1.430801   0.618600    2.236256  0.000167
53    53.0   0.918823    1.399486   0.627970    2.220255  0.000162
54    54.0   0.924805    1.378897   0.628575    2.203331  0.000156
55    55.0   0.929077    1.361056   0.634349    2.200020  0.000151
56    56.0   0.935425    1.341218   0.646323    2.155900  0.000146
57    57.0   0.942017    1.316189   0.633977    2.183505  0.000141
58    58.0   0.953125    1.288851   0.632543    2.199746  0.000136
59    59.0   0.956543    1.272260   0.648347    2.150353  0.000132
60    60.0   0.955322    1.265500   0.632132    2.200044  0.000127
61    61.0   0.956299    1.259138   0.645029    2.179458  0.000123
62    62.0   0.965820    1.233863   0.645842    2.158907  0.000119
63    63.0   0.963989    1.229759   0.661864    2.109298  0.000115
64    64.0   0.966919    1.212651   0.650996    2.132458  0.000111
65    65.0   0.971680    1.201963   0.659786    2.109169  0.000107
66    66.0   0.972900    1.193287   0.661646    2.104722  0.000103
67    67.0   0.974609    1.182899   0.664855    2.111113  0.000100
68    68.0   0.975342    1.177174   0.671636    2.091531  0.000097
69    69.0   0.976196    1.170893   0.676170    2.077948  0.000093
70    70.0   0.980713    1.154551   0.660282    2.128911  0.000090
71    71.0   0.979614    1.147005   0.675039    2.078835  0.000087
72    72.0   0.982056    1.142100   0.675163    2.090814  0.000084
73    73.0   0.984741    1.134408   0.672047    2.100999  0.000081
74    74.0   0.982300    1.130515   0.663630    2.102849  0.000079
75    75.0   0.985596    1.119537   0.676968    2.061996  0.000076
76    76.0   0.986572    1.118603   0.675589    2.063100  0.000073
77    77.0   0.986572    1.107020   0.670768    2.098510  0.000071
78    78.0   0.987915    1.099554   0.673248    2.093076  0.000069
79    79.0   0.989014    1.097807   0.671125    2.092834  0.000066
80    80.0   0.988647    1.094314   0.680162    2.069035  0.000064
81    81.0   0.990356    1.091056   0.680882    2.065895  0.000062
82    82.0   0.990112    1.087572   0.680867    2.073121  0.000060
83    83.0   0.990234    1.081240   0.673923    2.091812  0.000058
84    84.0   0.989136    1.082176   0.679627    2.071325  0.000056
85    85.0   0.990479    1.074099   0.683789    2.060861  0.000054
86    86.0   0.992188    1.072215   0.686269    2.061596  0.000052
87    87.0   0.991821    1.068980   0.685455    2.064956  0.000050
88    88.0   0.993408    1.060126   0.689741    2.071162  0.000049
89    89.0   0.992554    1.061230   0.689315    2.050521  0.000047
90    90.0   0.992065    1.059022   0.687703    2.065308  0.000045
91    91.0   0.993774    1.054693   0.687757    2.055029  0.000044
92    92.0   0.993896    1.051496   0.691477    2.054964  0.000042
93    93.0   0.992798    1.052648   0.686036    2.057239  0.000041
94    94.0   0.993164    1.045834   0.683610    2.076670  0.000040
95    95.0   0.993042    1.045623   0.687772    2.042625  0.000038
96    96.0   0.994751    1.044898   0.693337    2.050629  0.000037
97    97.0   0.994263    1.039377   0.697569    2.053783  0.000036
98    98.0   0.995361    1.035791   0.688640    2.070356  0.000034
99    99.0   0.995117    1.034261   0.687951    2.062489  0.000033
100  100.0   0.993896    1.036654   0.695833    2.047526  0.000032
101  101.0   0.995239    1.032808   0.695709    2.055444  0.000031
102  102.0   0.994507    1.032315   0.695515    2.059391  0.000030
103  103.0   0.994751    1.029132   0.696383    2.045247  0.000029
104  104.0   0.995483    1.027946   0.691973    2.049689  0.000028
105  105.0   0.995605    1.025936   0.693903    2.050436  0.000027
106  106.0   0.995483    1.022686   0.693105    2.045318  0.000026
107  107.0   0.996094    1.022932   0.697182    2.055447  0.000025
108  108.0   0.995361    1.022142   0.698243    2.044840  0.000024
109  109.0   0.995483    1.019163   0.697251    2.041922  0.000024
110  110.0   0.995483    1.018246   0.698065    2.046227  0.000023
111  111.0   0.995239    1.017658   0.696383    2.051637  0.000022
112  112.0   0.995728    1.015865   0.695019    2.044278  0.000021
113  113.0   0.996216    1.013280   0.699181    2.042083  0.000021
114  114.0   0.996582    1.014266   0.696453    2.049727  0.000020
115  115.0   0.996094    1.010685   0.700173    2.041473  0.000019
116  116.0   0.997070    1.008849   0.701344    2.040012  0.000019
117  117.0   0.997070    1.007971   0.701165    2.040478  0.000018
118  118.0   0.995850    1.011073   0.701716    2.038705  0.000017
119  119.0   0.996826    1.008738   0.700049    2.043621  0.000017
120  120.0   0.996216    1.008314   0.702033    2.049839  0.000016
121  121.0   0.996948    1.007916   0.701344    2.039139  0.000016
122  122.0   0.996216    1.006821   0.699305    2.041412  0.000015
123  123.0   0.996460    1.006059   0.699677    2.045629  0.000015
124  124.0   0.995605    1.004109   0.699979    2.042002  0.000014
125  125.0   0.996826    1.003270   0.703382    2.038520  0.000014
126  126.0   0.996826    1.002750   0.702281    2.047215  0.000013
127  127.0   0.996826    1.005855   0.700545    2.043586  0.000013
128  128.0   0.996460    1.002390   0.705064    2.040401  0.000012
129  129.0   0.996582    1.000421   0.702638    2.043717  0.000012
130  130.0   0.997070    1.000695   0.703397    2.049235  0.000011
131  131.0   0.996948    1.000407   0.702777    2.039594  0.000011
132  132.0   0.997803    1.000062   0.702088    2.046125  0.000011
133  133.0   0.996948    0.997173   0.704513    2.046156  0.000010
134  134.0   0.997192    0.996200   0.703397    2.042492  0.000010
135  135.0   0.995850    0.998825   0.701165    2.041005  0.000010
136  136.0   0.996704    0.995051   0.702529    2.042157  0.000009
137  137.0   0.997192    0.996881   0.704072    2.045811  0.000009
138  138.0   0.997437    0.994462   0.702832    2.045707  0.000009
139  139.0   0.996948    0.999803   0.703645    2.039138  0.000008
140  140.0   0.998047    0.993829   0.704513    2.042314  0.000008

EXPERIMENT: C-64
================

NAME: ghost_20200923200841

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'LambdaLR',
 'lr_scheduler_params': "{'lr_lambda': 'lambda epoch: 0.955 ** epoch'}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0      0.0   0.008179    5.322639   0.007688    5.246077  0.001000
1      1.0   0.008667    5.239359   0.010789    5.195228  0.000955
2      2.0   0.011475    5.192300   0.014633    5.180882  0.000912
3      3.0   0.013550    5.154901   0.016687    5.183117  0.000871
4      4.0   0.020142    5.104402   0.022089    5.090458  0.000832
5      5.0   0.023926    5.049687   0.022515    5.087637  0.000794
6      6.0   0.026978    4.996123   0.026235    5.054377  0.000759
7      7.0   0.035400    4.925839   0.034008    4.921810  0.000724
8      8.0   0.038696    4.856456   0.034683    4.886074  0.000692
9      9.0   0.047729    4.765473   0.043735    4.761674  0.000661
10    10.0   0.058350    4.666788   0.043472    4.772062  0.000631
11    11.0   0.067017    4.566506   0.064514    4.560758  0.000603
12    12.0   0.074951    4.455406   0.078334    4.474381  0.000575
13    13.0   0.085571    4.348905   0.066111    4.444746  0.000550
14    14.0   0.105713    4.244536   0.084093    4.412358  0.000525
15    15.0   0.123291    4.142185   0.093726    4.358845  0.000501
16    16.0   0.142944    4.041998   0.129014    4.109573  0.000479
17    17.0   0.157837    3.946609   0.132292    4.040828  0.000457
18    18.0   0.172974    3.843886   0.155373    3.939763  0.000437
19    19.0   0.188354    3.764309   0.199713    3.752689  0.000417
20    20.0   0.219727    3.659359   0.191931    3.740216  0.000398
21    21.0   0.240234    3.567122   0.214748    3.676471  0.000380
22    22.0   0.256836    3.483418   0.224817    3.608316  0.000363
23    23.0   0.294434    3.376348   0.281286    3.428117  0.000347
24    24.0   0.318481    3.288655   0.280914    3.389268  0.000331
25    25.0   0.349365    3.199436   0.258027    3.456225  0.000316
26    26.0   0.363892    3.116248   0.293563    3.318897  0.000302
27    27.0   0.396118    3.025962   0.335919    3.206524  0.000288
28    28.0   0.411255    2.946014   0.352839    3.159387  0.000275
29    29.0   0.454102    2.854448   0.366719    3.109644  0.000263
30    30.0   0.463379    2.784790   0.397240    2.998868  0.000251
31    31.0   0.494507    2.702043   0.394040    2.999518  0.000240
32    32.0   0.535034    2.599931   0.419492    2.907881  0.000229
33    33.0   0.548828    2.535553   0.441967    2.836769  0.000219
34    34.0   0.567505    2.474896   0.436892    2.839919  0.000209
35    35.0   0.583374    2.407846   0.443023    2.817189  0.000200
36    36.0   0.605713    2.345356   0.469125    2.736814  0.000191
37    37.0   0.628296    2.286040   0.473272    2.731547  0.000182
38    38.0   0.657227    2.211061   0.488083    2.673105  0.000174
39    39.0   0.682617    2.144290   0.482007    2.692288  0.000166
40    40.0   0.700806    2.090290   0.499383    2.661479  0.000159
41    41.0   0.710571    2.045551   0.521208    2.578466  0.000151
42    42.0   0.731445    1.990550   0.530796    2.543311  0.000145
43    43.0   0.750610    1.941966   0.527215    2.545133  0.000138
44    44.0   0.768311    1.900550   0.528594    2.552207  0.000132
45    45.0   0.789062    1.849010   0.538600    2.501079  0.000126
46    46.0   0.790405    1.812135   0.542235    2.498561  0.000120
47    47.0   0.813110    1.771450   0.546258    2.489912  0.000115
48    48.0   0.831421    1.726342   0.547661    2.478674  0.000110
49    49.0   0.842285    1.697050   0.553272    2.451385  0.000105
50    50.0   0.843506    1.667142   0.556303    2.442779  0.000100
51    51.0   0.848145    1.649880   0.564309    2.436221  0.000096
52    52.0   0.861938    1.602706   0.565549    2.436682  0.000091
53    53.0   0.880127    1.575435   0.569889    2.411263  0.000087
54    54.0   0.881104    1.553709   0.567106    2.412509  0.000083
55    55.0   0.892578    1.530362   0.571268    2.408487  0.000079
56    56.0   0.901245    1.511259   0.570564    2.388486  0.000076
57    57.0   0.900757    1.496879   0.576144    2.392413  0.000072
58    58.0   0.908203    1.474197   0.577345    2.396640  0.000069
59    59.0   0.907349    1.463621   0.584452    2.383676  0.000066
60    60.0   0.917358    1.444370   0.587553    2.363788  0.000063
61    61.0   0.922729    1.423101   0.578089    2.379123  0.000060
62    62.0   0.928711    1.410938   0.583600    2.360796  0.000058
63    63.0   0.928223    1.405632   0.581283    2.377360  0.000055
64    64.0   0.933594    1.379347   0.585762    2.359420  0.000053
65    65.0   0.935059    1.382445   0.586313    2.371423  0.000050
66    66.0   0.940552    1.366007   0.590683    2.354527  0.000048
67    67.0   0.943115    1.353758   0.591482    2.349115  0.000046
68    68.0   0.949463    1.345640   0.595078    2.354184  0.000044
69    69.0   0.948975    1.329325   0.591094    2.358190  0.000042
70    70.0   0.949829    1.325858   0.592404    2.366347  0.000040
71    71.0   0.952393    1.317909   0.592924    2.345163  0.000038
72    72.0   0.958374    1.300706   0.595559    2.343076  0.000036
73    73.0   0.959351    1.295876   0.597295    2.345259  0.000035
74    74.0   0.962402    1.289092   0.600132    2.336282  0.000033
75    75.0   0.961792    1.281697   0.595985    2.336172  0.000032
76    76.0   0.961426    1.278594   0.599666    2.345075  0.000030
77    77.0   0.962891    1.275571   0.601333    2.340410  0.000029
78    78.0   0.963745    1.266451   0.598659    2.336267  0.000028
79    79.0   0.968262    1.256271   0.602697    2.327064  0.000026
80    80.0   0.968018    1.254294   0.600395    2.341138  0.000025
81    81.0   0.970703    1.250955   0.600837    2.327047  0.000024
82    82.0   0.966064    1.251869   0.597295    2.344428  0.000023
83    83.0   0.969360    1.245491   0.604874    2.327174  0.000022
84    84.0   0.969116    1.239247   0.598356    2.329895  0.000021
85    85.0   0.973877    1.232421   0.601030    2.330245  0.000020
86    86.0   0.975220    1.231473   0.602751    2.324939  0.000019
87    87.0   0.971558    1.228043   0.601333    2.326235  0.000018
88    88.0   0.973755    1.225955   0.597186    2.328538  0.000017
89    89.0   0.976562    1.219860   0.601333    2.326627  0.000017
90    90.0   0.974243    1.222040   0.602146    2.329222  0.000016
91    91.0   0.974854    1.217227   0.602999    2.324713  0.000015
92    92.0   0.975098    1.215570   0.601720    2.325452  0.000014
93    93.0   0.977295    1.213208   0.600643    2.330749  0.000014
94    94.0   0.978638    1.209848   0.604061    2.318676  0.000013
95    95.0   0.975342    1.210014   0.601442    2.322793  0.000013
96    96.0   0.981445    1.199821   0.600465    2.324409  0.000012
97    97.0   0.977417    1.201888   0.600821    2.326999  0.000011
98    98.0   0.979492    1.198793   0.601953    2.322532  0.000011
99    99.0   0.978394    1.199820   0.601968    2.322989  0.000010
100  100.0   0.978760    1.197863   0.604433    2.322728  0.000010
101  101.0   0.979126    1.197956   0.603014    2.323808  0.000010
102  102.0   0.981323    1.195274   0.602558    2.320973  0.000009
103  103.0   0.979614    1.190605   0.605975    2.314683  0.000009
104  104.0   0.980591    1.187197   0.603332    2.322377  0.000008
105  105.0   0.980469    1.188078   0.605549    2.318587  0.000008
106  106.0   0.980957    1.191107   0.603262    2.326342  0.000008
107  107.0   0.979736    1.187280   0.605673    2.317415  0.000007
108  108.0   0.979370    1.188800   0.602007    2.331574  0.000007
109  109.0   0.980103    1.184256   0.604006    2.321511  0.000007
110  110.0   0.983398    1.181411   0.604254    2.327366  0.000006
111  111.0   0.980225    1.178829   0.606471    2.314137  0.000006
112  112.0   0.984131    1.180691   0.604076    2.321044  0.000006
113  113.0   0.982544    1.180748   0.604185    2.322530  0.000006
114  114.0   0.982178    1.178739   0.603084    2.329933  0.000005
115  115.0   0.982300    1.175399   0.602077    2.324376  0.000005
116  116.0   0.981323    1.179386   0.606967    2.314142  0.000005
117  117.0   0.981567    1.179475   0.604006    2.336601  0.000005
118  118.0   0.983154    1.172617   0.604557    2.319856  0.000004
119  119.0   0.984131    1.171401   0.601829    2.331795  0.000004
120  120.0   0.983887    1.173676   0.603952    2.322888  0.000004
121  121.0   0.984863    1.173304   0.605246    2.315791  0.000004
122  122.0   0.983643    1.177030   0.605425    2.321512  0.000004
123  123.0   0.983765    1.171333   0.604502    2.322953  0.000003
124  124.0   0.984375    1.170853   0.606928    2.316027  0.000003
125  125.0   0.983521    1.170965   0.603014    2.328494  0.000003
126  126.0   0.984009    1.169446   0.603634    2.322145  0.000003

EXPERIMENT: C-65
================

NAME: ghost_20200923201033

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'LambdaLR',
 'lr_scheduler_params': "{'lr_lambda': 'lambda epoch: 0.975 ** epoch'}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'colab',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

     epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0      0.0   0.006470    5.344137   0.009549    5.473823  0.001000
1      1.0   0.010864    5.247802   0.013834    5.194844  0.000975
2      2.0   0.013550    5.191469   0.015516    5.198911  0.000951
3      3.0   0.016113    5.138616   0.020407    5.130345  0.000927
4      4.0   0.018799    5.083341   0.020213    5.094924  0.000904
5      5.0   0.022583    5.049876   0.018919    5.086789  0.000881
6      6.0   0.026245    4.997996   0.028343    5.004228  0.000859
7      7.0   0.031738    4.927137   0.029420    4.922099  0.000838
8      8.0   0.036865    4.840380   0.038790    4.820726  0.000817
9      9.0   0.045898    4.748688   0.046836    4.714782  0.000796
10    10.0   0.052979    4.643820   0.059926    4.609014  0.000776
11    11.0   0.063965    4.539556   0.072079    4.547149  0.000757
12    12.0   0.071289    4.443955   0.081767    4.428454  0.000738
13    13.0   0.088989    4.333861   0.099733    4.285763  0.000720
14    14.0   0.102417    4.230345   0.066414    4.627458  0.000702
15    15.0   0.125854    4.121187   0.125968    4.107652  0.000684
16    16.0   0.132935    4.030778   0.148250    3.969421  0.000667
17    17.0   0.158325    3.921320   0.150661    3.968981  0.000650
18    18.0   0.169678    3.819144   0.154063    3.913928  0.000634
19    19.0   0.201904    3.708803   0.178042    3.819408  0.000618
20    20.0   0.217163    3.603246   0.195636    3.706870  0.000603
21    21.0   0.248169    3.503235   0.245572    3.546536  0.000588
22    22.0   0.276367    3.402774   0.258051    3.511499  0.000573
23    23.0   0.305054    3.294092   0.293121    3.338660  0.000559
24    24.0   0.339966    3.185271   0.273141    3.405550  0.000545
25    25.0   0.361084    3.094773   0.353032    3.128818  0.000531
26    26.0   0.392578    2.984945   0.312606    3.238154  0.000518
27    27.0   0.426514    2.880377   0.372889    3.041118  0.000505
28    28.0   0.449707    2.802261   0.426064    2.900243  0.000492
29    29.0   0.473267    2.699989   0.371098    3.063804  0.000480
30    30.0   0.508179    2.606472   0.454873    2.768190  0.000468
31    31.0   0.530640    2.537779   0.440984    2.833802  0.000456
32    32.0   0.556763    2.455586   0.458197    2.743001  0.000445
33    33.0   0.576538    2.390851   0.452547    2.761563  0.000434
34    34.0   0.604980    2.322781   0.488773    2.638657  0.000423
35    35.0   0.627319    2.244603   0.518550    2.515635  0.000412
36    36.0   0.655640    2.154006   0.491377    2.634556  0.000402
37    37.0   0.664185    2.118015   0.541352    2.450618  0.000392
38    38.0   0.681519    2.054480   0.560604    2.416186  0.000382
39    39.0   0.706177    1.988985   0.583460    2.344707  0.000373
40    40.0   0.724609    1.933067   0.566819    2.395380  0.000363
41    41.0   0.732178    1.888366   0.570881    2.370929  0.000354
42    42.0   0.764404    1.814941   0.562960    2.406537  0.000345
43    43.0   0.791016    1.763886   0.585018    2.333996  0.000337
44    44.0   0.794434    1.735720   0.593938    2.316047  0.000328
45    45.0   0.808472    1.680315   0.603952    2.273906  0.000320
46    46.0   0.824463    1.647399   0.611764    2.252529  0.000312
47    47.0   0.847900    1.589760   0.623669    2.193320  0.000304
48    48.0   0.848999    1.569924   0.623228    2.224138  0.000297
49    49.0   0.865723    1.534858   0.610440    2.248889  0.000289
50    50.0   0.876099    1.491771   0.635931    2.168603  0.000282
51    51.0   0.883911    1.469868   0.617073    2.227995  0.000275
52    52.0   0.890625    1.444215   0.643169    2.138973  0.000268
53    53.0   0.896606    1.418317   0.627459    2.211683  0.000261
54    54.0   0.904175    1.400907   0.639324    2.161374  0.000255
55    55.0   0.918335    1.354969   0.648075    2.126822  0.000248
56    56.0   0.922974    1.342479   0.623569    2.230635  0.000242
57    57.0   0.929688    1.325046   0.653709    2.123105  0.000236
58    58.0   0.933594    1.311293   0.650524    2.120088  0.000230
59    59.0   0.942505    1.279715   0.669334    2.067553  0.000225
60    60.0   0.947021    1.269894   0.667390    2.092389  0.000219
61    61.0   0.946289    1.264696   0.671706    2.055029  0.000213
62    62.0   0.954590    1.228648   0.678813    2.020326  0.000208
63    63.0   0.958130    1.218847   0.681348    2.044059  0.000203
64    64.0   0.957764    1.221343   0.665545    2.092009  0.000198
65    65.0   0.959229    1.203911   0.681433    2.024778  0.000193
66    66.0   0.965942    1.193131   0.672241    2.048955  0.000188
67    67.0   0.970703    1.171951   0.681115    2.056722  0.000183
68    68.0   0.970581    1.170757   0.670877    2.072508  0.000179
69    69.0   0.968506    1.168889   0.675659    2.043532  0.000174
70    70.0   0.973755    1.146409   0.678689    2.037906  0.000170
71    71.0   0.978027    1.129488   0.688540    2.004860  0.000166
72    72.0   0.980103    1.125729   0.686819    2.010371  0.000162
73    73.0   0.978638    1.128228   0.693377    1.991383  0.000158
74    74.0   0.977783    1.119900   0.685207    2.021821  0.000154
75    75.0   0.982056    1.108267   0.684076    2.042945  0.000150
76    76.0   0.982910    1.110590   0.689160    2.010405  0.000146
77    77.0   0.983643    1.093239   0.686611    2.019984  0.000142
78    78.0   0.983643    1.097568   0.701150    1.983092  0.000139
79    79.0   0.986328    1.082674   0.694136    2.003015  0.000135
80    80.0   0.987305    1.074186   0.698461    1.979849  0.000132
81    81.0   0.984741    1.082013   0.697375    2.009802  0.000129
82    82.0   0.987549    1.070907   0.692648    1.998626  0.000125
83    83.0   0.988159    1.058858   0.691834    2.021200  0.000122
84    84.0   0.989990    1.049739   0.703878    2.000050  0.000119
85    85.0   0.988037    1.052680   0.714543    1.947654  0.000116
86    86.0   0.988892    1.056564   0.705281    1.975069  0.000113
87    87.0   0.988159    1.052654   0.707514    1.963900  0.000111
88    88.0   0.992432    1.040301   0.708397    1.954692  0.000108
89    89.0   0.993042    1.034124   0.707955    1.967792  0.000105
90    90.0   0.990967    1.032525   0.709637    1.958378  0.000102
91    91.0   0.991333    1.034554   0.699290    1.998159  0.000100
92    92.0   0.994385    1.020122   0.710242    1.959784  0.000097
93    93.0   0.993164    1.020536   0.715520    1.953511  0.000095
94    94.0   0.994751    1.014090   0.709513    1.966162  0.000093
95    95.0   0.991943    1.018918   0.712256    1.953898  0.000090
96    96.0   0.991333    1.016251   0.717876    1.957893  0.000088
97    97.0   0.992432    1.025011   0.718829    1.935848  0.000086
98    98.0   0.993896    1.008293   0.727176    1.907410  0.000084
99    99.0   0.994263    1.004477   0.713729    1.965719  0.000082
100  100.0   0.994507    1.004248   0.720247    1.942892  0.000080
101  101.0   0.993774    1.007372   0.725083    1.914424  0.000078
102  102.0   0.994385    0.995374   0.713923    1.937295  0.000076
103  103.0   0.996094    0.995608   0.721185    1.921446  0.000074
104  104.0   0.995361    0.993968   0.712063    1.944319  0.000072
105  105.0   0.994385    0.994368   0.722053    1.931230  0.000070
106  106.0   0.996460    0.988623   0.724533    1.914328  0.000068
107  107.0   0.994995    0.991926   0.717217    1.932653  0.000067
108  108.0   0.996582    0.983909   0.723665    1.936634  0.000065
109  109.0   0.996460    0.982187   0.727316    1.923683  0.000063
110  110.0   0.995850    0.982793   0.717380    1.946598  0.000062
111  111.0   0.996094    0.981597   0.719999    1.947252  0.000060
112  112.0   0.995117    0.978856   0.726944    1.916175  0.000059
113  113.0   0.996216    0.975142   0.730664    1.921041  0.000057

EXPERIMENT: C-66
================

NAME: ghost_20200923221134

PARAMETERS:

{'all_optimizer_params': "{'lr': 0.001, 'weight_decay': 0.6}",
 'architecture': 'GhostNet',
 'augment_images': 'True',
 'augment_tensors': 'False',
 'batch_size': 64.0,
 'crop_to_bboxes': 'False',
 'dropout': 0.2,
 'erase_background': 'False',
 'grayscale': 'False',
 'image_augmentations': "{'RandomHorizontalFlip': {'p': 0.5}, 'RandomAffine': "
                        "{'degrees': 25, 'translate': [0.1, 0.1], 'scale': "
                        "[0.9, 1.1], 'shear': 8}, 'ColorJitter': "
                        "{'brightness': 0.2, 'contrast': 0.2, 'saturation': "
                        "0.2, 'hue': 0.1}}",
 'img_size': '[227, 227]',
 'learning_rate': 0.001,
 'loss_function': 'LabelSmoothingCrossEntropy',
 'loss_params': 'None',
 'lr_scheduler': 'LambdaLR',
 'lr_scheduler_params': "{'lr_lambda': 'lambda epoch: 0.98 ** epoch'}",
 'max_num_epochs': 200.0,
 'norm_params_gray': 'None',
 'norm_params_rgb': "{'mean': [0.4707, 0.4602, 0.455], 'std': [0.2594, 0.2585, "
                    '0.2635]}',
 'normalize': 'True',
 'num_params': 3041412.0,
 'optimizer': 'AdamW',
 'out_channels': 320.0,
 'runtime': 'local',
 'tensor_augmentations': 'None',
 'weight_decay': 0.6}

METRICS:

    epoch  train_acc  train_loss  valid_acc  valid_loss        lr
0     0.0   0.006836    5.333632   0.011409    5.229222  0.001000
1     1.0   0.010498    5.255720   0.015253    5.192206  0.000980
2     2.0   0.013672    5.182755   0.012401    5.165679  0.000960
3     3.0   0.018555    5.131796   0.020089    5.154072  0.000941
4     4.0   0.021484    5.074630   0.018849    5.091649  0.000922
5     5.0   0.025146    5.017294   0.033234    4.966829  0.000904
6     6.0   0.031006    4.961844   0.032242    5.009003  0.000886
7     7.0   0.033691    4.878064   0.028894    4.968028  0.000868
8     8.0   0.042358    4.804445   0.030878    4.917457  0.000851
9     9.0   0.046265    4.722146   0.042550    4.745238  0.000834
10   10.0   0.058228    4.627789   0.048185    4.782997  0.000817
11   11.0   0.065430    4.508288   0.074257    4.486092  0.000801
12   12.0   0.074951    4.419795   0.068770    4.487046  0.000785
13   13.0   0.090088    4.319292   0.078830    4.398803  0.000769
14   14.0   0.102417    4.223101   0.101399    4.254522  0.000754
15   15.0   0.115601    4.129714   0.097041    4.349296  0.000739
16   16.0   0.133301    4.024830   0.136276    4.045393  0.000724
17   17.0   0.159302    3.905889   0.115908    4.243799  0.000709
18   18.0   0.174316    3.812909   0.174694    3.839635  0.000695
19   19.0   0.199707    3.700031   0.199861    3.718932  0.000681
20   20.0   0.225586    3.601081   0.213330    3.645486  0.000668
21   21.0   0.253906    3.478692   0.219848    3.627368  0.000654
22   22.0   0.277954    3.380315   0.224043    3.580765  0.000641
23   23.0   0.309326    3.270297   0.240007    3.521343  0.000628
24   24.0   0.330078    3.180774   0.282191    3.386449  0.000616
25   25.0   0.366943    3.063187   0.282750    3.364734  0.000603
26   26.0   0.388794    2.983869   0.286168    3.340526  0.000591
27   27.0   0.416382    2.882670   0.331476    3.157967  0.000580
28   28.0   0.445190    2.795004   0.374755    3.019609  0.000568
29   29.0   0.469971    2.693741   0.380236    3.017569  0.000557
30   30.0   0.492188    2.647539   0.393489    2.979880  0.000545
31   31.0   0.522949    2.539708   0.439844    2.814769  0.000535
32   32.0   0.542358    2.463633   0.449858    2.725424  0.000524
33   33.0   0.576172    2.370942   0.461128    2.706780  0.000513
34   34.0   0.603271    2.302191   0.494556    2.611957  0.000503
35   35.0   0.616577    2.239602   0.495950    2.637091  0.000493
36   36.0   0.637573    2.178423   0.529308    2.526263  0.000483
37   37.0   0.660034    2.118814   0.493784    2.631816  0.000474
38   38.0   0.679932    2.051892   0.507598    2.613571  0.000464
39   39.0   0.706055    1.988868   0.528268    2.518679  0.000455
40   40.0   0.721436    1.942243   0.552960    2.415195  0.000446
41   41.0   0.747925    1.869093   0.523540    2.532102  0.000437
42   42.0   0.743286    1.872221   0.591527    2.310081  0.000428
43   43.0   0.764038    1.808916   0.550892    2.447386  0.000419
44   44.0   0.782227    1.750959   0.574973    2.337826  0.000411
45   45.0   0.798218    1.712096   0.603713    2.238545  0.000403
46   46.0   0.818970    1.657655   0.600861    2.278503  0.000395
47   47.0   0.830444    1.611003   0.591566    2.281191  0.000387
48   48.0   0.835327    1.596384   0.615067    2.200815  0.000379
49   49.0   0.845581    1.574832   0.626676    2.183831  0.000372
50   50.0   0.862427    1.517591   0.605231    2.251164  0.000364
51   51.0   0.876831    1.475756   0.625792    2.195838  0.000357
52   52.0   0.884888    1.451268   0.609919    2.247422  0.000350
53   53.0   0.890503    1.425894   0.634969    2.167078  0.000343
54   54.0   0.887207    1.435413   0.625514    2.178193  0.000336
55   55.0   0.915527    1.374489   0.633596    2.152018  0.000329
56   56.0   0.911987    1.374337   0.659429    2.109318  0.000323
57   57.0   0.918091    1.349452   0.660917    2.075254  0.000316
58   58.0   0.924561    1.329457   0.633396    2.174239  0.000310
59   59.0   0.936035    1.294208   0.646834    2.133731  0.000304
60   60.0   0.927612    1.308524   0.659979    2.097541  0.000298
61   61.0   0.940308    1.276664   0.680495    2.036150  0.000292
62   62.0   0.942627    1.259274   0.666497    2.087489  0.000286
63   63.0   0.941406    1.255653   0.643789    2.122944  0.000280
64   64.0   0.944580    1.235621   0.669295    2.071523  0.000274
65   65.0   0.949829    1.236616   0.686602    2.017166  0.000269
66   66.0   0.954468    1.211295   0.687107    2.022860  0.000264
67   67.0   0.954834    1.206636   0.666576    2.043707  0.000258
68   68.0   0.957642    1.199749   0.643789    2.141397  0.000253
69   69.0   0.957397    1.189861   0.667444    2.088339  0.000248
70   70.0   0.963501    1.171464   0.688386    2.006618  0.000243
71   71.0   0.965942    1.170662   0.688695    1.989024  0.000238
72   72.0   0.969727    1.156306   0.681968    2.024325  0.000233
73   73.0   0.970825    1.135020   0.696298    1.975375  0.000229
74   74.0   0.968628    1.146875   0.700530    1.942327  0.000224
75   75.0   0.970581    1.135163   0.686750    2.013440  0.000220
76   76.0   0.971680    1.136435   0.672891    2.053674  0.000215
77   77.0   0.976196    1.122446   0.679077    2.051590  0.000211
78   78.0   0.975586    1.113963   0.682455    2.029263  0.000207
79   79.0   0.975220    1.114232   0.693888    1.988319  0.000203
80   80.0   0.980469    1.097052   0.675892    2.051804  0.000199
81   81.0   0.977905    1.095230   0.703700    1.955624  0.000195
82   82.0   0.982178    1.081237   0.686478    2.030061  0.000191
83   83.0   0.977661    1.097040   0.653600    2.157368  0.000187
84   84.0   0.983521    1.079700   0.693268    2.013041  0.000183
85   85.0   0.981812    1.080351   0.693268    2.011103  0.000180
86   86.0   0.980347    1.083527   0.695833    1.970055  0.000176
87   87.0   0.987183    1.057056   0.690213    1.990908  0.000172
88   88.0   0.983765    1.065336   0.701607    1.973827  0.000169
89   89.0   0.980347    1.069378   0.704607    1.953976  0.000166