# Logging settings:
loglevel: 'INFO'
logging_dir: 'logs'
log_to_neptune: False
neptune_username: '<neptune.ai username>'
neptune_project_name: '<neptune.ai project name>'
neptune_api_token: '<neptune.ai API token>'

# Train/test dataset and devkit location
stanford_raw_data_path: '<path to the folder containing: car_ims.tgz, cars_annos.mat, car_devkit.tgz>'
stanford_data_path: 'input/stanford'

# General data preprocessinng settings
image_size: [227, 227]
convert_to_grayscale: False
normalize: True  
normalization_params_rgb:  # Applied when 'convert_to_grayscale==False'
  mean: [0.4707, 0.4602, 0.4550]
  std: [0.2594, 0.2585, 0.2635]
normalization_params_grayscale:  # Applied when 'convert_to_grayscale==True'
  mean: [0.4627]
  std: [0.2545]

# Training data augmentation settings
augment_images: True
image_augmentations:  # to be applied consecutively
  RandomHorizontalFlip:  # has to be a valid transformation from 'torchvision.transforms'
    p: 0.5  # transformation parameters to be passed as '**dict'
  RandomAffine:
    degrees: 25
    translate: [0.1, 0.1]
    scale: [0.9, 1.1]
    shear: 8
augment_tensors: True
tensor_augmentations:  # to be applied consecutively
  RandomErasing:
    p: 0.5
    scale: [0.02, 0.25]

# Network and training settings
architecture: 'sq_10'  # Possible options in 'models.arch_dict'
batch_size: 16
num_epochs: 100

# Optimizer settings
optimizer: Adam  # valid optimizer from 'torch.optim'
optimizer_params:
  lr: 0.001
  weight_decay: 0
lr_scheduler: MultiStepLR  # # valid lr_scheduler from 'torch.optim' or None
lr_scheduler_params:  # scheduler parameters to be passed as '**dict'
  milestones: [15, 30, 45, 60, 75]
  gamma: 0.1

# Loss function settings
loss_function: CrossEntropyLoss  # valid loss function from 'torch.nn' or custom LabelSmoothingCrossEntropy
loss_params:  # loss parameters to be passed as '**dict'

# Output settings
output_path: 'output'
