# Logging settings:
loglevel: 'INFO'
logging_dir: 'logs'
log_to_neptune: False
neptune_username: '<neptune.ai username>'
neptune_project_name: '<neptune.ai project name>'
neptune_api_token: '<neptune.ai API token>'

# Train/test dataset and devkit location
stanford_raw_data_path: '<path to the folder containing: car_ims.tgz, cars_annos.mat, car_devkit.tgz>'
stanford_data_path: 'input/stanford'

# Output settings
output_path: 'output'

# General data preprocessinng settings
image_size: &img_size [227, 227]  # Anchor to use in augmentations if needed
convert_to_grayscale: False
normalize: True  
normalization_params_rgb:  # Applied when 'convert_to_grayscale==False'
  mean: [0.4707, 0.4602, 0.4550]
  std: [0.2594, 0.2585, 0.2635]
normalization_params_grayscale:  # Applied when 'convert_to_grayscale==True'
  mean: [0.4627]
  std: [0.2545]

# Training data augmentation settings
crop_to_bboxes: True  # crop training images using bounding boxes
erase_background: True  # erase background outside bboxes to preserve ratios (only if 'crop_to_bboxes==True') 
augment_images: True
image_augmentations:  # to be applied consecutively
  RandomHorizontalFlip:  # has to be a valid transformation from 'torchvision.transforms'
    p: 0.5  # transformation parameters to be passed as '**dict'
  RandomAffine:
    degrees: 25
    translate: [0.1, 0.1]
    scale: [0.9, 1.1]
    shear: 8
  ColorJitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
augment_tensors: False
tensor_augmentations:  # to be applied consecutively
  RandomErasing:
    p: 0.5
    scale: [0.02, 0.25]

# Network and training settings
architecture: 'ghost'  # Possible options in 'models.arch_dict'
batch_size: 64
num_epochs: 200

# Architecture modifications (right now GhostNet only!)
dropout: 0.2  # droupout rate before the last Linear layer
output_channels: 320  # output channels to be mapped to the number of classes

# Optimizer settings
optimizer: AdamW  # valid optimizer from 'torch.optim'
optimizer_params:
  lr: 0.001
  weight_decay: 0.6
lr_scheduler: MultiStepLR  # valid lr_scheduler from 'torch.optim' or None
lr_scheduler_params:  # scheduler parameters to be passed as '**dict'
  gamma: 0.1
  milestones: [67, 82, 95, 107]

# Loss function settings
loss_function: LabelSmoothingCrossEntropy  # valid loss function from 'torch.nn' or custom LabelSmoothingCrossEntropy
loss_params:  # loss parameters to be passed as '**dict'
